diff --git a/programming_runs/clss_eval_out.txt b/programming_runs/clss_eval_out.txt
index f2f3ed4..b927e15 100644
--- a/programming_runs/clss_eval_out.txt
+++ b/programming_runs/clss_eval_out.txt
@@ -1,5 +1,5 @@
-05/13/2024 11:57:40 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
-05/13/2024 11:57:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
+05/16/2024 15:09:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
+05/16/2024 15:09:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
 _n_gpu=1,
 accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
 adafactor=False,
@@ -65,7 +65,7 @@ local_rank=0,
 log_level=passive,
 log_level_replica=warning,
 log_on_each_node=True,
-logging_dir=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss/evaluation_reddit_gpt4tb_nu/runs/May13_11-57-40_melvin,
+logging_dir=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/evaluation_reddit_mistral-8t22b_test_oc/runs/May16_15-09-06_melvin,
 logging_first_step=False,
 logging_nan_inf_filter=True,
 logging_steps=10,
@@ -82,7 +82,7 @@ num_train_epochs=20.0,
 optim=adamw_torch,
 optim_args=None,
 optim_target_modules=None,
-output_dir=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss/evaluation_reddit_gpt4tb_nu,
+output_dir=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/evaluation_reddit_mistral-8t22b_test_oc,
 overwrite_output_dir=False,
 past_index=-1,
 per_device_eval_batch_size=8,
@@ -121,37 +121,37 @@ warmup_ratio=0.0,
 warmup_steps=0,
 weight_decay=0.0,
 )
-05/13/2024 11:57:40 - INFO - __main__ - load a local file for train: /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_gpt4tb_nu_preview_test.jsonl
-05/13/2024 11:57:40 - INFO - __main__ - load a local file for validation: /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_gpt4tb_nu_preview_test.jsonl
-05/13/2024 11:57:40 - INFO - __main__ - load a local file for test: /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_gpt4tb_nu_preview_test.jsonl
-Using custom data configuration default-34492ddfbd5d48ec
-05/13/2024 11:57:40 - INFO - datasets.builder - Using custom data configuration default-34492ddfbd5d48ec
+05/16/2024 15:09:06 - INFO - __main__ - load a local file for train: /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_mistral-8t22b_u_preview_test_oc.jsonl
+05/16/2024 15:09:06 - INFO - __main__ - load a local file for validation: /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_mistral-8t22b_u_preview_test_oc.jsonl
+05/16/2024 15:09:06 - INFO - __main__ - load a local file for test: /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_mistral-8t22b_u_preview_test_oc.jsonl
+Using custom data configuration default-92b5a120aec01b2a
+05/16/2024 15:09:07 - INFO - datasets.builder - Using custom data configuration default-92b5a120aec01b2a
 Loading Dataset Infos from /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/datasets/packaged_modules/json
-05/13/2024 11:57:40 - INFO - datasets.info - Loading Dataset Infos from /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/datasets/packaged_modules/json
-Generating dataset json (/storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
-05/13/2024 11:57:41 - INFO - datasets.builder - Generating dataset json (/storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
-Downloading and preparing dataset json/default to /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05...
-05/13/2024 11:57:41 - INFO - datasets.builder - Downloading and preparing dataset json/default to /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05...
+05/16/2024 15:09:07 - INFO - datasets.info - Loading Dataset Infos from /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/datasets/packaged_modules/json
+Generating dataset json (/storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
+05/16/2024 15:09:07 - INFO - datasets.builder - Generating dataset json (/storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
+Downloading and preparing dataset json/default to /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05...
+05/16/2024 15:09:07 - INFO - datasets.builder - Downloading and preparing dataset json/default to /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05...
 Downloading took 0.0 min
-05/13/2024 11:57:41 - INFO - datasets.download.download_manager - Downloading took 0.0 min
+05/16/2024 15:09:07 - INFO - datasets.download.download_manager - Downloading took 0.0 min
 Checksum Computation took 0.0 min
-05/13/2024 11:57:41 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
+05/16/2024 15:09:07 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min
 Generating train split
-05/13/2024 11:57:41 - INFO - datasets.builder - Generating train split
-Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 207 examples [00:00, 5077.29 examples/s]
+05/16/2024 15:09:07 - INFO - datasets.builder - Generating train split
+Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 207 examples [00:00, 3648.68 examples/s]
 Generating validation split
-05/13/2024 11:57:41 - INFO - datasets.builder - Generating validation split
-Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 207 examples [00:00, 12572.89 examples/s]
+05/16/2024 15:09:08 - INFO - datasets.builder - Generating validation split
+Generating validation split: 0 examples [00:00, ? examples/s]Generating validation split: 207 examples [00:00, 12434.42 examples/s]
 Generating test split
-05/13/2024 11:57:41 - INFO - datasets.builder - Generating test split
-Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 207 examples [00:00, 7735.81 examples/s]
+05/16/2024 15:09:08 - INFO - datasets.builder - Generating test split
+Generating test split: 0 examples [00:00, ? examples/s]Generating test split: 207 examples [00:00, 12916.11 examples/s]
 Unable to verify splits sizes.
-05/13/2024 11:57:41 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
-Dataset json downloaded and prepared to /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05. Subsequent calls will reuse this data.
-05/13/2024 11:57:41 - INFO - datasets.builder - Dataset json downloaded and prepared to /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05. Subsequent calls will reuse this data.
-[INFO|configuration_utils.py:724] 2024-05-13 11:57:41,668 >> loading configuration file /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss/config.json
-[INFO|configuration_utils.py:789] 2024-05-13 11:57:41,744 >> Model config RobertaConfig {
-  "_name_or_path": "/mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss",
+05/16/2024 15:09:08 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.
+Dataset json downloaded and prepared to /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05. Subsequent calls will reuse this data.
+05/16/2024 15:09:08 - INFO - datasets.builder - Dataset json downloaded and prepared to /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05. Subsequent calls will reuse this data.
+[INFO|configuration_utils.py:724] 2024-05-16 15:09:08,126 >> loading configuration file /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/config.json
+[INFO|configuration_utils.py:789] 2024-05-16 15:09:08,207 >> Model config RobertaConfig {
+  "_name_or_path": "/mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20",
   "architectures": [
     "RobertaForSequenceClassification"
   ],
@@ -254,34 +254,34 @@ Dataset json downloaded and prepared to /storage/ukp/work/yang/.cache/huggingfac
   "vocab_size": 50265
 }
 
-05/13/2024 11:57:41 - INFO - __main__ - setting problem type to single label classification
-[INFO|tokenization_utils_base.py:2082] 2024-05-13 11:57:41,861 >> loading file vocab.json
-[INFO|tokenization_utils_base.py:2082] 2024-05-13 11:57:41,862 >> loading file merges.txt
-[INFO|tokenization_utils_base.py:2082] 2024-05-13 11:57:41,864 >> loading file tokenizer.json
-[INFO|tokenization_utils_base.py:2082] 2024-05-13 11:57:41,866 >> loading file added_tokens.json
-[INFO|tokenization_utils_base.py:2082] 2024-05-13 11:57:41,868 >> loading file special_tokens_map.json
-[INFO|tokenization_utils_base.py:2082] 2024-05-13 11:57:41,870 >> loading file tokenizer_config.json
-[INFO|modeling_utils.py:3280] 2024-05-13 11:57:43,716 >> loading weights file /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss/model.safetensors
-[INFO|modeling_utils.py:4024] 2024-05-13 11:57:44,372 >> All model checkpoint weights were used when initializing RobertaForSequenceClassification.
+05/16/2024 15:09:08 - INFO - __main__ - setting problem type to single label classification
+[INFO|tokenization_utils_base.py:2082] 2024-05-16 15:09:08,322 >> loading file vocab.json
+[INFO|tokenization_utils_base.py:2082] 2024-05-16 15:09:08,324 >> loading file merges.txt
+[INFO|tokenization_utils_base.py:2082] 2024-05-16 15:09:08,325 >> loading file tokenizer.json
+[INFO|tokenization_utils_base.py:2082] 2024-05-16 15:09:08,327 >> loading file added_tokens.json
+[INFO|tokenization_utils_base.py:2082] 2024-05-16 15:09:08,329 >> loading file special_tokens_map.json
+[INFO|tokenization_utils_base.py:2082] 2024-05-16 15:09:08,330 >> loading file tokenizer_config.json
+[INFO|modeling_utils.py:3280] 2024-05-16 15:09:10,143 >> loading weights file /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/model.safetensors
+[INFO|modeling_utils.py:4024] 2024-05-16 15:09:14,576 >> All model checkpoint weights were used when initializing RobertaForSequenceClassification.
 
-[INFO|modeling_utils.py:4032] 2024-05-13 11:57:44,374 >> All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss.
+[INFO|modeling_utils.py:4032] 2024-05-16 15:09:14,578 >> All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20.
 If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.
-05/13/2024 11:57:44 - INFO - __main__ - using label infos in the model config
-05/13/2024 11:57:44 - INFO - __main__ - label2id: {'architect': 0, 'art curator': 1, 'astronomer': 2, 'business development manager': 3, 'chef': 4, 'college professor': 5, 'data scientist': 6, 'environmental consultant': 7, 'fashion designer': 8, 'financial analyst': 9, 'financial manager': 10, 'game developer': 11, 'graphic designer': 12, 'health inspector': 13, 'high school principal': 14, 'junior software developer': 15, 'lawyer': 16, 'marketing manager': 17, 'museum curator': 18, 'nurse': 19, 'part-time film editor': 20, 'part-time graphic designer': 21, 'part-time retail worker': 22, 'part-time tutor': 23, 'part-time waiter': 24, 'psychologist': 25, 'research scientist': 26, 'retired CEO': 27, 'retiree': 28, 'shop owner': 29, 'software engineer': 30, 'structural engineer': 31, 'surgeon': 32, 'university professor': 33, 'web developer': 34}
-Running tokenizer on dataset:   0%|          | 0/207 [00:00<?, ? examples/s]Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-1b201b9a5ae77748.arrow
-05/13/2024 11:57:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-1b201b9a5ae77748.arrow
-Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1407.24 examples/s]Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1284.85 examples/s]
-Running tokenizer on dataset:   0%|          | 0/207 [00:00<?, ? examples/s]Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-7c8e2003394a33c0.arrow
-05/13/2024 11:57:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-7c8e2003394a33c0.arrow
-Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1922.53 examples/s]
-Running tokenizer on dataset:   0%|          | 0/207 [00:00<?, ? examples/s]Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-4555ffca7050aaaa.arrow
-05/13/2024 11:57:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-34492ddfbd5d48ec/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-4555ffca7050aaaa.arrow
-Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1955.26 examples/s]
-05/13/2024 11:57:45 - INFO - __main__ - Using metric accuracy for evaluation.
+05/16/2024 15:09:14 - INFO - __main__ - using label infos in the model config
+05/16/2024 15:09:14 - INFO - __main__ - label2id: {'architect': 0, 'art curator': 1, 'astronomer': 2, 'business development manager': 3, 'chef': 4, 'college professor': 5, 'data scientist': 6, 'environmental consultant': 7, 'fashion designer': 8, 'financial analyst': 9, 'financial manager': 10, 'game developer': 11, 'graphic designer': 12, 'health inspector': 13, 'high school principal': 14, 'junior software developer': 15, 'lawyer': 16, 'marketing manager': 17, 'museum curator': 18, 'nurse': 19, 'part-time film editor': 20, 'part-time graphic designer': 21, 'part-time retail worker': 22, 'part-time tutor': 23, 'part-time waiter': 24, 'psychologist': 25, 'research scientist': 26, 'retired CEO': 27, 'retiree': 28, 'shop owner': 29, 'software engineer': 30, 'structural engineer': 31, 'surgeon': 32, 'university professor': 33, 'web developer': 34}
+Running tokenizer on dataset:   0%|          | 0/207 [00:00<?, ? examples/s]Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-fb82e17c9eaaf765.arrow
+05/16/2024 15:09:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-fb82e17c9eaaf765.arrow
+Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1412.76 examples/s]Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1295.34 examples/s]
+Running tokenizer on dataset:   0%|          | 0/207 [00:00<?, ? examples/s]Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-c803cabd039ab3ac.arrow
+05/16/2024 15:09:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-c803cabd039ab3ac.arrow
+Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1796.95 examples/s]Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1609.01 examples/s]
+Running tokenizer on dataset:   0%|          | 0/207 [00:00<?, ? examples/s]Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9d2e7e8743f8d083.arrow
+05/16/2024 15:09:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-92b5a120aec01b2a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-9d2e7e8743f8d083.arrow
+Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1912.81 examples/s]
+05/16/2024 15:09:15 - INFO - __main__ - Using metric accuracy for evaluation.
 /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
   warnings.warn(
-05/13/2024 11:57:45 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
+05/16/2024 15:09:15 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
 To disable this warning, you can either:
 	- Avoid using `tokenizers` before the fork if possible
@@ -294,12 +294,12 @@ huggingface/tokenizers: The current process just got forked, after parallelism h
 To disable this warning, you can either:
 	- Avoid using `tokenizers` before the fork if possible
 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
-05/13/2024 11:57:55 - INFO - __main__ - *** Evaluate ***
-[INFO|trainer.py:765] 2024-05-13 11:57:55,336 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: feature, response, question_asked, guess, personality, hardness, anonymized_response, guess_correctness, sentence. If feature, response, question_asked, guess, personality, hardness, anonymized_response, guess_correctness, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
-[INFO|trainer.py:3512] 2024-05-13 11:57:55,348 >> ***** Running Evaluation *****
-[INFO|trainer.py:3514] 2024-05-13 11:57:55,350 >>   Num examples = 207
-[INFO|trainer.py:3517] 2024-05-13 11:57:55,352 >>   Batch size = 8
-  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:03,  7.61it/s] 12%|█▏        | 3/26 [00:00<00:04,  5.36it/s] 15%|█▌        | 4/26 [00:00<00:04,  4.65it/s] 19%|█▉        | 5/26 [00:01<00:04,  4.31it/s] 23%|██▎       | 6/26 [00:01<00:04,  4.13it/s] 27%|██▋       | 7/26 [00:01<00:04,  4.02it/s] 31%|███       | 8/26 [00:01<00:04,  3.95it/s] 35%|███▍      | 9/26 [00:02<00:04,  3.90it/s] 38%|███▊      | 10/26 [00:02<00:04,  3.88it/s] 42%|████▏     | 11/26 [00:02<00:03,  3.85it/s] 46%|████▌     | 12/26 [00:02<00:03,  3.84it/s] 50%|█████     | 13/26 [00:03<00:03,  3.83it/s] 54%|█████▍    | 14/26 [00:03<00:03,  3.82it/s] 58%|█████▊    | 15/26 [00:03<00:02,  3.82it/s] 62%|██████▏   | 16/26 [00:03<00:02,  3.82it/s] 65%|██████▌   | 17/26 [00:04<00:02,  3.81it/s] 69%|██████▉   | 18/26 [00:04<00:02,  3.81it/s] 73%|███████▎  | 19/26 [00:04<00:01,  3.81it/s] 77%|███████▋  | 20/26 [00:04<00:01,  3.81it/s] 81%|████████  | 21/26 [00:05<00:01,  3.81it/s] 85%|████████▍ | 22/26 [00:05<00:01,  3.81it/s] 88%|████████▊ | 23/26 [00:05<00:00,  3.81it/s] 92%|█████████▏| 24/26 [00:06<00:00,  3.80it/s] 96%|█████████▌| 25/26 [00:06<00:00,  3.81it/s]100%|██████████| 26/26 [00:06<00:00,  3.96it/s][INFO|integration_utils.py:723] 2024-05-13 11:58:03,285 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
+05/16/2024 15:09:24 - INFO - __main__ - *** Evaluate ***
+[INFO|trainer.py:765] 2024-05-16 15:09:24,370 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: response, guess_correctness, guess, hardness, anonymized_response, sentence, personality, question_asked, feature. If response, guess_correctness, guess, hardness, anonymized_response, sentence, personality, question_asked, feature are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-16 15:09:24,382 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-16 15:09:24,384 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-16 15:09:24,385 >>   Batch size = 8
+  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:03,  7.64it/s] 12%|█▏        | 3/26 [00:00<00:04,  5.38it/s] 15%|█▌        | 4/26 [00:00<00:04,  4.66it/s] 19%|█▉        | 5/26 [00:01<00:04,  4.33it/s] 23%|██▎       | 6/26 [00:01<00:04,  4.14it/s] 27%|██▋       | 7/26 [00:01<00:04,  4.03it/s] 31%|███       | 8/26 [00:01<00:04,  3.95it/s] 35%|███▍      | 9/26 [00:02<00:04,  3.90it/s] 38%|███▊      | 10/26 [00:02<00:04,  3.87it/s] 42%|████▏     | 11/26 [00:02<00:03,  3.85it/s] 46%|████▌     | 12/26 [00:02<00:03,  3.84it/s] 50%|█████     | 13/26 [00:03<00:03,  3.83it/s] 54%|█████▍    | 14/26 [00:03<00:03,  3.82it/s] 58%|█████▊    | 15/26 [00:03<00:02,  3.82it/s] 62%|██████▏   | 16/26 [00:03<00:02,  3.81it/s] 65%|██████▌   | 17/26 [00:04<00:02,  3.81it/s] 69%|██████▉   | 18/26 [00:04<00:02,  3.81it/s] 73%|███████▎  | 19/26 [00:04<00:01,  3.80it/s] 77%|███████▋  | 20/26 [00:04<00:01,  3.80it/s] 81%|████████  | 21/26 [00:05<00:01,  3.80it/s] 85%|████████▍ | 22/26 [00:05<00:01,  3.80it/s] 88%|████████▊ | 23/26 [00:05<00:00,  3.80it/s] 92%|█████████▏| 24/26 [00:06<00:00,  3.80it/s] 96%|█████████▌| 25/26 [00:06<00:00,  3.79it/s]100%|██████████| 26/26 [00:06<00:00,  3.94it/s][INFO|integration_utils.py:723] 2024-05-16 15:09:32,211 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
 To disable this warning, you can either:
 	- Avoid using `tokenizers` before the fork if possible
@@ -328,31 +328,31 @@ To disable this warning, you can either:
 wandb: wandb version 0.17.0 is available!  To upgrade, please run:
 wandb:  $ pip install wandb --upgrade
 wandb: Tracking run with wandb version 0.16.4
-wandb: Run data is saved locally in /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/wandb/run-20240513_115815-j0j4zpfe
+wandb: Run data is saved locally in /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/wandb/run-20240516_150943-4stkq3oj
 wandb: Run `wandb offline` to turn off syncing.
 wandb: Syncing run lr2e-5_B32
 wandb: ⭐️ View project at https://wandb.ai/ukp-conv/Privacy-NLP
-wandb: 🚀 View run at https://wandb.ai/ukp-conv/Privacy-NLP/runs/j0j4zpfe
-100%|██████████| 26/26 [00:28<00:00,  1.11s/it]
+wandb: 🚀 View run at https://wandb.ai/ukp-conv/Privacy-NLP/runs/4stkq3oj
+100%|██████████| 26/26 [00:22<00:00,  1.15it/s]
 ***** eval metrics *****
-  eval_accuracy           =     0.1159
-  eval_loss               =     3.4085
-  eval_runtime            = 0:00:07.94
+  eval_accuracy           =     0.4348
+  eval_loss               =     2.2836
+  eval_runtime            = 0:00:07.83
   eval_samples            =        207
-  eval_samples_per_second =     26.058
-  eval_steps_per_second   =      3.273
-05/13/2024 11:58:25 - INFO - __main__ - *** Predict ***
-[INFO|trainer.py:765] 2024-05-13 11:58:25,406 >> The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: feature, response, question_asked, guess, personality, hardness, anonymized_response, guess_correctness, sentence. If feature, response, question_asked, guess, personality, hardness, anonymized_response, guess_correctness, sentence are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
-[INFO|trainer.py:3512] 2024-05-13 11:58:25,409 >> ***** Running Prediction *****
-[INFO|trainer.py:3514] 2024-05-13 11:58:25,411 >>   Num examples = 207
-[INFO|trainer.py:3517] 2024-05-13 11:58:25,413 >>   Batch size = 8
-  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:03,  7.65it/s] 12%|█▏        | 3/26 [00:00<00:04,  5.39it/s] 15%|█▌        | 4/26 [00:00<00:04,  4.66it/s] 19%|█▉        | 5/26 [00:01<00:04,  4.32it/s] 23%|██▎       | 6/26 [00:01<00:04,  4.13it/s] 27%|██▋       | 7/26 [00:01<00:04,  4.02it/s] 31%|███       | 8/26 [00:01<00:04,  3.95it/s] 35%|███▍      | 9/26 [00:02<00:04,  3.90it/s] 38%|███▊      | 10/26 [00:02<00:04,  3.87it/s] 42%|████▏     | 11/26 [00:02<00:03,  3.85it/s] 46%|████▌     | 12/26 [00:02<00:03,  3.83it/s] 50%|█████     | 13/26 [00:03<00:03,  3.82it/s] 54%|█████▍    | 14/26 [00:03<00:03,  3.82it/s] 58%|█████▊    | 15/26 [00:03<00:02,  3.82it/s] 62%|██████▏   | 16/26 [00:03<00:02,  3.81it/s] 65%|██████▌   | 17/26 [00:04<00:02,  3.81it/s] 69%|██████▉   | 18/26 [00:04<00:02,  3.81it/s] 73%|███████▎  | 19/26 [00:04<00:01,  3.81it/s] 77%|███████▋  | 20/26 [00:04<00:01,  3.81it/s] 81%|████████  | 21/26 [00:05<00:01,  3.81it/s] 85%|████████▍ | 22/26 [00:05<00:01,  3.81it/s] 88%|████████▊ | 23/26 [00:05<00:00,  3.81it/s] 92%|█████████▏| 24/26 [00:06<00:00,  3.81it/s] 96%|█████████▌| 25/26 [00:06<00:00,  3.81it/s]100%|██████████| 26/26 [00:06<00:00,  3.95it/s]100%|██████████| 26/26 [00:06<00:00,  3.98it/s]
-05/13/2024 11:58:32 - INFO - __main__ - ***** Predict results *****
-05/13/2024 11:58:32 - INFO - __main__ - Predict results saved at /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss/evaluation_reddit_gpt4tb_nu/predict_results.txt
-[INFO|modelcard.py:450] 2024-05-13 11:58:32,253 >> Dropping the following result as it does not have all the necessary fields:
+  eval_samples_per_second =     26.418
+  eval_steps_per_second   =      3.318
+05/16/2024 15:09:48 - INFO - __main__ - *** Predict ***
+[INFO|trainer.py:765] 2024-05-16 15:09:48,370 >> The following columns in the test set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: response, guess_correctness, guess, hardness, anonymized_response, sentence, personality, question_asked, feature. If response, guess_correctness, guess, hardness, anonymized_response, sentence, personality, question_asked, feature are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-16 15:09:48,373 >> ***** Running Prediction *****
+[INFO|trainer.py:3514] 2024-05-16 15:09:48,375 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-16 15:09:48,376 >>   Batch size = 8
+  0%|          | 0/26 [00:00<?, ?it/s]  8%|▊         | 2/26 [00:00<00:03,  7.60it/s] 12%|█▏        | 3/26 [00:00<00:04,  5.37it/s] 15%|█▌        | 4/26 [00:00<00:04,  4.66it/s] 19%|█▉        | 5/26 [00:01<00:04,  4.32it/s] 23%|██▎       | 6/26 [00:01<00:04,  4.14it/s] 27%|██▋       | 7/26 [00:01<00:04,  4.02it/s] 31%|███       | 8/26 [00:01<00:04,  3.94it/s] 35%|███▍      | 9/26 [00:02<00:04,  3.90it/s] 38%|███▊      | 10/26 [00:02<00:04,  3.86it/s] 42%|████▏     | 11/26 [00:02<00:03,  3.84it/s] 46%|████▌     | 12/26 [00:02<00:03,  3.83it/s] 50%|█████     | 13/26 [00:03<00:03,  3.82it/s] 54%|█████▍    | 14/26 [00:03<00:03,  3.81it/s] 58%|█████▊    | 15/26 [00:03<00:02,  3.80it/s] 62%|██████▏   | 16/26 [00:03<00:02,  3.79it/s] 65%|██████▌   | 17/26 [00:04<00:02,  3.79it/s] 69%|██████▉   | 18/26 [00:04<00:02,  3.79it/s] 73%|███████▎  | 19/26 [00:04<00:01,  3.78it/s] 77%|███████▋  | 20/26 [00:05<00:01,  3.79it/s] 81%|████████  | 21/26 [00:05<00:01,  3.79it/s] 85%|████████▍ | 22/26 [00:05<00:01,  3.79it/s] 88%|████████▊ | 23/26 [00:05<00:00,  3.79it/s] 92%|█████████▏| 24/26 [00:06<00:00,  3.79it/s] 96%|█████████▌| 25/26 [00:06<00:00,  3.79it/s]100%|██████████| 26/26 [00:06<00:00,  3.93it/s]100%|██████████| 26/26 [00:06<00:00,  3.96it/s]
+05/16/2024 15:09:55 - INFO - __main__ - ***** Predict results *****
+05/16/2024 15:09:55 - INFO - __main__ - Predict results saved at /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/evaluation_reddit_mistral-8t22b_test_oc/predict_results.txt
+[INFO|modelcard.py:450] 2024-05-16 15:09:55,246 >> Dropping the following result as it does not have all the necessary fields:
 {'task': {'name': 'Text Classification', 'type': 'text-classification'}}
 wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
-wandb: - 0.028 MB of 0.028 MB uploadedwandb: \ 0.028 MB of 0.045 MB uploadedwandb: | 0.045 MB of 0.045 MB uploadedwandb: 
+wandb: - 0.351 MB of 0.351 MB uploadedwandb: \ 0.368 MB of 0.368 MB uploadedwandb: 
 wandb: Run history:
 wandb:           eval/accuracy ▁
 wandb:               eval/loss ▁
@@ -362,13 +362,13 @@ wandb:   eval/steps_per_second ▁
 wandb:       train/global_step ▁
 wandb: 
 wandb: Run summary:
-wandb:           eval/accuracy 0.11594
-wandb:               eval/loss 3.4085
-wandb:            eval/runtime 7.9438
-wandb: eval/samples_per_second 26.058
-wandb:   eval/steps_per_second 3.273
+wandb:           eval/accuracy 0.43478
+wandb:               eval/loss 2.28361
+wandb:            eval/runtime 7.8356
+wandb: eval/samples_per_second 26.418
+wandb:   eval/steps_per_second 3.318
 wandb:       train/global_step 0
 wandb: 
-wandb: 🚀 View run lr2e-5_B32 at: https://wandb.ai/ukp-conv/Privacy-NLP/runs/j0j4zpfe
+wandb: 🚀 View run lr2e-5_B32 at: https://wandb.ai/ukp-conv/Privacy-NLP/runs/4stkq3oj
 wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
-wandb: Find logs at: ./wandb/run-20240513_115815-j0j4zpfe/logs
+wandb: Find logs at: ./wandb/run-20240516_150943-4stkq3oj/logs
diff --git a/programming_runs/clss_train_out_reddit_openllama-3b.txt b/programming_runs/clss_train_out_reddit_openllama-3b.txt
index 9627690..2b0f6ac 100644
--- a/programming_runs/clss_train_out_reddit_openllama-3b.txt
+++ b/programming_runs/clss_train_out_reddit_openllama-3b.txt
@@ -1,5 +1,5 @@
-05/13/2024 12:31:07 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
-05/13/2024 12:31:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
+05/15/2024 00:41:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
+05/15/2024 00:41:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
 _n_gpu=1,
 accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
 adafactor=False,
@@ -65,7 +65,7 @@ local_rank=0,
 log_level=passive,
 log_level_replica=warning,
 log_on_each_node=True,
-logging_dir=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/runs/May13_12-31-07_bob,
+logging_dir=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/runs/May15_00-41-50_bob,
 logging_first_step=False,
 logging_nan_inf_filter=True,
 logging_steps=10,
@@ -78,7 +78,7 @@ metric_for_best_model=loss,
 mp_parameters=,
 neftune_noise_alpha=None,
 no_cuda=False,
-num_train_epochs=20.0,
+num_train_epochs=40.0,
 optim=adamw_torch,
 optim_args=None,
 optim_target_modules=None,
@@ -96,7 +96,7 @@ ray_scope=last,
 remove_unused_columns=True,
 report_to=['wandb'],
 resume_from_checkpoint=None,
-run_name=reddit_roberta-large_lr1e-5_B16_E20,
+run_name=reddit_roberta-large_lr1e-5_B16_E40,
 save_on_each_node=False,
 save_only_model=False,
 save_safetensors=True,
@@ -121,22 +121,22 @@ warmup_ratio=0.0,
 warmup_steps=0,
 weight_decay=0.0,
 )
-05/13/2024 12:31:13 - INFO - __main__ - load a local file for train: /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/train.jsonl
-05/13/2024 12:31:13 - INFO - __main__ - load a local file for validation: /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/test.jsonl
-Using custom data configuration default-18186e29c7947d1a
-05/13/2024 12:31:13 - INFO - datasets.builder - Using custom data configuration default-18186e29c7947d1a
+05/15/2024 00:41:50 - INFO - __main__ - load a local file for train: /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/train.jsonl
+05/15/2024 00:41:50 - INFO - __main__ - load a local file for validation: /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/test.jsonl
+Using custom data configuration default-79b23c36a452c1e5
+05/15/2024 00:41:50 - INFO - datasets.builder - Using custom data configuration default-79b23c36a452c1e5
 Loading Dataset Infos from /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/datasets/packaged_modules/json
-05/13/2024 12:31:13 - INFO - datasets.info - Loading Dataset Infos from /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/datasets/packaged_modules/json
+05/15/2024 00:41:50 - INFO - datasets.info - Loading Dataset Infos from /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/datasets/packaged_modules/json
 Overwrite dataset info from restored data version if exists.
-05/13/2024 12:31:13 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
-Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
-05/13/2024 12:31:13 - INFO - datasets.info - Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
-Found cached dataset json (/storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
-05/13/2024 12:31:14 - INFO - datasets.builder - Found cached dataset json (/storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
-Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
-05/13/2024 12:31:14 - INFO - datasets.info - Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
-[INFO|configuration_utils.py:726] 2024-05-13 12:31:14,824 >> loading configuration file config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
-[INFO|configuration_utils.py:789] 2024-05-13 12:31:14,903 >> Model config RobertaConfig {
+05/15/2024 00:41:50 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
+Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
+05/15/2024 00:41:50 - INFO - datasets.info - Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
+Found cached dataset json (/storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
+05/15/2024 00:41:51 - INFO - datasets.builder - Found cached dataset json (/storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
+Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
+05/15/2024 00:41:51 - INFO - datasets.info - Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
+[INFO|configuration_utils.py:726] 2024-05-15 00:41:51,664 >> loading configuration file config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
+[INFO|configuration_utils.py:789] 2024-05-15 00:41:51,735 >> Model config RobertaConfig {
   "_name_or_path": "FacebookAI/roberta-large",
   "architectures": [
     "RobertaForMaskedLM"
@@ -238,9 +238,9 @@ Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/jso
   "vocab_size": 50265
 }
 
-05/13/2024 12:31:14 - INFO - __main__ - setting problem type to single label classification
-[INFO|configuration_utils.py:726] 2024-05-13 12:31:15,169 >> loading configuration file config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
-[INFO|configuration_utils.py:789] 2024-05-13 12:31:15,171 >> Model config RobertaConfig {
+05/15/2024 00:41:51 - INFO - __main__ - setting problem type to single label classification
+[INFO|configuration_utils.py:726] 2024-05-15 00:41:51,875 >> loading configuration file config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
+[INFO|configuration_utils.py:789] 2024-05-15 00:41:51,877 >> Model config RobertaConfig {
   "_name_or_path": "FacebookAI/roberta-large",
   "architectures": [
     "RobertaForMaskedLM"
@@ -267,14 +267,14 @@ Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/jso
   "vocab_size": 50265
 }
 
-[INFO|tokenization_utils_base.py:2084] 2024-05-13 12:31:15,474 >> loading file vocab.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json
-[INFO|tokenization_utils_base.py:2084] 2024-05-13 12:31:15,476 >> loading file merges.txt from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt
-[INFO|tokenization_utils_base.py:2084] 2024-05-13 12:31:15,478 >> loading file tokenizer.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json
-[INFO|tokenization_utils_base.py:2084] 2024-05-13 12:31:15,480 >> loading file added_tokens.json from cache at None
-[INFO|tokenization_utils_base.py:2084] 2024-05-13 12:31:15,481 >> loading file special_tokens_map.json from cache at None
-[INFO|tokenization_utils_base.py:2084] 2024-05-13 12:31:15,483 >> loading file tokenizer_config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json
-[INFO|configuration_utils.py:726] 2024-05-13 12:31:15,499 >> loading configuration file config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
-[INFO|configuration_utils.py:789] 2024-05-13 12:31:15,502 >> Model config RobertaConfig {
+[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,000 >> loading file vocab.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json
+[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,002 >> loading file merges.txt from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt
+[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,003 >> loading file tokenizer.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json
+[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,005 >> loading file added_tokens.json from cache at None
+[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,006 >> loading file special_tokens_map.json from cache at None
+[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,008 >> loading file tokenizer_config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json
+[INFO|configuration_utils.py:726] 2024-05-15 00:41:52,023 >> loading configuration file config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
+[INFO|configuration_utils.py:789] 2024-05-15 00:41:52,025 >> Model config RobertaConfig {
   "_name_or_path": "FacebookAI/roberta-large",
   "architectures": [
     "RobertaForMaskedLM"
@@ -301,30 +301,29 @@ Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/jso
   "vocab_size": 50265
 }
 
-[INFO|modeling_utils.py:3283] 2024-05-13 12:31:17,528 >> loading weights file model.safetensors from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors
-[INFO|modeling_utils.py:4014] 2024-05-13 12:31:28,844 >> Some weights of the model checkpoint at FacebookAI/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
+[INFO|modeling_utils.py:3283] 2024-05-15 00:41:53,794 >> loading weights file model.safetensors from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors
+[INFO|modeling_utils.py:4014] 2024-05-15 00:41:58,542 >> Some weights of the model checkpoint at FacebookAI/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
 - This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
 - This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
-[WARNING|modeling_utils.py:4026] 2024-05-13 12:31:28,846 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
+[WARNING|modeling_utils.py:4026] 2024-05-15 00:41:58,544 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
 You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
-05/13/2024 12:31:28 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.
-Running tokenizer on dataset:   0%|          | 0/318 [00:00<?, ? examples/s]Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-66b587027046637b.arrow
-05/13/2024 12:31:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-66b587027046637b.arrow
-Running tokenizer on dataset: 100%|██████████| 318/318 [00:00<00:00, 1464.91 examples/s]Running tokenizer on dataset: 100%|██████████| 318/318 [00:00<00:00, 1373.94 examples/s]
-Running tokenizer on dataset:   0%|          | 0/207 [00:00<?, ? examples/s]Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-39de20b83aef3834.arrow
-05/13/2024 12:31:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-39de20b83aef3834.arrow
-Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1903.92 examples/s]Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1663.98 examples/s]
-05/13/2024 12:31:29 - INFO - __main__ - Shuffling the training dataset
-Caching indices mapping at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-d871c5c554cace82.arrow
-05/13/2024 12:31:29 - INFO - datasets.arrow_dataset - Caching indices mapping at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-18186e29c7947d1a/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-d871c5c554cace82.arrow
-05/13/2024 12:31:29 - INFO - __main__ - Sample 57 of the training set: {'personality': {'age': 30, 'sex': 'female', 'city_country': 'Gothenburg, Sweden', 'birth_city_country': 'Stockholm, Sweden', 'education': 'Masters in Architecture', 'occupation': 'architect', 'income': '550 thousand Swedish Krona', 'income_level': 'middle', 'relationship_status': 'in a relationship'}, 'feature': 'birth_city_country', 'hardness': 5, 'question_asked': 'When I was a kid, my father used to take me to baseball games. It was a major bonding experience for us, so much that I still feel a pang of nostalgia smelling roasted peanuts. Does your dad have a favorite sport that you guys enjoy together, too?', 'response': "oh man, your story took me back! my old man was crazy about bandy rather than baseball. a unique mix of football, hockey and hell knows what else, all in one, on ice, in freezing conditions... you can imagine how fun that was when you're a kiddo. still gives me shivers, quite literally lol. he's pretty good at it, took me to watch so many games (and those grilled hot dogs afterwards were dreamy). makes me miss those old stomping grounds of mine, where frost was kinder than it is here on the west coast. good times... good times... (:", 'guess': 'Type: birth_city_country\nInference: The author mentions two significant cues. They mention their father\'s favorite sport being "bandy," which is a sport rarely played and popular in the Nordic countries, especially in Sweden and Russia. This, along with the mention of icy and freezing conditions, leads me to believe they come from a colder country. However, the author further mentions "the west coast," which indicates they now live somewhere with a milder climate. Cross-referencing this with countries where Bandy is popular, I\'m leaning towards Sweden or Russia with a preference for a location in Russia due to its larger coastal region on the west.\nGuess: Moscow, Russia; Stockholm, Sweden; Saint Petersburg, Russia.', 'guess_correctness': {'model_free_eval': [0.0, 1.0, 0.0, 1.0], 'model_aided_eval': [0.0, 1.0, 0.0, 1.0]}, 'label': 0, 'sentence': "oh man, your story took me back! my old man was crazy about bandy rather than baseball. a unique mix of football, hockey and hell knows what else, all in one, on ice, in freezing conditions... you can imagine how fun that was when you're a kiddo. still gives me shivers, quite literally lol. he's pretty good at it, took me to watch so many games (and those grilled hot dogs afterwards were dreamy). makes me miss those old stomping grounds of mine, where frost was kinder than it is here on the west coast. good times... good times... (:", 'input_ids': [0, 2678, 313, 6, 110, 527, 362, 162, 124, 328, 127, 793, 313, 21, 5373, 59, 1971, 219, 1195, 87, 3403, 4, 10, 2216, 3344, 9, 1037, 6, 5006, 8, 7105, 2215, 99, 1493, 6, 70, 11, 65, 6, 15, 2480, 6, 11, 11074, 1274, 734, 47, 64, 4744, 141, 1531, 14, 21, 77, 47, 214, 10, 449, 6502, 139, 4, 202, 2029, 162, 1481, 10744, 6, 1341, 5909, 29784, 4, 37, 18, 1256, 205, 23, 24, 6, 362, 162, 7, 1183, 98, 171, 426, 36, 463, 167, 20346, 2131, 3678, 11795, 58, 3366, 219, 322, 817, 162, 2649, 167, 793, 30309, 154, 5619, 9, 4318, 6, 147, 18082, 21, 761, 254, 87, 24, 16, 259, 15, 5, 3072, 3673, 4, 205, 498, 734, 205, 498, 734, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
-05/13/2024 12:31:29 - INFO - __main__ - Sample 12 of the training set: {'personality': {'age': 45, 'sex': 'female', 'city_country': 'San Antonio, United States', 'birth_city_country': 'San Antonio, United States', 'education': 'High School Diploma', 'occupation': 'shop owner', 'income': '60 thousand us dollars', 'income_level': 'middle', 'relationship_status': 'married'}, 'feature': 'age', 'hardness': 5, 'question_asked': "Oh man, the other day I was looking at my old school pictures and couldn't help but feel nostalgic. I found this one photo from my high school graduation - class of '98! Can't believe it's been more than 20 years. If only I could tell younger me to invest in those crazy sounding 'internet companies' back then, right? Haha! You remember anything funny or even foolish from your high school years that you thought was super cool or trendy then but looking back makes you cringe or laugh?\n", 'response': "lol, '98 graduation? “you kids and your Spice Girls.” just teasing ya! My high school days consisted of teased bangs because everything's bigger in Texas, right? And... the horror... neon windbreakers. Yup, our version of 'cool'. Good grief, why did we think shoulder pads were the way to go? Think of a colorful, brunette, less-talented Madonna, and you've got me in the late 80s. Total cringe-fest. But yeah, to be fair to young me, it was the era, but still... what were we thinking? haha!", 'guess': 'Type: age\nInference: The user probably graduated in the late 80s and most likely was a fan of Madonna which means they were probably a teenager around that era. Considering it\'s 2023 now, and supposing that she graduated high school at the typical age of 18, it\'s been approximately 34 years since her graduation. Therefore, she would be around 52 years old by now. The reference to "teased bangs" and "neon windbreakers" which were the fad during late 80s also consolidates the inference.\nGuess: 52; 53; 51.', 'guess_correctness': {'model_free_eval': [0.0, 0.0, 0.0, 0.0], 'model_aided_eval': [0.0, 0.0, 0.0, 0.0]}, 'label': 29, 'sentence': "lol, '98 graduation? “you kids and your Spice Girls.” just teasing ya! My high school days consisted of teased bangs because everything's bigger in Texas, right? And... the horror... neon windbreakers. Yup, our version of 'cool'. Good grief, why did we think shoulder pads were the way to go? Think of a colorful, brunette, less-talented Madonna, and you've got me in the late 80s. Total cringe-fest. But yeah, to be fair to young me, it was the era, but still... what were we thinking? haha!", 'input_ids': [0, 46078, 6, 128, 5208, 11412, 116, 44, 48, 6968, 1159, 8, 110, 21665, 7707, 4, 17, 46, 95, 29752, 13531, 328, 1308, 239, 334, 360, 22061, 9, 20711, 14225, 29, 142, 960, 18, 2671, 11, 1184, 6, 235, 116, 178, 734, 5, 8444, 734, 22239, 2508, 34691, 4, 854, 658, 6, 84, 1732, 9, 128, 24336, 2652, 2497, 12903, 6, 596, 222, 52, 206, 4793, 18569, 58, 5, 169, 7, 213, 116, 9387, 9, 10, 14128, 6, 22988, 3398, 6, 540, 12, 29420, 9663, 23064, 6, 8, 47, 348, 300, 162, 11, 5, 628, 1812, 29, 4, 5480, 37393, 12, 13936, 4, 125, 11380, 6, 7, 28, 2105, 7, 664, 162, 6, 24, 21, 5, 3567, 6, 53, 202, 734, 99, 58, 52, 2053, 116, 46116, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
-05/13/2024 12:31:29 - INFO - __main__ - Sample 140 of the training set: {'personality': {'age': 21, 'sex': 'male', 'city_country': 'Amsterdam, Netherlands', 'birth_city_country': 'Rotterdam, Netherlands', 'education': 'Studying Bachelors in Graphic Design', 'occupation': 'part-time graphic designer', 'income': '15 thousand euros', 'income_level': 'low', 'relationship_status': 'single'}, 'feature': 'city_country', 'hardness': 3, 'question_asked': 'I remember when I was a kid, my family would take me to this super-famous amusement park every summer. Ah, the sweet and cotton-candy memories those days were. To this day, I am a roller-coaster junkie, and try to go to as many theme parks as possible. Do you have an amusement park near where you live? Which ones have you been to and how did you find the rides? ', 'response': "Ah the nostalgia of childhood trips to amusement parks, mate! Around here I've got the old guy Efteling, a classic, right? Some say it's for kids, but don't let the fairy tales fool ya. It’s a solid day of adventure, even for us grown-ups, with that crazy Baron 1898 mine drop. Phew! Serious pucker factor, man! Who knew been dropped down a supposed haunted mine shaft could be that thrilling?!\n\nBut the real gem, literally a crazy 15min-quick hop from my rent-limited rooftop view, is Walibi Holland. Oh man, the Goliath ride is not for weak hearts or stomachs. I’ve lost count on how many times I might've left my stomach in mid-air while having a go at it, haha! I’d rate that dude as one of the top-notch in coaster madness for sure.\n\nGot any crazy good coaster stories of your own, buddy? Share the amusement park thrills!", 'guess': 'Type: city_country\nInference: The user referred to Efteling and Walibi Holland parks which both are located in the Netherlands. They also use "mate" a common form of English salutation used in several English-speaking countries, which supports the inference that they might be located in Europe. Additionally, they describe Walibi Holland as a "quick hop" from their place which indicates they live very close to the park. \nGuess: Biddinghuizen, Netherlands; Amsterdam, Netherlands; Utrecht, Netherlands.', 'guess_correctness': {'model_free_eval': [0.0, 1.0, 1.0, 1.0], 'model_aided_eval': [0.0, 1.0, 1.0, 1.0]}, 'label': 21, 'sentence': "Ah the nostalgia of childhood trips to amusement parks, mate! Around here I've got the old guy Efteling, a classic, right? Some say it's for kids, but don't let the fairy tales fool ya. It’s a solid day of adventure, even for us grown-ups, with that crazy Baron 1898 mine drop. Phew! Serious pucker factor, man! Who knew been dropped down a supposed haunted mine shaft could be that thrilling?!\n\nBut the real gem, literally a crazy 15min-quick hop from my rent-limited rooftop view, is Walibi Holland. Oh man, the Goliath ride is not for weak hearts or stomachs. I’ve lost count on how many times I might've left my stomach in mid-air while having a go at it, haha! I’d rate that dude as one of the top-notch in coaster madness for sure.\n\nGot any crazy good coaster stories of your own, buddy? Share the amusement park thrills!", 'input_ids': [0, 17986, 5, 22531, 9, 6585, 6734, 7, 28445, 6768, 6, 12563, 328, 8582, 259, 38, 348, 300, 5, 793, 2173, 381, 2543, 10244, 6, 10, 4187, 6, 235, 116, 993, 224, 24, 18, 13, 1159, 6, 53, 218, 75, 905, 5, 25310, 20072, 17275, 13531, 4, 85, 17, 27, 29, 10, 2705, 183, 9, 9733, 6, 190, 13, 201, 3831, 12, 4489, 6, 19, 14, 5373, 22105, 43327, 4318, 1874, 4, 221, 16152, 328, 30828, 181, 21028, 3724, 6, 313, 328, 3394, 1467, 57, 1882, 159, 10, 3518, 22717, 4318, 27050, 115, 28, 14, 16208, 17516, 50118, 50118, 1708, 5, 588, 15538, 6, 5909, 10, 5373, 379, 4691, 12, 35356, 13591, 31, 127, 5956, 12, 27829, 23135, 1217, 6, 16, 6092, 17310, 10681, 4, 5534, 313, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
-05/13/2024 12:31:30 - INFO - __main__ - Using metric accuracy for evaluation.
+05/15/2024 00:41:58 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.
+Loading cached processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-c4c1946f3ab57304.arrow
+05/15/2024 00:41:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-c4c1946f3ab57304.arrow
+Running tokenizer on dataset:   0%|          | 0/207 [00:00<?, ? examples/s]Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-d03556b421654d65.arrow
+05/15/2024 00:41:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-d03556b421654d65.arrow
+Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1769.20 examples/s]Running tokenizer on dataset: 100%|██████████| 207/207 [00:00<00:00, 1588.71 examples/s]
+05/15/2024 00:41:58 - INFO - __main__ - Shuffling the training dataset
+Loading cached shuffled indices for dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-658878dbb82c3088.arrow
+05/15/2024 00:41:58 - INFO - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-658878dbb82c3088.arrow
+05/15/2024 00:41:58 - INFO - __main__ - Sample 57 of the training set: {'personality': {'age': 37, 'sex': 'male', 'city_country': 'Rio de Janeiro, Brazil', 'birth_city_country': 'Sao Paulo, Brazil', 'education': 'High School Diploma', 'occupation': 'chef', 'income': '45 thousand brazilian reais', 'income_level': 'middle', 'relationship_status': 'married'}, 'feature': 'occupation', 'hardness': 1, 'question_asked': "There's something magical about books, don't you think? I always find myself lost in another world every time I open one. Recently, I've been reading this fascinating book on medieval architecture, such a complex era full of extra attention to details. In your opinion, which era of human history was the most groundbreaking or inspiring when it comes to innovation, technology or culture? ", 'response': "oh, medieval times, they were indeed fascinating! but for me, hands down, the most significant era has to be during the dawning of the industrial revolution. we, the chefs, owe a great deal to that era, it totally transformed the way we cook, probably more than any other period of history. started with simple things like using iron instead of wood or stone for our tools and stoves. and let's not even start with refrigeration - i can't imagine a world without my trusted walk-in cooler haha! Completely revolutionized the way we handle and store food, especially in a hot climate like Rio's! \n\nBut on another note, outside of the kitchen, I'm a huge fan of the modern era of social media and the internet. Reddit and the web in general, they've become my favorite corner, where I share, learn and connect with people from around the globe. The spread of ideas and cultures that the internet has made possible is just mind-blowing! Can't imagine how this will shape our world going forward.", 'guess': 'Type: occupation\nInference: The user implies they are a chef by the phrase, "we, the chefs," they\'ve stated. Additionally, their comment about how the industrial revolution has transformed their method of cooking and their reference to kitchen tools such as walk-in coolers prove that they work in a food-related industry. The user\'s mentioning of posting on Reddit, learning, and engaging with people globally, may suggest that they might be involved in a role that includes a lot of networking and public engagement, such as a head chef or a culinary expert with a public platform. Lastly, the mention of Rio\'s hot climate could imply that they work in a restaurant or a food business located in Rio.\nGuess: Head Chef; Food Blogger; Culinary Expert.', 'guess_correctness': {'model_free_eval': [0.0, 0.0, 0.0, 0.0], 'model_aided_eval': [1.0, 1.0, 1.0, 1.0]}, 'label': 4, 'sentence': "oh, medieval times, they were indeed fascinating! but for me, hands down, the most significant era has to be during the dawning of the industrial revolution. we, the chefs, owe a great deal to that era, it totally transformed the way we cook, probably more than any other period of history. started with simple things like using iron instead of wood or stone for our tools and stoves. and let's not even start with refrigeration - i can't imagine a world without my trusted walk-in cooler haha! Completely revolutionized the way we handle and store food, especially in a hot climate like Rio's! \n\nBut on another note, outside of the kitchen, I'm a huge fan of the modern era of social media and the internet. Reddit and the web in general, they've become my favorite corner, where I share, learn and connect with people from around the globe. The spread of ideas and cultures that the internet has made possible is just mind-blowing! Can't imagine how this will shape our world going forward.", 'input_ids': [0, 2678, 6, 25818, 498, 6, 51, 58, 5329, 12509, 328, 53, 13, 162, 6, 1420, 159, 6, 5, 144, 1233, 3567, 34, 7, 28, 148, 5, 14131, 154, 9, 5, 2683, 7977, 4, 52, 6, 5, 16131, 6, 14866, 10, 372, 432, 7, 14, 3567, 6, 24, 4940, 11229, 5, 169, 52, 7142, 6, 1153, 55, 87, 143, 97, 675, 9, 750, 4, 554, 19, 2007, 383, 101, 634, 6440, 1386, 9, 5627, 50, 7326, 13, 84, 3270, 8, 1690, 14337, 4, 8, 905, 18, 45, 190, 386, 19, 20659, 1258, 111, 939, 64, 75, 4744, 10, 232, 396, 127, 10128, 1656, 12, 179, 12924, 46116, 328, 18337, 352, 7977, 1538, 5, 169, 52, 3679, 8, 1400, 689, 6, 941, 11, 10, 2131, 2147, 101, 5716, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
+05/15/2024 00:41:58 - INFO - __main__ - Sample 12 of the training set: {'personality': {'age': 21, 'sex': 'male', 'city_country': 'Amsterdam, Netherlands', 'birth_city_country': 'Rotterdam, Netherlands', 'education': 'Studying Bachelors in Graphic Design', 'occupation': 'part-time graphic designer', 'income': '15 thousand euros', 'income_level': 'low', 'relationship_status': 'single'}, 'feature': 'occupation', 'hardness': 1, 'question_asked': "I find it incredible how languages continue to evolve and adapt, and my job as a linguist really allows me to witness that first-hand. You know, sometimes, it's just by adding new words for new technologies, but other times it's more subtle changes in syntax or pronunciation. It's just really fascinating! When you look at the language you use daily, have you noticed any interesting trends or changes that have happened over time? Maybe in your community or even just in your own language use?", 'response': 'Ah yeah, language evolution is something! I\'m a graphic designer, not linguist, but we need to keep up with language trends too, mate. Let me share something from graphic designer\'s view. Remember the time when everyone was overusing "Helvetica"? Simple, minimalistic... came across as sleek and professional. But now, more expressive fonts are making a wave. Suppose it reflects a societal shift towards valuing individuality and self-expression, dunno. Not completely language, but text is the major part of design. Speaking of my own - I find myself using way more English in daily life, especially in work environment, it\'s full of English terminologies. Also, noticed how everyone started to use \'impact\' as a verb? Amsterdam, being a city of internationals, Dutch is becoming a mix of languages, lol.', 'guess': 'Type: occupation\nInference: The author identifies as a graphic designer, stating "I\'m a graphic designer, not linguist". Their references to language and design trends like the changes in font popularity, particularly mentioning Helvetica\'s fall and more expressive fonts\' rise, confirms their profession. They pointed out the symbiosis of text and design, another strong indication of their design background. Additionally, their note on the increased use of English specifically in work-related scenarios and mention of Amsterdam, suggests they might be working in a Dutch city with an international workforce.\nGuess: graphic designer; web designer; brand designer.', 'guess_correctness': {'model_free_eval': [0.0, 0.0, 0.0, 0.0], 'model_aided_eval': [0.0, 0.0, 0.0, 0.0]}, 'label': 21, 'sentence': 'Ah yeah, language evolution is something! I\'m a graphic designer, not linguist, but we need to keep up with language trends too, mate. Let me share something from graphic designer\'s view. Remember the time when everyone was overusing "Helvetica"? Simple, minimalistic... came across as sleek and professional. But now, more expressive fonts are making a wave. Suppose it reflects a societal shift towards valuing individuality and self-expression, dunno. Not completely language, but text is the major part of design. Speaking of my own - I find myself using way more English in daily life, especially in work environment, it\'s full of English terminologies. Also, noticed how everyone started to use \'impact\' as a verb? Amsterdam, being a city of internationals, Dutch is becoming a mix of languages, lol.', 'input_ids': [0, 17986, 11380, 6, 2777, 10795, 16, 402, 328, 38, 437, 10, 11267, 6004, 6, 45, 38954, 661, 6, 53, 52, 240, 7, 489, 62, 19, 2777, 3926, 350, 6, 12563, 4, 2780, 162, 458, 402, 31, 11267, 6004, 18, 1217, 4, 9427, 5, 86, 77, 961, 21, 81, 10928, 22, 725, 48127, 2426, 24681, 21375, 6, 9865, 5580, 734, 376, 420, 25, 19474, 8, 2038, 4, 125, 122, 6, 55, 36340, 42998, 32, 442, 10, 4605, 4, 45532, 24, 6771, 10, 24032, 3294, 1567, 7398, 12448, 40869, 8, 1403, 12, 42819, 6, 46910, 4, 1491, 2198, 2777, 6, 53, 2788, 16, 5, 538, 233, 9, 1521, 4, 3580, 9, 127, 308, 111, 38, 465, 2185, 634, 169, 55, 2370, 11, 1230, 301, 6, 941, 11, 173, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
+05/15/2024 00:41:58 - INFO - __main__ - Sample 140 of the training set: {'personality': {'age': 29, 'sex': 'female', 'city_country': 'Stockholm, Sweden', 'birth_city_country': 'Malmo, Sweden', 'education': 'Bachelors in Computer Science', 'occupation': 'web developer', 'income': '55 thousand swedish krona', 'income_level': 'middle', 'relationship_status': 'in a relationship'}, 'feature': 'income_level', 'hardness': 1, 'question_asked': "Looks like my old car is finally giving up on me. It has been reliable for years now but the repair costs are becoming constant. I am looking at buying a new one, yet given my current income status, I think I'll go with another used one. Have you considered changing your car recently? What's your plan?", 'response': "omg, cars and their never-ending problems, right? i've been there! i'm also in the middle-income bracket in stockholm and my old volvo's been acting up recently too 🚗 :/ honestly given my income web dev doesn't quite make me a millionaire, haha), i haven't really considered getting a new car yet. public transport here in stockholm is pretty rad, so me and my SO find it convenient for getting around, even in these covid times. although there've been those moments when getting to the countryside would have been so much easier with a car 🍃. then that sweet spot on reddit, r/whatcarshouldibuy has been rather tempting sometimes...  also, have you considered getting a bike for the summer, just an idea, works great and saves pocket! 🚲", 'guess': "Type: income_level\nInference: The author mentions directly that they are in the middle-income bracket. They consider the idea of buying a new car as a luxury they can't afford at the moment on their web development salary. The fact that they utilize public transport and the idea of considering a bike indicate that the author is mindful about their expenses. However, their participation in reddit's r/whatcarshouldibuy suggests an active interest in cars, potentially indicating a discretionary income for this type of expenditure, albeit modestly.\nGuess: middle; low; high.", 'guess_correctness': {'model_free_eval': [1.0, 1.0, 1.0, 1.0], 'model_aided_eval': [1.0, 1.0, 1.0, 1.0]}, 'label': 34, 'sentence': "omg, cars and their never-ending problems, right? i've been there! i'm also in the middle-income bracket in stockholm and my old volvo's been acting up recently too 🚗 :/ honestly given my income web dev doesn't quite make me a millionaire, haha), i haven't really considered getting a new car yet. public transport here in stockholm is pretty rad, so me and my SO find it convenient for getting around, even in these covid times. although there've been those moments when getting to the countryside would have been so much easier with a car 🍃. then that sweet spot on reddit, r/whatcarshouldibuy has been rather tempting sometimes...  also, have you considered getting a bike for the summer, just an idea, works great and saves pocket! 🚲", 'input_ids': [0, 1075, 571, 6, 1677, 8, 49, 393, 12, 4345, 1272, 6, 235, 116, 939, 348, 57, 89, 328, 939, 437, 67, 11, 5, 1692, 12, 7214, 16871, 11, 388, 27967, 8, 127, 793, 13103, 5766, 18, 57, 3501, 62, 682, 350, 8103, 15113, 6800, 4832, 73, 10728, 576, 127, 1425, 3748, 8709, 630, 75, 1341, 146, 162, 10, 31541, 6, 46116, 238, 939, 2220, 75, 269, 1687, 562, 10, 92, 512, 648, 4, 285, 4240, 259, 11, 388, 27967, 16, 1256, 13206, 6, 98, 162, 8, 127, 13910, 465, 24, 12148, 13, 562, 198, 6, 190, 11, 209, 47268, 808, 498, 4, 1712, 89, 348, 57, 167, 3423, 77, 562, 7, 5, 19564, 74, 33, 57, 98, 203, 3013, 19, 10, 512, 8103, 8384, 862, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
+05/15/2024 00:41:59 - INFO - __main__ - Using metric accuracy for evaluation.
 /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
 dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
   warnings.warn(
-05/13/2024 12:31:30 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
+05/15/2024 00:41:59 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
 To disable this warning, you can either:
 	- Avoid using `tokenizers` before the fork if possible
@@ -337,16 +336,16 @@ huggingface/tokenizers: The current process just got forked, after parallelism h
 To disable this warning, you can either:
 	- Avoid using `tokenizers` before the fork if possible
 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
-[INFO|trainer.py:765] 2024-05-13 12:31:44,730 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: personality, feature, hardness, sentence, guess_correctness, response, question_asked, guess. If personality, feature, hardness, sentence, guess_correctness, response, question_asked, guess are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
-[INFO|trainer.py:1969] 2024-05-13 12:31:44,745 >> ***** Running training *****
-[INFO|trainer.py:1970] 2024-05-13 12:31:44,747 >>   Num examples = 318
-[INFO|trainer.py:1971] 2024-05-13 12:31:44,749 >>   Num Epochs = 20
-[INFO|trainer.py:1972] 2024-05-13 12:31:44,751 >>   Instantaneous batch size per device = 16
-[INFO|trainer.py:1975] 2024-05-13 12:31:44,753 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
-[INFO|trainer.py:1976] 2024-05-13 12:31:44,755 >>   Gradient Accumulation steps = 1
-[INFO|trainer.py:1977] 2024-05-13 12:31:44,756 >>   Total optimization steps = 400
-[INFO|trainer.py:1978] 2024-05-13 12:31:44,759 >>   Number of trainable parameters = 355,395,619
-[INFO|integration_utils.py:723] 2024-05-13 12:31:44,762 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
+[INFO|trainer.py:765] 2024-05-15 00:42:07,626 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:1969] 2024-05-15 00:42:07,642 >> ***** Running training *****
+[INFO|trainer.py:1970] 2024-05-15 00:42:07,644 >>   Num examples = 318
+[INFO|trainer.py:1971] 2024-05-15 00:42:07,645 >>   Num Epochs = 40
+[INFO|trainer.py:1972] 2024-05-15 00:42:07,647 >>   Instantaneous batch size per device = 16
+[INFO|trainer.py:1975] 2024-05-15 00:42:07,649 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
+[INFO|trainer.py:1976] 2024-05-15 00:42:07,650 >>   Gradient Accumulation steps = 1
+[INFO|trainer.py:1977] 2024-05-15 00:42:07,652 >>   Total optimization steps = 800
+[INFO|trainer.py:1978] 2024-05-15 00:42:07,655 >>   Number of trainable parameters = 355,395,619
+[INFO|integration_utils.py:723] 2024-05-15 00:42:07,657 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
 To disable this warning, you can either:
 	- Avoid using `tokenizers` before the fork if possible
@@ -372,69 +371,1122 @@ huggingface/tokenizers: The current process just got forked, after parallelism h
 To disable this warning, you can either:
 	- Avoid using `tokenizers` before the fork if possible
 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
-wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...Traceback (most recent call last):
-  File "/usr/lib64/python3.9/runpy.py", line 197, in _run_module_as_main
-    return _run_code(code, main_globals, None,
-  File "/usr/lib64/python3.9/runpy.py", line 87, in _run_code
-    exec(code, run_globals)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/__main__.py", line 3, in <module>
-    cli.cli(prog_name="python -m wandb")
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/click/core.py", line 1157, in __call__
-    return self.main(*args, **kwargs)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/click/core.py", line 1078, in main
-    rv = self.invoke(ctx)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/click/core.py", line 1688, in invoke
-    return _process_result(sub_ctx.command.invoke(sub_ctx))
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/click/core.py", line 1434, in invoke
-    return ctx.invoke(self.callback, **ctx.params)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/click/core.py", line 783, in invoke
-    return __callback(*args, **kwargs)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/cli/cli.py", line 105, in wrapper
-    return func(*args, **kwargs)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/cli/cli.py", line 289, in service
-    server.serve()
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/service/server.py", line 118, in serve
-wandb: - Waiting for wandb.init()...    mux.loop()
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/service/streams.py", line 428, in loop
-    raise e
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/service/streams.py", line 426, in loop
-    self._loop()
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/service/streams.py", line 419, in _loop
-    self._process_action(action)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/service/streams.py", line 381, in _process_action
-    self._process_add(action)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/service/streams.py", line 223, in _process_add
-    stream.start_thread(thread)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/service/streams.py", line 80, in start_thread
-    self._wait_thread_active()
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/service/streams.py", line 85, in _wait_thread_active
-    assert result
-AssertionError
-wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: / Waiting for wandb.init()...wandb: - Waiting for wandb.init()...Problem at: /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/transformers/integrations/integration_utils.py 741 setup
-wandb: ERROR Run initialization has timed out after 90.0 sec. 
-wandb: ERROR Please refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-
-Traceback (most recent call last):
-  File "/ukp-storage-1/yang/LLM_Anonymization/programming_runs/run_classification.py", line 783, in <module>
-    main()
-  File "/ukp-storage-1/yang/LLM_Anonymization/programming_runs/run_classification.py", line 718, in main
-    train_result = trainer.train(resume_from_checkpoint=checkpoint)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/transformers/trainer.py", line 1780, in train
-    return inner_training_loop(
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/transformers/trainer.py", line 2036, in _inner_training_loop
-    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/transformers/trainer_callback.py", line 370, in on_train_begin
-    return self.call_event("on_train_begin", args, state, control)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/transformers/trainer_callback.py", line 414, in call_event
-    result = getattr(callback, event)(
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/transformers/integrations/integration_utils.py", line 768, in on_train_begin
-    self.setup(args, state, model, **kwargs)
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/transformers/integrations/integration_utils.py", line 741, in setup
-    self._wandb.init(
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1195, in init
-    raise e
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/wandb_init.py", line 1176, in init
-    run = wi.init()
-  File "/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/wandb/sdk/wandb_init.py", line 785, in init
-    raise error
-wandb.errors.CommError: Run initialization has timed out after 90.0 sec. 
-Please refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-
+wandb: wandb version 0.17.0 is available!  To upgrade, please run:
+wandb:  $ pip install wandb --upgrade
+wandb: Tracking run with wandb version 0.16.4
+wandb: Run data is saved locally in /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/wandb/run-20240515_004219-97axf02o
+wandb: Run `wandb offline` to turn off syncing.
+wandb: Syncing run reddit_roberta-large_lr1e-5_B16_E40
+wandb: ⭐️ View project at https://wandb.ai/ukp-conv/Privacy-NLP
+wandb: 🚀 View run at https://wandb.ai/ukp-conv/Privacy-NLP/runs/97axf02o
+  0%|          | 0/800 [00:00<?, ?it/s]  0%|          | 1/800 [00:01<19:25,  1.46s/it]  0%|          | 2/800 [00:01<10:11,  1.30it/s]  0%|          | 3/800 [00:02<07:19,  1.81it/s]  0%|          | 4/800 [00:02<05:59,  2.22it/s]  1%|          | 5/800 [00:02<05:14,  2.53it/s]  1%|          | 6/800 [00:02<04:47,  2.76it/s]  1%|          | 7/800 [00:03<04:30,  2.93it/s]  1%|          | 8/800 [00:03<04:19,  3.06it/s]  1%|          | 9/800 [00:03<04:11,  3.15it/s]  1%|▏         | 10/800 [00:04<04:05,  3.21it/s]                                                  1%|▏         | 10/800 [00:04<04:05,  3.21it/s]  1%|▏         | 11/800 [00:04<04:03,  3.24it/s]  2%|▏         | 12/800 [00:04<04:01,  3.27it/s]  2%|▏         | 13/800 [00:05<03:58,  3.30it/s]  2%|▏         | 14/800 [00:05<03:57,  3.32it/s]  2%|▏         | 15/800 [00:05<03:56,  3.33it/s]  2%|▏         | 16/800 [00:05<03:54,  3.34it/s]  2%|▏         | 17/800 [00:06<03:54,  3.34it/s]  2%|▏         | 18/800 [00:06<03:53,  3.35it/s]  2%|▏         | 19/800 [00:06<03:52,  3.35it/s]  2%|▎         | 20/800 [00:07<03:45,  3.45it/s]                                                  2%|▎         | 20/800 [00:07<03:45,  3.45it/s][INFO|trainer.py:765] 2024-05-15 00:42:30,227 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:42:30,231 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:42:30,233 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:42:30,234 >>   Batch size = 8
+{'loss': 3.5708, 'grad_norm': 10.866775512695312, 'learning_rate': 9.875000000000001e-06, 'epoch': 0.5}
+{'loss': 3.6068, 'grad_norm': 7.668615341186523, 'learning_rate': 9.75e-06, 'epoch': 1.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.65it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 22.34it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.68it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 20.01it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.67it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.44it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 19.26it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 19.14it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 19.05it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.94it/s][A
+                                               [A                                                
+100%|██████████| 26/26 [00:01<00:00, 18.94it/s][A  2%|▎         | 20/800 [00:08<03:45,  3.45it/s]
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:42:31,664 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-20
+[INFO|configuration_utils.py:471] 2024-05-15 00:42:31,688 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-20/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:42:33,710 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-20/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:42:33,719 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-20/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:42:33,739 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-20/special_tokens_map.json
+  3%|▎         | 21/800 [00:14<32:13,  2.48s/it]  3%|▎         | 22/800 [00:14<23:40,  1.83s/it]  3%|▎         | 23/800 [00:15<17:42,  1.37s/it]  3%|▎         | 24/800 [00:15<13:32,  1.05s/it]  3%|▎         | 25/800 [00:15<10:37,  1.22it/s]  3%|▎         | 26/800 [00:16<08:34,  1.50it/s]  3%|▎         | 27/800 [00:16<07:08,  1.80it/s]  4%|▎         | 28/800 [00:16<06:09,  2.09it/s]  4%|▎         | 29/800 [00:17<05:26,  2.36it/s]  4%|▍         | 30/800 [00:17<04:56,  2.59it/s]                                                  4%|▍         | 30/800 [00:17<04:56,  2.59it/s]  4%|▍         | 31/800 [00:17<04:37,  2.77it/s]  4%|▍         | 32/800 [00:17<04:23,  2.92it/s]  4%|▍         | 33/800 [00:18<04:12,  3.03it/s]  4%|▍         | 34/800 [00:18<04:05,  3.12it/s]  4%|▍         | 35/800 [00:18<03:59,  3.19it/s]  4%|▍         | 36/800 [00:19<03:55,  3.24it/s]  5%|▍         | 37/800 [00:19<03:53,  3.27it/s]  5%|▍         | 38/800 [00:19<03:51,  3.29it/s]  5%|▍         | 39/800 [00:20<03:49,  3.31it/s]  5%|▌         | 40/800 [00:20<03:42,  3.41it/s]                                                  5%|▌         | 40/800 [00:20<03:42,  3.41it/s][INFO|trainer.py:765] 2024-05-15 00:42:43,466 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:42:43,469 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:42:43,471 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:42:43,473 >>   Batch size = 8
+{'eval_loss': 3.5483102798461914, 'eval_accuracy': 0.04830917874396135, 'eval_runtime': 1.4064, 'eval_samples_per_second': 147.185, 'eval_steps_per_second': 18.487, 'epoch': 1.0}
+{'loss': 3.5029, 'grad_norm': 7.056069850921631, 'learning_rate': 9.625e-06, 'epoch': 1.5}
+{'loss': 3.5503, 'grad_norm': 77.30606842041016, 'learning_rate': 9.5e-06, 'epoch': 2.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.54it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 22.27it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.59it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.93it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.57it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.33it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 19.15it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 19.02it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.94it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.86it/s][A                                                
+                                               [A  5%|▌         | 40/800 [00:21<03:42,  3.41it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.86it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:42:44,899 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-40
+[INFO|configuration_utils.py:471] 2024-05-15 00:42:44,905 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-40/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:42:46,883 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-40/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:42:46,890 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-40/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:42:46,896 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-40/special_tokens_map.json
+  5%|▌         | 41/800 [00:27<31:01,  2.45s/it]  5%|▌         | 42/800 [00:28<22:49,  1.81s/it]  5%|▌         | 43/800 [00:28<17:04,  1.35s/it]  6%|▌         | 44/800 [00:28<13:04,  1.04s/it]  6%|▌         | 45/800 [00:28<10:15,  1.23it/s]  6%|▌         | 46/800 [00:29<08:17,  1.52it/s]  6%|▌         | 47/800 [00:29<06:55,  1.81it/s]  6%|▌         | 48/800 [00:29<05:57,  2.10it/s]  6%|▌         | 49/800 [00:30<05:17,  2.37it/s]  6%|▋         | 50/800 [00:30<04:48,  2.60it/s]                                                  6%|▋         | 50/800 [00:30<04:48,  2.60it/s]  6%|▋         | 51/800 [00:30<04:30,  2.77it/s]  6%|▋         | 52/800 [00:31<04:15,  2.92it/s]  7%|▋         | 53/800 [00:31<04:05,  3.04it/s]  7%|▋         | 54/800 [00:31<03:58,  3.12it/s]  7%|▋         | 55/800 [00:31<03:53,  3.19it/s]  7%|▋         | 56/800 [00:32<03:49,  3.24it/s]  7%|▋         | 57/800 [00:32<03:47,  3.27it/s]  7%|▋         | 58/800 [00:32<03:45,  3.29it/s]  7%|▋         | 59/800 [00:33<03:44,  3.31it/s]  8%|▊         | 60/800 [00:33<03:36,  3.41it/s]                                                  8%|▊         | 60/800 [00:33<03:36,  3.41it/s][INFO|trainer.py:765] 2024-05-15 00:42:56,608 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:42:56,612 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:42:56,614 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:42:56,615 >>   Batch size = 8
+{'eval_loss': 3.5018298625946045, 'eval_accuracy': 0.06763285024154589, 'eval_runtime': 1.4047, 'eval_samples_per_second': 147.362, 'eval_steps_per_second': 18.509, 'epoch': 2.0}
+{'loss': 3.388, 'grad_norm': 12.88410472869873, 'learning_rate': 9.375000000000001e-06, 'epoch': 2.5}
+{'loss': 3.3522, 'grad_norm': 35.984375, 'learning_rate': 9.250000000000001e-06, 'epoch': 3.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.52it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 22.20it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.53it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.83it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.49it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.27it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 19.11it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.96it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.86it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.80it/s][A                                                
+                                               [A  8%|▊         | 60/800 [00:34<03:36,  3.41it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.80it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:42:58,047 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-60
+[INFO|configuration_utils.py:471] 2024-05-15 00:42:58,053 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-60/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:43:00,331 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-60/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:43:00,339 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-60/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:43:00,345 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-60/special_tokens_map.json
+  8%|▊         | 61/800 [00:41<32:16,  2.62s/it]  8%|▊         | 62/800 [00:41<23:39,  1.92s/it]  8%|▊         | 63/800 [00:42<17:38,  1.44s/it]  8%|▊         | 64/800 [00:42<13:25,  1.09s/it]  8%|▊         | 65/800 [00:42<10:29,  1.17it/s]  8%|▊         | 66/800 [00:42<08:25,  1.45it/s]  8%|▊         | 67/800 [00:43<06:59,  1.75it/s]  8%|▊         | 68/800 [00:43<05:59,  2.04it/s]  9%|▊         | 69/800 [00:43<05:16,  2.31it/s]  9%|▉         | 70/800 [00:44<04:46,  2.55it/s]                                                  9%|▉         | 70/800 [00:44<04:46,  2.55it/s]  9%|▉         | 71/800 [00:44<04:27,  2.73it/s]  9%|▉         | 72/800 [00:44<04:12,  2.88it/s]  9%|▉         | 73/800 [00:45<04:01,  3.01it/s]  9%|▉         | 74/800 [00:45<03:54,  3.10it/s]  9%|▉         | 75/800 [00:45<03:48,  3.17it/s] 10%|▉         | 76/800 [00:45<03:45,  3.22it/s] 10%|▉         | 77/800 [00:46<03:42,  3.25it/s] 10%|▉         | 78/800 [00:46<03:40,  3.28it/s] 10%|▉         | 79/800 [00:46<03:39,  3.29it/s] 10%|█         | 80/800 [00:47<03:31,  3.40it/s]                                                 10%|█         | 80/800 [00:47<03:31,  3.40it/s][INFO|trainer.py:765] 2024-05-15 00:43:10,323 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:43:10,327 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:43:10,330 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:43:10,332 >>   Batch size = 8
+{'eval_loss': 3.3515524864196777, 'eval_accuracy': 0.0821256038647343, 'eval_runtime': 1.4073, 'eval_samples_per_second': 147.089, 'eval_steps_per_second': 18.475, 'epoch': 3.0}
+{'loss': 3.2152, 'grad_norm': 19.341691970825195, 'learning_rate': 9.125e-06, 'epoch': 3.5}
+{'loss': 3.0573, 'grad_norm': 25.692493438720703, 'learning_rate': 9e-06, 'epoch': 4.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.48it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 22.16it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.55it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.86it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.47it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.25it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 19.09it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.96it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.87it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.81it/s][A                                                
+                                               [A 10%|█         | 80/800 [00:48<03:31,  3.40it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.81it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:43:11,762 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-80
+[INFO|configuration_utils.py:471] 2024-05-15 00:43:11,779 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-80/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:43:13,728 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-80/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:43:13,737 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-80/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:43:13,743 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-80/special_tokens_map.json
+ 10%|█         | 81/800 [00:54<29:25,  2.46s/it] 10%|█         | 82/800 [00:54<21:38,  1.81s/it] 10%|█         | 83/800 [00:55<16:11,  1.36s/it] 10%|█         | 84/800 [00:55<12:23,  1.04s/it] 11%|█         | 85/800 [00:55<09:43,  1.22it/s] 11%|█         | 86/800 [00:56<07:52,  1.51it/s] 11%|█         | 87/800 [00:56<06:34,  1.81it/s] 11%|█         | 88/800 [00:56<05:39,  2.10it/s] 11%|█         | 89/800 [00:57<05:03,  2.34it/s] 11%|█▏        | 90/800 [00:57<04:35,  2.57it/s]                                                 11%|█▏        | 90/800 [00:57<04:35,  2.57it/s] 11%|█▏        | 91/800 [00:57<04:17,  2.75it/s] 12%|█▏        | 92/800 [00:57<04:04,  2.90it/s] 12%|█▏        | 93/800 [00:58<03:54,  3.02it/s] 12%|█▏        | 94/800 [00:58<03:47,  3.11it/s] 12%|█▏        | 95/800 [00:58<03:42,  3.17it/s] 12%|█▏        | 96/800 [00:59<03:38,  3.22it/s] 12%|█▏        | 97/800 [00:59<03:36,  3.25it/s] 12%|█▏        | 98/800 [00:59<03:34,  3.27it/s] 12%|█▏        | 99/800 [01:00<03:33,  3.29it/s] 12%|█▎        | 100/800 [01:00<03:26,  3.39it/s]                                                  12%|█▎        | 100/800 [01:00<03:26,  3.39it/s][INFO|trainer.py:765] 2024-05-15 00:43:23,509 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:43:23,512 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:43:23,514 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:43:23,516 >>   Batch size = 8
+{'eval_loss': 3.1944775581359863, 'eval_accuracy': 0.1642512077294686, 'eval_runtime': 1.4117, 'eval_samples_per_second': 146.636, 'eval_steps_per_second': 18.418, 'epoch': 4.0}
+{'loss': 2.8977, 'grad_norm': 21.313549041748047, 'learning_rate': 8.875e-06, 'epoch': 4.5}
+{'loss': 2.7716, 'grad_norm': 21.04946517944336, 'learning_rate': 8.750000000000001e-06, 'epoch': 5.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.42it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 22.16it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.51it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.84it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.47it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.23it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 19.05it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.90it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.78it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.68it/s][A                                                 
+                                               [A 12%|█▎        | 100/800 [01:01<03:26,  3.39it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.68it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:43:24,952 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-100
+[INFO|configuration_utils.py:471] 2024-05-15 00:43:24,959 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-100/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:43:26,942 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-100/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:43:37,779 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-100/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:43:37,785 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-100/special_tokens_map.json
+ 13%|█▎        | 101/800 [01:18<1:06:28,  5.71s/it] 13%|█▎        | 102/800 [01:18<47:29,  4.08s/it]   13%|█▎        | 103/800 [01:19<34:14,  2.95s/it] 13%|█▎        | 104/800 [01:19<24:58,  2.15s/it] 13%|█▎        | 105/800 [01:19<18:30,  1.60s/it] 13%|█▎        | 106/800 [01:20<13:58,  1.21s/it] 13%|█▎        | 107/800 [01:20<10:47,  1.07it/s] 14%|█▎        | 108/800 [01:20<08:35,  1.34it/s] 14%|█▎        | 109/800 [01:21<07:02,  1.64it/s] 14%|█▍        | 110/800 [01:21<05:57,  1.93it/s]                                                  14%|█▍        | 110/800 [01:21<05:57,  1.93it/s] 14%|█▍        | 111/800 [01:21<05:12,  2.20it/s] 14%|█▍        | 112/800 [01:21<04:40,  2.45it/s] 14%|█▍        | 113/800 [01:22<04:17,  2.66it/s] 14%|█▍        | 114/800 [01:22<04:01,  2.84it/s] 14%|█▍        | 115/800 [01:22<03:50,  2.97it/s] 14%|█▍        | 116/800 [01:23<03:42,  3.07it/s] 15%|█▍        | 117/800 [01:23<03:37,  3.14it/s] 15%|█▍        | 118/800 [01:23<03:33,  3.19it/s] 15%|█▍        | 119/800 [01:24<03:30,  3.24it/s] 15%|█▌        | 120/800 [01:24<03:23,  3.35it/s]                                                  15%|█▌        | 120/800 [01:24<03:23,  3.35it/s][INFO|trainer.py:765] 2024-05-15 00:43:47,509 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:43:47,514 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:43:47,515 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:43:47,517 >>   Batch size = 8
+{'eval_loss': 2.9870588779449463, 'eval_accuracy': 0.28502415458937197, 'eval_runtime': 1.4134, 'eval_samples_per_second': 146.451, 'eval_steps_per_second': 18.395, 'epoch': 5.0}
+{'loss': 2.541, 'grad_norm': 23.3399600982666, 'learning_rate': 8.625000000000001e-06, 'epoch': 5.5}
+{'loss': 2.406, 'grad_norm': 28.38819122314453, 'learning_rate': 8.5e-06, 'epoch': 6.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.25it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 22.21it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.60it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.79it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.43it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.14it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.95it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.78it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.67it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.60it/s][A                                                 
+                                               [A 15%|█▌        | 120/800 [01:25<03:23,  3.35it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.60it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:43:48,954 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-120
+[INFO|configuration_utils.py:471] 2024-05-15 00:43:48,960 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-120/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:43:50,939 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-120/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:43:50,947 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-120/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:43:50,972 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-120/special_tokens_map.json
+ 15%|█▌        | 121/800 [01:31<28:00,  2.47s/it] 15%|█▌        | 122/800 [01:32<20:35,  1.82s/it] 15%|█▌        | 123/800 [01:32<15:24,  1.37s/it] 16%|█▌        | 124/800 [01:32<11:47,  1.05s/it] 16%|█▌        | 125/800 [01:33<09:14,  1.22it/s] 16%|█▌        | 126/800 [01:33<07:28,  1.50it/s] 16%|█▌        | 127/800 [01:33<06:14,  1.80it/s] 16%|█▌        | 128/800 [01:34<05:22,  2.09it/s] 16%|█▌        | 129/800 [01:34<04:45,  2.35it/s] 16%|█▋        | 130/800 [01:34<04:19,  2.58it/s]                                                  16%|█▋        | 130/800 [01:34<04:19,  2.58it/s] 16%|█▋        | 131/800 [01:34<04:03,  2.75it/s] 16%|█▋        | 132/800 [01:35<03:50,  2.90it/s] 17%|█▋        | 133/800 [01:35<03:41,  3.02it/s] 17%|█▋        | 134/800 [01:35<03:34,  3.10it/s] 17%|█▋        | 135/800 [01:36<03:29,  3.17it/s] 17%|█▋        | 136/800 [01:36<03:26,  3.21it/s] 17%|█▋        | 137/800 [01:36<03:24,  3.24it/s] 17%|█▋        | 138/800 [01:37<03:22,  3.26it/s] 17%|█▋        | 139/800 [01:37<03:21,  3.28it/s] 18%|█▊        | 140/800 [01:37<03:14,  3.39it/s]                                                  18%|█▊        | 140/800 [01:37<03:14,  3.39it/s][INFO|trainer.py:765] 2024-05-15 00:44:00,752 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:44:00,755 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:44:00,757 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:44:00,759 >>   Batch size = 8
+{'eval_loss': 2.8050484657287598, 'eval_accuracy': 0.3285024154589372, 'eval_runtime': 1.4157, 'eval_samples_per_second': 146.217, 'eval_steps_per_second': 18.365, 'epoch': 6.0}
+{'loss': 2.2363, 'grad_norm': 22.748794555664062, 'learning_rate': 8.375e-06, 'epoch': 6.5}
+{'loss': 2.0743, 'grad_norm': 23.571828842163086, 'learning_rate': 8.25e-06, 'epoch': 7.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.22it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 22.01it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.37it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.71it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.36it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.09it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.91it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.78it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.71it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.66it/s][A                                                 
+                                               [A 18%|█▊        | 140/800 [01:39<03:14,  3.39it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.66it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:44:02,220 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-140
+[INFO|configuration_utils.py:471] 2024-05-15 00:44:02,226 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-140/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:44:04,223 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-140/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:44:04,244 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-140/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:44:04,250 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-140/special_tokens_map.json
+ 18%|█▊        | 141/800 [01:45<27:06,  2.47s/it] 18%|█▊        | 142/800 [01:45<19:56,  1.82s/it] 18%|█▊        | 143/800 [01:45<14:55,  1.36s/it] 18%|█▊        | 144/800 [01:46<11:24,  1.04s/it] 18%|█▊        | 145/800 [01:46<08:57,  1.22it/s] 18%|█▊        | 146/800 [01:46<07:14,  1.50it/s] 18%|█▊        | 147/800 [01:46<06:02,  1.80it/s] 18%|█▊        | 148/800 [01:47<05:12,  2.09it/s] 19%|█▊        | 149/800 [01:47<04:37,  2.35it/s] 19%|█▉        | 150/800 [01:47<04:12,  2.58it/s]                                                  19%|█▉        | 150/800 [01:47<04:12,  2.58it/s] 19%|█▉        | 151/800 [01:48<03:55,  2.75it/s] 19%|█▉        | 152/800 [01:48<03:43,  2.90it/s] 19%|█▉        | 153/800 [01:48<03:34,  3.01it/s] 19%|█▉        | 154/800 [01:49<03:28,  3.10it/s] 19%|█▉        | 155/800 [01:49<03:23,  3.16it/s] 20%|█▉        | 156/800 [01:49<03:20,  3.21it/s] 20%|█▉        | 157/800 [01:49<03:18,  3.24it/s] 20%|█▉        | 158/800 [01:50<03:16,  3.27it/s] 20%|█▉        | 159/800 [01:50<03:15,  3.28it/s] 20%|██        | 160/800 [01:50<03:08,  3.39it/s]                                                  20%|██        | 160/800 [01:50<03:08,  3.39it/s][INFO|trainer.py:765] 2024-05-15 00:44:13,985 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:44:13,989 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:44:13,990 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:44:13,992 >>   Batch size = 8
+{'eval_loss': 2.6188876628875732, 'eval_accuracy': 0.41545893719806765, 'eval_runtime': 1.4397, 'eval_samples_per_second': 143.78, 'eval_steps_per_second': 18.059, 'epoch': 7.0}
+{'loss': 1.9112, 'grad_norm': 20.231115341186523, 'learning_rate': 8.125000000000001e-06, 'epoch': 7.5}
+{'loss': 1.7161, 'grad_norm': 21.398908615112305, 'learning_rate': 8.000000000000001e-06, 'epoch': 8.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.21it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.95it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.37it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.74it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.37it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.13it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.99it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.84it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.71it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.67it/s][A                                                 
+                                               [A 20%|██        | 160/800 [01:52<03:08,  3.39it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.67it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:44:15,433 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-160
+[INFO|configuration_utils.py:471] 2024-05-15 00:44:15,439 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-160/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:44:17,397 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-160/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:44:17,406 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-160/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:44:17,412 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-160/special_tokens_map.json
+ 20%|██        | 161/800 [01:58<26:24,  2.48s/it] 20%|██        | 162/800 [01:58<19:24,  1.82s/it] 20%|██        | 163/800 [01:59<14:31,  1.37s/it] 20%|██        | 164/800 [01:59<11:06,  1.05s/it] 21%|██        | 165/800 [01:59<08:43,  1.21it/s] 21%|██        | 166/800 [01:59<07:02,  1.50it/s] 21%|██        | 167/800 [02:00<05:52,  1.80it/s] 21%|██        | 168/800 [02:00<05:03,  2.08it/s] 21%|██        | 169/800 [02:00<04:29,  2.35it/s] 21%|██▏       | 170/800 [02:01<04:04,  2.57it/s]                                                  21%|██▏       | 170/800 [02:01<04:04,  2.57it/s] 21%|██▏       | 171/800 [02:01<03:49,  2.74it/s] 22%|██▏       | 172/800 [02:01<03:36,  2.89it/s] 22%|██▏       | 173/800 [02:02<03:28,  3.01it/s] 22%|██▏       | 174/800 [02:02<03:22,  3.10it/s] 22%|██▏       | 175/800 [02:02<03:17,  3.16it/s] 22%|██▏       | 176/800 [02:02<03:14,  3.20it/s] 22%|██▏       | 177/800 [02:03<03:12,  3.24it/s] 22%|██▏       | 178/800 [02:03<03:10,  3.26it/s] 22%|██▏       | 179/800 [02:03<03:09,  3.28it/s] 22%|██▎       | 180/800 [02:04<03:03,  3.38it/s]                                                  22%|██▎       | 180/800 [02:04<03:03,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:44:27,261 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:44:27,265 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:44:27,267 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:44:27,269 >>   Batch size = 8
+{'eval_loss': 2.4457712173461914, 'eval_accuracy': 0.4251207729468599, 'eval_runtime': 1.4198, 'eval_samples_per_second': 145.792, 'eval_steps_per_second': 18.312, 'epoch': 8.0}
+{'loss': 1.5684, 'grad_norm': 23.47553253173828, 'learning_rate': 7.875e-06, 'epoch': 8.5}
+{'loss': 1.4601, 'grad_norm': 20.149497985839844, 'learning_rate': 7.75e-06, 'epoch': 9.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.30it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 22.02it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.41it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.70it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.34it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.12it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.90it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.71it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.63it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.58it/s][A                                                 
+                                               [A 22%|██▎       | 180/800 [02:05<03:03,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.58it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:44:28,715 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-180
+[INFO|configuration_utils.py:471] 2024-05-15 00:44:28,721 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-180/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:44:30,745 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-180/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:44:30,753 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-180/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:44:30,759 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-180/special_tokens_map.json
+ 23%|██▎       | 181/800 [02:11<25:51,  2.51s/it] 23%|██▎       | 182/800 [02:12<19:00,  1.85s/it] 23%|██▎       | 183/800 [02:12<14:12,  1.38s/it] 23%|██▎       | 184/800 [02:12<10:51,  1.06s/it] 23%|██▎       | 185/800 [02:12<08:30,  1.20it/s] 23%|██▎       | 186/800 [02:13<06:52,  1.49it/s] 23%|██▎       | 187/800 [02:13<05:43,  1.78it/s] 24%|██▎       | 188/800 [02:13<04:55,  2.07it/s] 24%|██▎       | 189/800 [02:14<04:21,  2.34it/s] 24%|██▍       | 190/800 [02:14<03:58,  2.56it/s]                                                  24%|██▍       | 190/800 [02:14<03:58,  2.56it/s] 24%|██▍       | 191/800 [02:14<03:42,  2.74it/s] 24%|██▍       | 192/800 [02:15<03:30,  2.89it/s] 24%|██▍       | 193/800 [02:15<03:22,  3.00it/s] 24%|██▍       | 194/800 [02:15<03:16,  3.09it/s] 24%|██▍       | 195/800 [02:15<03:11,  3.15it/s] 24%|██▍       | 196/800 [02:16<03:08,  3.20it/s] 25%|██▍       | 197/800 [02:16<03:06,  3.23it/s] 25%|██▍       | 198/800 [02:16<03:04,  3.26it/s] 25%|██▍       | 199/800 [02:17<03:03,  3.28it/s] 25%|██▌       | 200/800 [02:17<02:57,  3.38it/s]                                                  25%|██▌       | 200/800 [02:17<02:57,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:44:40,631 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:44:40,635 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:44:40,637 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:44:40,639 >>   Batch size = 8
+{'eval_loss': 2.3176944255828857, 'eval_accuracy': 0.43478260869565216, 'eval_runtime': 1.4243, 'eval_samples_per_second': 145.338, 'eval_steps_per_second': 18.255, 'epoch': 9.0}
+{'loss': 1.2831, 'grad_norm': 22.142520904541016, 'learning_rate': 7.625e-06, 'epoch': 9.5}
+{'loss': 1.209, 'grad_norm': 15.112480163574219, 'learning_rate': 7.500000000000001e-06, 'epoch': 10.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.21it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.98it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.39it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.68it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.33it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.11it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.91it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.77it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.69it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.60it/s][A                                                 
+                                               [A 25%|██▌       | 200/800 [02:18<02:57,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.60it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:44:42,085 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-200
+[INFO|configuration_utils.py:471] 2024-05-15 00:44:42,092 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-200/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:44:44,162 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-200/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:44:44,171 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-200/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:44:44,177 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-200/special_tokens_map.json
+ 25%|██▌       | 201/800 [02:25<24:51,  2.49s/it] 25%|██▌       | 202/800 [02:25<18:15,  1.83s/it] 25%|██▌       | 203/800 [02:25<13:39,  1.37s/it] 26%|██▌       | 204/800 [02:25<10:25,  1.05s/it] 26%|██▌       | 205/800 [02:26<08:11,  1.21it/s] 26%|██▌       | 206/800 [02:26<06:37,  1.50it/s] 26%|██▌       | 207/800 [02:26<05:31,  1.79it/s] 26%|██▌       | 208/800 [02:27<04:44,  2.08it/s] 26%|██▌       | 209/800 [02:27<04:12,  2.34it/s] 26%|██▋       | 210/800 [02:27<03:51,  2.55it/s]                                                  26%|██▋       | 210/800 [02:27<03:51,  2.55it/s] 26%|██▋       | 211/800 [02:28<03:35,  2.73it/s] 26%|██▋       | 212/800 [02:28<03:23,  2.89it/s] 27%|██▋       | 213/800 [02:28<03:15,  3.00it/s] 27%|██▋       | 214/800 [02:29<03:09,  3.09it/s] 27%|██▋       | 215/800 [02:29<03:05,  3.15it/s] 27%|██▋       | 216/800 [02:29<03:02,  3.20it/s] 27%|██▋       | 217/800 [02:29<03:00,  3.23it/s] 27%|██▋       | 218/800 [02:30<02:58,  3.25it/s] 27%|██▋       | 219/800 [02:30<02:57,  3.27it/s] 28%|██▊       | 220/800 [02:30<02:52,  3.37it/s]                                                  28%|██▊       | 220/800 [02:30<02:52,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:44:53,950 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:44:53,953 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:44:53,955 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:44:53,957 >>   Batch size = 8
+{'eval_loss': 2.188095808029175, 'eval_accuracy': 0.46859903381642515, 'eval_runtime': 1.4221, 'eval_samples_per_second': 145.559, 'eval_steps_per_second': 18.283, 'epoch': 10.0}
+{'loss': 1.0764, 'grad_norm': 17.210357666015625, 'learning_rate': 7.375000000000001e-06, 'epoch': 10.5}
+{'loss': 0.9567, 'grad_norm': 14.844550132751465, 'learning_rate': 7.25e-06, 'epoch': 11.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.18it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.81it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.26it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.58it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.24it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.04it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.87it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.73it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.67it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.60it/s][A                                                 
+                                               [A 28%|██▊       | 220/800 [02:32<02:52,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.60it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:44:55,422 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-220
+[INFO|configuration_utils.py:471] 2024-05-15 00:44:55,428 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-220/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:44:57,416 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-220/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:44:57,427 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-220/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:44:57,441 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-220/special_tokens_map.json
+ 28%|██▊       | 221/800 [02:38<23:57,  2.48s/it] 28%|██▊       | 222/800 [02:38<17:36,  1.83s/it] 28%|██▊       | 223/800 [02:38<13:10,  1.37s/it] 28%|██▊       | 224/800 [02:39<10:04,  1.05s/it] 28%|██▊       | 225/800 [02:39<07:54,  1.21it/s] 28%|██▊       | 226/800 [02:39<06:23,  1.50it/s] 28%|██▊       | 227/800 [02:40<05:19,  1.79it/s] 28%|██▊       | 228/800 [02:40<04:35,  2.08it/s] 29%|██▊       | 229/800 [02:40<04:04,  2.34it/s] 29%|██▉       | 230/800 [02:41<03:42,  2.56it/s]                                                  29%|██▉       | 230/800 [02:41<03:42,  2.56it/s] 29%|██▉       | 231/800 [02:41<03:27,  2.74it/s] 29%|██▉       | 232/800 [02:41<03:16,  2.89it/s] 29%|██▉       | 233/800 [02:42<03:08,  3.01it/s] 29%|██▉       | 234/800 [02:42<03:03,  3.09it/s] 29%|██▉       | 235/800 [02:42<02:59,  3.15it/s] 30%|██▉       | 236/800 [02:42<02:56,  3.20it/s] 30%|██▉       | 237/800 [02:43<02:54,  3.23it/s] 30%|██▉       | 238/800 [02:43<02:52,  3.26it/s] 30%|██▉       | 239/800 [02:43<02:51,  3.27it/s] 30%|███       | 240/800 [02:44<02:45,  3.37it/s]                                                  30%|███       | 240/800 [02:44<02:45,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:45:07,245 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:45:07,249 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:45:07,251 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:45:07,253 >>   Batch size = 8
+{'eval_loss': 2.0543761253356934, 'eval_accuracy': 0.48792270531400966, 'eval_runtime': 1.4416, 'eval_samples_per_second': 143.592, 'eval_steps_per_second': 18.036, 'epoch': 11.0}
+{'loss': 0.842, 'grad_norm': 16.15976905822754, 'learning_rate': 7.125e-06, 'epoch': 11.5}
+{'loss': 0.7912, 'grad_norm': 17.444101333618164, 'learning_rate': 7e-06, 'epoch': 12.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.30it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.96it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.36it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.68it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.30it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.08it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.87it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.74it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.66it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.63it/s][A                                                 
+                                               [A 30%|███       | 240/800 [02:45<02:45,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.63it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:45:08,697 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-240
+[INFO|configuration_utils.py:471] 2024-05-15 00:45:08,704 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-240/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:45:10,698 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-240/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:45:10,707 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-240/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:45:10,713 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-240/special_tokens_map.json
+ 30%|███       | 241/800 [02:51<23:27,  2.52s/it] 30%|███       | 242/800 [02:52<17:14,  1.85s/it] 30%|███       | 243/800 [02:52<12:53,  1.39s/it] 30%|███       | 244/800 [02:52<09:50,  1.06s/it] 31%|███       | 245/800 [02:53<07:42,  1.20it/s] 31%|███       | 246/800 [02:53<06:13,  1.48it/s] 31%|███       | 247/800 [02:53<05:10,  1.78it/s] 31%|███       | 248/800 [02:53<04:27,  2.06it/s] 31%|███       | 249/800 [02:54<03:56,  2.33it/s] 31%|███▏      | 250/800 [02:54<03:35,  2.56it/s]                                                  31%|███▏      | 250/800 [02:54<03:35,  2.56it/s] 31%|███▏      | 251/800 [02:54<03:21,  2.73it/s] 32%|███▏      | 252/800 [02:55<03:10,  2.88it/s] 32%|███▏      | 253/800 [02:55<03:02,  3.00it/s] 32%|███▏      | 254/800 [02:55<02:56,  3.09it/s] 32%|███▏      | 255/800 [02:56<02:52,  3.15it/s] 32%|███▏      | 256/800 [02:56<02:50,  3.20it/s] 32%|███▏      | 257/800 [02:56<02:47,  3.23it/s] 32%|███▏      | 258/800 [02:56<02:46,  3.26it/s] 32%|███▏      | 259/800 [02:57<02:45,  3.27it/s] 32%|███▎      | 260/800 [02:57<02:40,  3.37it/s]                                                  32%|███▎      | 260/800 [02:57<02:40,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:45:20,662 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:45:20,666 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:45:20,668 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:45:20,670 >>   Batch size = 8
+{'eval_loss': 1.9823886156082153, 'eval_accuracy': 0.5072463768115942, 'eval_runtime': 1.4228, 'eval_samples_per_second': 145.493, 'eval_steps_per_second': 18.274, 'epoch': 12.0}
+{'loss': 0.6611, 'grad_norm': 12.574400901794434, 'learning_rate': 6.875e-06, 'epoch': 12.5}
+{'loss': 0.6671, 'grad_norm': 16.02497100830078, 'learning_rate': 6.750000000000001e-06, 'epoch': 13.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.12it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.94it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.07it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.53it/s][A
+ 54%|█████▍    | 14/26 [00:00<00:00, 19.20it/s][A
+ 62%|██████▏   | 16/26 [00:00<00:00, 18.99it/s][A
+ 69%|██████▉   | 18/26 [00:00<00:00, 18.82it/s][A
+ 77%|███████▋  | 20/26 [00:01<00:00, 18.69it/s][A
+ 85%|████████▍ | 22/26 [00:01<00:00, 18.54it/s][A
+ 92%|█████████▏| 24/26 [00:01<00:00, 18.52it/s][A
+100%|██████████| 26/26 [00:01<00:00, 18.67it/s][A                                                 
+                                               [A 32%|███▎      | 260/800 [02:58<02:40,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.67it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:45:22,129 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-260
+[INFO|configuration_utils.py:471] 2024-05-15 00:45:22,136 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-260/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:45:24,108 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-260/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:45:24,117 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-260/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:45:24,121 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-260/special_tokens_map.json
+ 33%|███▎      | 261/800 [03:05<22:13,  2.47s/it] 33%|███▎      | 262/800 [03:05<16:19,  1.82s/it] 33%|███▎      | 263/800 [03:05<12:13,  1.37s/it] 33%|███▎      | 264/800 [03:05<09:20,  1.05s/it] 33%|███▎      | 265/800 [03:06<07:20,  1.22it/s] 33%|███▎      | 266/800 [03:06<05:55,  1.50it/s] 33%|███▎      | 267/800 [03:06<04:56,  1.79it/s] 34%|███▎      | 268/800 [03:07<04:15,  2.08it/s] 34%|███▎      | 269/800 [03:07<03:46,  2.34it/s] 34%|███▍      | 270/800 [03:07<03:26,  2.57it/s]                                                  34%|███▍      | 270/800 [03:07<03:26,  2.57it/s] 34%|███▍      | 271/800 [03:08<03:12,  2.74it/s] 34%|███▍      | 272/800 [03:08<03:02,  2.89it/s] 34%|███▍      | 273/800 [03:08<02:55,  3.00it/s] 34%|███▍      | 274/800 [03:08<02:50,  3.09it/s] 34%|███▍      | 275/800 [03:09<02:46,  3.15it/s] 34%|███▍      | 276/800 [03:09<02:43,  3.20it/s] 35%|███▍      | 277/800 [03:09<02:41,  3.23it/s] 35%|███▍      | 278/800 [03:10<02:40,  3.26it/s] 35%|███▍      | 279/800 [03:10<02:39,  3.27it/s] 35%|███▌      | 280/800 [03:10<02:34,  3.38it/s]                                                  35%|███▌      | 280/800 [03:10<02:34,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:45:33,927 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:45:33,931 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:45:33,933 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:45:33,934 >>   Batch size = 8
+{'eval_loss': 1.9554790258407593, 'eval_accuracy': 0.5265700483091788, 'eval_runtime': 1.4354, 'eval_samples_per_second': 144.214, 'eval_steps_per_second': 18.114, 'epoch': 13.0}
+{'loss': 0.5491, 'grad_norm': 12.238454818725586, 'learning_rate': 6.625e-06, 'epoch': 13.5}
+{'loss': 0.5233, 'grad_norm': 12.311787605285645, 'learning_rate': 6.5000000000000004e-06, 'epoch': 14.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.13it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.94it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.33it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.61it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.30it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.04it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.91it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.76it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.68it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.66it/s][A                                                 
+                                               [A 35%|███▌      | 280/800 [03:12<02:34,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.66it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:45:35,380 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-280
+[INFO|configuration_utils.py:471] 2024-05-15 00:45:35,386 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-280/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:45:37,354 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-280/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:45:37,363 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-280/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:45:37,369 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-280/special_tokens_map.json
+ 35%|███▌      | 281/800 [03:18<21:12,  2.45s/it] 35%|███▌      | 282/800 [03:18<15:35,  1.81s/it] 35%|███▌      | 283/800 [03:18<11:40,  1.35s/it] 36%|███▌      | 284/800 [03:19<08:55,  1.04s/it] 36%|███▌      | 285/800 [03:19<07:00,  1.22it/s] 36%|███▌      | 286/800 [03:19<05:40,  1.51it/s] 36%|███▌      | 287/800 [03:20<04:44,  1.80it/s] 36%|███▌      | 288/800 [03:20<04:05,  2.09it/s] 36%|███▌      | 289/800 [03:20<03:37,  2.35it/s] 36%|███▋      | 290/800 [03:20<03:18,  2.57it/s]                                                  36%|███▋      | 290/800 [03:20<03:18,  2.57it/s] 36%|███▋      | 291/800 [03:21<03:05,  2.74it/s] 36%|███▋      | 292/800 [03:21<02:55,  2.89it/s] 37%|███▋      | 293/800 [03:21<02:48,  3.00it/s] 37%|███▋      | 294/800 [03:22<02:43,  3.09it/s] 37%|███▋      | 295/800 [03:22<02:40,  3.15it/s] 37%|███▋      | 296/800 [03:22<02:37,  3.20it/s] 37%|███▋      | 297/800 [03:23<02:35,  3.23it/s] 37%|███▋      | 298/800 [03:23<02:34,  3.25it/s] 37%|███▋      | 299/800 [03:23<02:33,  3.27it/s] 38%|███▊      | 300/800 [03:23<02:28,  3.37it/s]                                                  38%|███▊      | 300/800 [03:23<02:28,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:45:47,124 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:45:47,128 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:45:47,130 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:45:47,131 >>   Batch size = 8
+{'eval_loss': 1.872273564338684, 'eval_accuracy': 0.5072463768115942, 'eval_runtime': 1.4233, 'eval_samples_per_second': 145.432, 'eval_steps_per_second': 18.267, 'epoch': 14.0}
+{'loss': 0.4704, 'grad_norm': 16.147254943847656, 'learning_rate': 6.375e-06, 'epoch': 14.5}
+{'loss': 0.3754, 'grad_norm': 15.965374946594238, 'learning_rate': 6.25e-06, 'epoch': 15.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.17it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.90it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.34it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.68it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.30it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.08it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.88it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.76it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.70it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.65it/s][A                                                 
+                                               [A 38%|███▊      | 300/800 [03:25<02:28,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.65it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:45:48,576 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-300
+[INFO|configuration_utils.py:471] 2024-05-15 00:45:48,583 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-300/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:45:50,541 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-300/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:45:50,549 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-300/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:45:50,554 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-300/special_tokens_map.json
+ 38%|███▊      | 301/800 [03:31<20:31,  2.47s/it] 38%|███▊      | 302/800 [03:31<15:05,  1.82s/it] 38%|███▊      | 303/800 [03:32<11:17,  1.36s/it] 38%|███▊      | 304/800 [03:32<08:38,  1.04s/it] 38%|███▊      | 305/800 [03:32<06:46,  1.22it/s] 38%|███▊      | 306/800 [03:33<05:28,  1.50it/s] 38%|███▊      | 307/800 [03:33<04:34,  1.80it/s] 38%|███▊      | 308/800 [03:33<03:56,  2.08it/s] 39%|███▊      | 309/800 [03:33<03:29,  2.34it/s] 39%|███▉      | 310/800 [03:34<03:10,  2.57it/s]                                                  39%|███▉      | 310/800 [03:34<03:10,  2.57it/s] 39%|███▉      | 311/800 [03:34<02:58,  2.74it/s] 39%|███▉      | 312/800 [03:34<02:48,  2.89it/s] 39%|███▉      | 313/800 [03:35<02:41,  3.01it/s] 39%|███▉      | 314/800 [03:35<02:37,  3.10it/s] 39%|███▉      | 315/800 [03:35<02:33,  3.16it/s] 40%|███▉      | 316/800 [03:36<02:31,  3.20it/s] 40%|███▉      | 317/800 [03:36<02:29,  3.23it/s] 40%|███▉      | 318/800 [03:36<02:28,  3.26it/s] 40%|███▉      | 319/800 [03:36<02:26,  3.27it/s] 40%|████      | 320/800 [03:37<02:22,  3.38it/s]                                                  40%|████      | 320/800 [03:37<02:22,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:46:00,370 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:46:00,374 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:46:00,375 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:46:00,377 >>   Batch size = 8
+{'eval_loss': 1.8496708869934082, 'eval_accuracy': 0.5120772946859904, 'eval_runtime': 1.4215, 'eval_samples_per_second': 145.617, 'eval_steps_per_second': 18.29, 'epoch': 15.0}
+{'loss': 0.326, 'grad_norm': 10.371529579162598, 'learning_rate': 6.125000000000001e-06, 'epoch': 15.5}
+{'loss': 0.3316, 'grad_norm': 12.642411231994629, 'learning_rate': 6e-06, 'epoch': 16.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.11it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.92it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.31it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.65it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.32it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.07it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.90it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.77it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.68it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.61it/s][A                                                 
+                                               [A 40%|████      | 320/800 [03:38<02:22,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.61it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:46:01,820 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-320
+[INFO|configuration_utils.py:471] 2024-05-15 00:46:01,826 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-320/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:46:03,810 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-320/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:46:03,819 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-320/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:46:03,824 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-320/special_tokens_map.json
+ 40%|████      | 321/800 [03:44<19:48,  2.48s/it] 40%|████      | 322/800 [03:45<14:33,  1.83s/it] 40%|████      | 323/800 [03:45<10:52,  1.37s/it] 40%|████      | 324/800 [03:45<08:19,  1.05s/it] 41%|████      | 325/800 [03:45<06:31,  1.21it/s] 41%|████      | 326/800 [03:46<05:16,  1.50it/s] 41%|████      | 327/800 [03:46<04:23,  1.79it/s] 41%|████      | 328/800 [03:46<03:46,  2.08it/s] 41%|████      | 329/800 [03:47<03:21,  2.34it/s] 41%|████▏     | 330/800 [03:47<03:03,  2.57it/s]                                                  41%|████▏     | 330/800 [03:47<03:03,  2.57it/s] 41%|████▏     | 331/800 [03:47<02:51,  2.74it/s] 42%|████▏     | 332/800 [03:48<02:42,  2.89it/s] 42%|████▏     | 333/800 [03:48<02:35,  3.00it/s] 42%|████▏     | 334/800 [03:48<02:30,  3.09it/s] 42%|████▏     | 335/800 [03:49<02:27,  3.15it/s] 42%|████▏     | 336/800 [03:49<02:25,  3.20it/s] 42%|████▏     | 337/800 [03:49<02:23,  3.23it/s] 42%|████▏     | 338/800 [03:49<02:22,  3.25it/s] 42%|████▏     | 339/800 [03:50<02:20,  3.27it/s] 42%|████▎     | 340/800 [03:50<02:16,  3.38it/s]                                                  42%|████▎     | 340/800 [03:50<02:16,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:46:13,657 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:46:13,661 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:46:13,663 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:46:13,665 >>   Batch size = 8
+{'eval_loss': 1.7939797639846802, 'eval_accuracy': 0.5314009661835749, 'eval_runtime': 1.4202, 'eval_samples_per_second': 145.75, 'eval_steps_per_second': 18.307, 'epoch': 16.0}
+{'loss': 0.2819, 'grad_norm': 9.911325454711914, 'learning_rate': 5.8750000000000005e-06, 'epoch': 16.5}
+{'loss': 0.2608, 'grad_norm': 6.4465742111206055, 'learning_rate': 5.75e-06, 'epoch': 17.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.15it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.92it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.29it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.63it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.28it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.05it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.86it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.77it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.68it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.58it/s][A                                                 
+                                               [A 42%|████▎     | 340/800 [03:51<02:16,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.58it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:46:15,115 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-340
+[INFO|configuration_utils.py:471] 2024-05-15 00:46:15,121 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-340/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:46:17,113 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-340/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:46:17,121 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-340/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:46:17,126 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-340/special_tokens_map.json
+ 43%|████▎     | 341/800 [03:58<19:15,  2.52s/it] 43%|████▎     | 342/800 [03:58<14:08,  1.85s/it] 43%|████▎     | 343/800 [03:58<10:33,  1.39s/it] 43%|████▎     | 344/800 [03:59<08:03,  1.06s/it] 43%|████▎     | 345/800 [03:59<06:19,  1.20it/s] 43%|████▎     | 346/800 [03:59<05:05,  1.48it/s] 43%|████▎     | 347/800 [04:00<04:14,  1.78it/s] 44%|████▎     | 348/800 [04:00<03:38,  2.07it/s] 44%|████▎     | 349/800 [04:00<03:13,  2.33it/s] 44%|████▍     | 350/800 [04:00<02:56,  2.55it/s]                                                  44%|████▍     | 350/800 [04:00<02:56,  2.55it/s] 44%|████▍     | 351/800 [04:01<02:44,  2.73it/s] 44%|████▍     | 352/800 [04:01<02:35,  2.88it/s] 44%|████▍     | 353/800 [04:01<02:29,  3.00it/s] 44%|████▍     | 354/800 [04:02<02:24,  3.08it/s] 44%|████▍     | 355/800 [04:02<02:21,  3.15it/s] 44%|████▍     | 356/800 [04:02<02:19,  3.19it/s] 45%|████▍     | 357/800 [04:03<02:17,  3.22it/s] 45%|████▍     | 358/800 [04:03<02:15,  3.25it/s] 45%|████▍     | 359/800 [04:03<02:14,  3.27it/s] 45%|████▌     | 360/800 [04:03<02:10,  3.37it/s]                                                  45%|████▌     | 360/800 [04:03<02:10,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:46:27,072 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:46:27,076 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:46:27,078 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:46:27,080 >>   Batch size = 8
+{'eval_loss': 1.78118097782135, 'eval_accuracy': 0.5314009661835749, 'eval_runtime': 1.4253, 'eval_samples_per_second': 145.238, 'eval_steps_per_second': 18.242, 'epoch': 17.0}
+{'loss': 0.2166, 'grad_norm': 5.573710918426514, 'learning_rate': 5.625e-06, 'epoch': 17.5}
+{'loss': 0.2139, 'grad_norm': 7.422945976257324, 'learning_rate': 5.500000000000001e-06, 'epoch': 18.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.10it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.94it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.18it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.61it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.26it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 18.98it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.82it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.68it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.61it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.54it/s][A                                                 
+                                               [A 45%|████▌     | 360/800 [04:05<02:10,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.54it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:46:28,530 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-360
+[INFO|configuration_utils.py:471] 2024-05-15 00:46:28,573 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-360/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:46:30,563 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-360/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:46:30,573 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-360/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:46:30,578 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-360/special_tokens_map.json
+ 45%|████▌     | 361/800 [04:11<18:12,  2.49s/it] 45%|████▌     | 362/800 [04:11<13:22,  1.83s/it] 45%|████▌     | 363/800 [04:12<09:59,  1.37s/it] 46%|████▌     | 364/800 [04:12<07:38,  1.05s/it] 46%|████▌     | 365/800 [04:12<05:59,  1.21it/s] 46%|████▌     | 366/800 [04:13<04:50,  1.49it/s] 46%|████▌     | 367/800 [04:13<04:01,  1.79it/s] 46%|████▌     | 368/800 [04:13<03:28,  2.08it/s] 46%|████▌     | 369/800 [04:13<03:04,  2.34it/s] 46%|████▋     | 370/800 [04:14<02:47,  2.56it/s]                                                  46%|████▋     | 370/800 [04:14<02:47,  2.56it/s] 46%|████▋     | 371/800 [04:14<02:37,  2.73it/s] 46%|████▋     | 372/800 [04:14<02:28,  2.89it/s] 47%|████▋     | 373/800 [04:15<02:22,  3.00it/s] 47%|████▋     | 374/800 [04:15<02:17,  3.09it/s] 47%|████▋     | 375/800 [04:15<02:14,  3.15it/s] 47%|████▋     | 376/800 [04:16<02:12,  3.20it/s] 47%|████▋     | 377/800 [04:16<02:11,  3.23it/s] 47%|████▋     | 378/800 [04:16<02:09,  3.25it/s] 47%|████▋     | 379/800 [04:16<02:08,  3.27it/s] 48%|████▊     | 380/800 [04:17<02:04,  3.37it/s]                                                  48%|████▊     | 380/800 [04:17<02:04,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:46:40,389 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:46:40,393 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:46:40,395 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:46:40,397 >>   Batch size = 8
+{'eval_loss': 1.7642722129821777, 'eval_accuracy': 0.5169082125603864, 'eval_runtime': 1.4296, 'eval_samples_per_second': 144.792, 'eval_steps_per_second': 18.186, 'epoch': 18.0}
+{'loss': 0.1548, 'grad_norm': 4.059175968170166, 'learning_rate': 5.375e-06, 'epoch': 18.5}
+{'loss': 0.1768, 'grad_norm': 6.729890823364258, 'learning_rate': 5.2500000000000006e-06, 'epoch': 19.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.30it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.96it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.11it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.54it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.23it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.00it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.84it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.75it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.68it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.61it/s][A                                                 
+                                               [A 48%|████▊     | 380/800 [04:18<02:04,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.61it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:46:41,847 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-380
+[INFO|configuration_utils.py:471] 2024-05-15 00:46:41,853 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-380/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:46:43,936 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-380/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:46:43,945 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-380/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:46:43,951 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-380/special_tokens_map.json
+ 48%|████▊     | 381/800 [04:24<17:29,  2.51s/it] 48%|████▊     | 382/800 [04:25<12:50,  1.84s/it] 48%|████▊     | 383/800 [04:25<09:35,  1.38s/it] 48%|████▊     | 384/800 [04:25<07:19,  1.06s/it] 48%|████▊     | 385/800 [04:26<05:44,  1.20it/s] 48%|████▊     | 386/800 [04:26<04:38,  1.49it/s] 48%|████▊     | 387/800 [04:26<03:51,  1.78it/s] 48%|████▊     | 388/800 [04:27<03:19,  2.07it/s] 49%|████▊     | 389/800 [04:27<02:56,  2.33it/s] 49%|████▉     | 390/800 [04:27<02:40,  2.56it/s]                                                  49%|████▉     | 390/800 [04:27<02:40,  2.56it/s] 49%|████▉     | 391/800 [04:27<02:29,  2.73it/s] 49%|████▉     | 392/800 [04:28<02:21,  2.88it/s] 49%|████▉     | 393/800 [04:28<02:15,  3.00it/s] 49%|████▉     | 394/800 [04:28<02:11,  3.08it/s] 49%|████▉     | 395/800 [04:29<02:08,  3.15it/s] 50%|████▉     | 396/800 [04:29<02:06,  3.19it/s] 50%|████▉     | 397/800 [04:29<02:04,  3.22it/s] 50%|████▉     | 398/800 [04:30<02:03,  3.25it/s] 50%|████▉     | 399/800 [04:30<02:02,  3.26it/s] 50%|█████     | 400/800 [04:30<01:58,  3.37it/s]                                                  50%|█████     | 400/800 [04:30<01:58,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:46:53,768 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:46:53,772 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:46:53,774 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:46:53,776 >>   Batch size = 8
+{'eval_loss': 1.7602332830429077, 'eval_accuracy': 0.5507246376811594, 'eval_runtime': 1.4271, 'eval_samples_per_second': 145.049, 'eval_steps_per_second': 18.219, 'epoch': 19.0}
+{'loss': 0.135, 'grad_norm': 4.990705966949463, 'learning_rate': 5.125e-06, 'epoch': 19.5}
+{'loss': 0.1223, 'grad_norm': 3.0085973739624023, 'learning_rate': 5e-06, 'epoch': 20.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.02it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.92it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.31it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.60it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.27it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 18.99it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.82it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.67it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.56it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.56it/s][A                                                 
+                                               [A 50%|█████     | 400/800 [04:32<01:58,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.56it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:46:55,225 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-400
+[INFO|configuration_utils.py:471] 2024-05-15 00:46:55,232 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-400/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:46:57,216 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-400/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:46:57,225 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-400/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:46:57,231 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-400/special_tokens_map.json
+ 50%|█████     | 401/800 [04:38<16:27,  2.48s/it] 50%|█████     | 402/800 [04:38<12:05,  1.82s/it] 50%|█████     | 403/800 [04:38<09:02,  1.37s/it] 50%|█████     | 404/800 [04:39<06:54,  1.05s/it] 51%|█████     | 405/800 [04:39<05:25,  1.21it/s] 51%|█████     | 406/800 [04:39<04:22,  1.50it/s] 51%|█████     | 407/800 [04:39<03:39,  1.79it/s] 51%|█████     | 408/800 [04:40<03:08,  2.08it/s] 51%|█████     | 409/800 [04:40<02:47,  2.34it/s] 51%|█████▏    | 410/800 [04:40<02:32,  2.56it/s]                                                  51%|█████▏    | 410/800 [04:40<02:32,  2.56it/s] 51%|█████▏    | 411/800 [04:41<02:22,  2.74it/s] 52%|█████▏    | 412/800 [04:41<02:14,  2.89it/s] 52%|█████▏    | 413/800 [04:41<02:08,  3.00it/s] 52%|█████▏    | 414/800 [04:42<02:05,  3.08it/s] 52%|█████▏    | 415/800 [04:42<02:02,  3.15it/s] 52%|█████▏    | 416/800 [04:42<02:00,  3.20it/s] 52%|█████▏    | 417/800 [04:43<01:58,  3.22it/s] 52%|█████▏    | 418/800 [04:43<01:57,  3.25it/s] 52%|█████▏    | 419/800 [04:43<01:56,  3.27it/s] 52%|█████▎    | 420/800 [04:43<01:52,  3.37it/s]                                                  52%|█████▎    | 420/800 [04:43<01:52,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:47:07,045 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:47:07,049 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:47:07,051 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:47:07,052 >>   Batch size = 8
+{'eval_loss': 1.7330666780471802, 'eval_accuracy': 0.5507246376811594, 'eval_runtime': 1.4268, 'eval_samples_per_second': 145.085, 'eval_steps_per_second': 18.223, 'epoch': 20.0}
+{'loss': 0.12, 'grad_norm': 3.028963088989258, 'learning_rate': 4.875e-06, 'epoch': 20.5}
+{'loss': 0.0943, 'grad_norm': 4.148632049560547, 'learning_rate': 4.75e-06, 'epoch': 21.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.28it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.94it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.32it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.64it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.27it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.01it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.85it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.72it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.64it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.62it/s][A                                                 
+                                               [A 52%|█████▎    | 420/800 [04:45<01:52,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.62it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:47:08,497 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-420
+[INFO|configuration_utils.py:471] 2024-05-15 00:47:08,504 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-420/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:47:10,484 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-420/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:47:10,493 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-420/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:47:10,499 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-420/special_tokens_map.json
+ 53%|█████▎    | 421/800 [04:51<15:44,  2.49s/it] 53%|█████▎    | 422/800 [04:51<11:32,  1.83s/it] 53%|█████▎    | 423/800 [04:52<08:37,  1.37s/it] 53%|█████▎    | 424/800 [04:52<06:35,  1.05s/it] 53%|█████▎    | 425/800 [04:52<05:10,  1.21it/s] 53%|█████▎    | 426/800 [04:53<04:10,  1.49it/s] 53%|█████▎    | 427/800 [04:53<03:28,  1.79it/s] 54%|█████▎    | 428/800 [04:53<02:59,  2.08it/s] 54%|█████▎    | 429/800 [04:53<02:38,  2.34it/s] 54%|█████▍    | 430/800 [04:54<02:24,  2.56it/s]                                                  54%|█████▍    | 430/800 [04:54<02:24,  2.56it/s] 54%|█████▍    | 431/800 [04:54<02:14,  2.74it/s] 54%|█████▍    | 432/800 [04:54<02:07,  2.88it/s] 54%|█████▍    | 433/800 [04:55<02:02,  3.01it/s] 54%|█████▍    | 434/800 [04:55<01:58,  3.09it/s] 54%|█████▍    | 435/800 [04:55<01:55,  3.15it/s] 55%|█████▍    | 436/800 [04:56<01:53,  3.20it/s] 55%|█████▍    | 437/800 [04:56<01:52,  3.23it/s] 55%|█████▍    | 438/800 [04:56<01:51,  3.25it/s] 55%|█████▍    | 439/800 [04:56<01:50,  3.27it/s] 55%|█████▌    | 440/800 [04:57<01:46,  3.38it/s]                                                  55%|█████▌    | 440/800 [04:57<01:46,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:47:20,365 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:47:20,368 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:47:20,370 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:47:20,372 >>   Batch size = 8
+{'eval_loss': 1.7419999837875366, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4232, 'eval_samples_per_second': 145.442, 'eval_steps_per_second': 18.268, 'epoch': 21.0}
+{'loss': 0.0891, 'grad_norm': 2.853480815887451, 'learning_rate': 4.625000000000001e-06, 'epoch': 21.5}
+{'loss': 0.0888, 'grad_norm': 2.53889799118042, 'learning_rate': 4.5e-06, 'epoch': 22.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.06it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.97it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.36it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.71it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.34it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.10it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.94it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.82it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.76it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.67it/s][A                                                 
+                                               [A 55%|█████▌    | 440/800 [04:58<01:46,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.67it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:47:21,809 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-440
+[INFO|configuration_utils.py:471] 2024-05-15 00:47:21,815 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-440/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:47:23,815 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-440/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:47:23,824 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-440/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:47:23,830 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-440/special_tokens_map.json
+ 55%|█████▌    | 441/800 [05:04<14:52,  2.49s/it] 55%|█████▌    | 442/800 [05:05<10:55,  1.83s/it] 55%|█████▌    | 443/800 [05:05<08:09,  1.37s/it] 56%|█████▌    | 444/800 [05:05<06:14,  1.05s/it] 56%|█████▌    | 445/800 [05:06<04:53,  1.21it/s] 56%|█████▌    | 446/800 [05:06<03:56,  1.49it/s] 56%|█████▌    | 447/800 [05:06<03:17,  1.79it/s] 56%|█████▌    | 448/800 [05:06<02:49,  2.07it/s] 56%|█████▌    | 449/800 [05:07<02:30,  2.34it/s] 56%|█████▋    | 450/800 [05:07<02:16,  2.56it/s]                                                  56%|█████▋    | 450/800 [05:07<02:16,  2.56it/s] 56%|█████▋    | 451/800 [05:07<02:07,  2.73it/s] 56%|█████▋    | 452/800 [05:08<02:00,  2.89it/s] 57%|█████▋    | 453/800 [05:08<01:55,  3.00it/s] 57%|█████▋    | 454/800 [05:08<01:52,  3.09it/s] 57%|█████▋    | 455/800 [05:09<01:49,  3.14it/s] 57%|█████▋    | 456/800 [05:09<01:47,  3.20it/s] 57%|█████▋    | 457/800 [05:09<01:46,  3.23it/s] 57%|█████▋    | 458/800 [05:09<01:45,  3.25it/s] 57%|█████▋    | 459/800 [05:10<01:44,  3.27it/s] 57%|█████▊    | 460/800 [05:10<01:40,  3.38it/s]                                                  57%|█████▊    | 460/800 [05:10<01:40,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:47:33,680 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:47:33,683 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:47:33,685 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:47:33,687 >>   Batch size = 8
+{'eval_loss': 1.7405375242233276, 'eval_accuracy': 0.5410628019323671, 'eval_runtime': 1.4165, 'eval_samples_per_second': 146.133, 'eval_steps_per_second': 18.355, 'epoch': 22.0}
+{'loss': 0.0721, 'grad_norm': 1.9914536476135254, 'learning_rate': 4.3750000000000005e-06, 'epoch': 22.5}
+{'loss': 0.0689, 'grad_norm': 2.5989325046539307, 'learning_rate': 4.25e-06, 'epoch': 23.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.07it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.78it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.28it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.68it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.40it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.19it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.98it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.87it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.81it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.70it/s][A                                                 
+                                               [A 57%|█████▊    | 460/800 [05:11<01:40,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.70it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:47:35,124 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-460
+[INFO|configuration_utils.py:471] 2024-05-15 00:47:35,129 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-460/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:47:37,075 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-460/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:47:37,083 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-460/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:47:37,088 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-460/special_tokens_map.json
+ 58%|█████▊    | 461/800 [05:17<13:45,  2.44s/it] 58%|█████▊    | 462/800 [05:18<10:06,  1.79s/it] 58%|█████▊    | 463/800 [05:18<07:33,  1.35s/it] 58%|█████▊    | 464/800 [05:18<05:46,  1.03s/it] 58%|█████▊    | 465/800 [05:19<04:32,  1.23it/s] 58%|█████▊    | 466/800 [05:19<03:40,  1.52it/s] 58%|█████▊    | 467/800 [05:19<03:03,  1.81it/s] 58%|█████▊    | 468/800 [05:20<02:38,  2.09it/s] 59%|█████▊    | 469/800 [05:20<02:20,  2.35it/s] 59%|█████▉    | 470/800 [05:20<02:08,  2.58it/s]                                                  59%|█████▉    | 470/800 [05:20<02:08,  2.58it/s] 59%|█████▉    | 471/800 [05:20<01:59,  2.75it/s] 59%|█████▉    | 472/800 [05:21<01:53,  2.90it/s] 59%|█████▉    | 473/800 [05:21<01:48,  3.01it/s] 59%|█████▉    | 474/800 [05:21<01:45,  3.09it/s] 59%|█████▉    | 475/800 [05:22<01:43,  3.15it/s] 60%|█████▉    | 476/800 [05:22<01:41,  3.20it/s] 60%|█████▉    | 477/800 [05:22<01:40,  3.23it/s] 60%|█████▉    | 478/800 [05:23<01:39,  3.25it/s] 60%|█████▉    | 479/800 [05:23<01:38,  3.27it/s] 60%|██████    | 480/800 [05:23<01:34,  3.37it/s]                                                  60%|██████    | 480/800 [05:23<01:34,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:47:46,821 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:47:46,825 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:47:46,826 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:47:46,827 >>   Batch size = 8
+{'eval_loss': 1.7343838214874268, 'eval_accuracy': 0.5314009661835749, 'eval_runtime': 1.4159, 'eval_samples_per_second': 146.197, 'eval_steps_per_second': 18.363, 'epoch': 23.0}
+{'loss': 0.0621, 'grad_norm': 2.6775898933410645, 'learning_rate': 4.125e-06, 'epoch': 23.5}
+{'loss': 0.056, 'grad_norm': 1.2823213338851929, 'learning_rate': 4.000000000000001e-06, 'epoch': 24.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 28.98it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.99it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.39it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.72it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.35it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.17it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.92it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.77it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.71it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.68it/s][A                                                 
+                                               [A 60%|██████    | 480/800 [05:25<01:34,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.68it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:47:48,256 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-480
+[INFO|configuration_utils.py:471] 2024-05-15 00:47:48,260 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-480/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:47:50,151 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-480/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:47:50,159 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-480/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:47:50,164 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-480/special_tokens_map.json
+ 60%|██████    | 481/800 [05:31<12:55,  2.43s/it] 60%|██████    | 482/800 [05:31<09:29,  1.79s/it] 60%|██████    | 483/800 [05:31<07:06,  1.35s/it] 60%|██████    | 484/800 [05:31<05:26,  1.03s/it] 61%|██████    | 485/800 [05:32<04:16,  1.23it/s] 61%|██████    | 486/800 [05:32<03:27,  1.52it/s] 61%|██████    | 487/800 [05:32<02:52,  1.81it/s] 61%|██████    | 488/800 [05:33<02:28,  2.10it/s] 61%|██████    | 489/800 [05:33<02:12,  2.35it/s] 61%|██████▏   | 490/800 [05:33<02:00,  2.58it/s]                                                  61%|██████▏   | 490/800 [05:33<02:00,  2.58it/s] 61%|██████▏   | 491/800 [05:34<01:52,  2.75it/s] 62%|██████▏   | 492/800 [05:34<01:46,  2.90it/s] 62%|██████▏   | 493/800 [05:34<01:41,  3.01it/s] 62%|██████▏   | 494/800 [05:35<01:38,  3.09it/s] 62%|██████▏   | 495/800 [05:35<01:36,  3.16it/s] 62%|██████▏   | 496/800 [05:35<01:35,  3.20it/s] 62%|██████▏   | 497/800 [05:35<01:33,  3.23it/s] 62%|██████▏   | 498/800 [05:36<01:32,  3.25it/s] 62%|██████▏   | 499/800 [05:36<01:32,  3.27it/s] 62%|██████▎   | 500/800 [05:36<01:28,  3.38it/s]                                                  62%|██████▎   | 500/800 [05:36<01:28,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:47:59,949 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:47:59,953 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:47:59,955 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:47:59,957 >>   Batch size = 8
+{'eval_loss': 1.7349575757980347, 'eval_accuracy': 0.5555555555555556, 'eval_runtime': 1.412, 'eval_samples_per_second': 146.602, 'eval_steps_per_second': 18.414, 'epoch': 24.0}
+{'loss': 0.0548, 'grad_norm': 2.415985345840454, 'learning_rate': 3.875e-06, 'epoch': 24.5}
+{'loss': 0.0465, 'grad_norm': 7.516856670379639, 'learning_rate': 3.7500000000000005e-06, 'epoch': 25.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.18it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.97it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.37it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.68it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.29it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.05it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.87it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.74it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.64it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.61it/s][A                                                 
+                                               [A 62%|██████▎   | 500/800 [05:38<01:28,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.61it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:48:01,404 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-500
+[INFO|configuration_utils.py:471] 2024-05-15 00:48:01,411 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-500/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:48:03,420 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-500/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:48:03,429 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-500/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:48:03,435 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-500/special_tokens_map.json
+ 63%|██████▎   | 501/800 [05:44<12:24,  2.49s/it] 63%|██████▎   | 502/800 [05:44<09:06,  1.83s/it] 63%|██████▎   | 503/800 [05:45<06:48,  1.37s/it] 63%|██████▎   | 504/800 [05:45<05:11,  1.05s/it] 63%|██████▎   | 505/800 [05:45<04:03,  1.21it/s] 63%|██████▎   | 506/800 [05:45<03:16,  1.49it/s] 63%|██████▎   | 507/800 [05:46<02:43,  1.79it/s] 64%|██████▎   | 508/800 [05:46<02:20,  2.08it/s] 64%|██████▎   | 509/800 [05:46<02:04,  2.34it/s] 64%|██████▍   | 510/800 [05:47<01:52,  2.57it/s]                                                  64%|██████▍   | 510/800 [05:47<01:52,  2.57it/s] 64%|██████▍   | 511/800 [05:47<01:45,  2.74it/s] 64%|██████▍   | 512/800 [05:47<01:39,  2.89it/s] 64%|██████▍   | 513/800 [05:48<01:35,  3.00it/s] 64%|██████▍   | 514/800 [05:48<01:32,  3.09it/s] 64%|██████▍   | 515/800 [05:48<01:30,  3.16it/s] 64%|██████▍   | 516/800 [05:48<01:28,  3.20it/s] 65%|██████▍   | 517/800 [05:49<01:27,  3.23it/s] 65%|██████▍   | 518/800 [05:49<01:26,  3.25it/s] 65%|██████▍   | 519/800 [05:49<01:25,  3.27it/s] 65%|██████▌   | 520/800 [05:50<01:22,  3.38it/s]                                                  65%|██████▌   | 520/800 [05:50<01:22,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:48:13,266 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:48:13,270 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:48:13,271 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:48:13,273 >>   Batch size = 8
+{'eval_loss': 1.7352557182312012, 'eval_accuracy': 0.5458937198067633, 'eval_runtime': 1.4231, 'eval_samples_per_second': 145.455, 'eval_steps_per_second': 18.27, 'epoch': 25.0}
+{'loss': 0.0432, 'grad_norm': 0.8870682120323181, 'learning_rate': 3.625e-06, 'epoch': 25.5}
+{'loss': 0.0413, 'grad_norm': 2.2859182357788086, 'learning_rate': 3.5e-06, 'epoch': 26.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 28.96it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.95it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.33it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.71it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.32it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.09it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.89it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.73it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.62it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.60it/s][A                                                 
+                                               [A 65%|██████▌   | 520/800 [05:51<01:22,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.60it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:48:14,719 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-520
+[INFO|configuration_utils.py:471] 2024-05-15 00:48:14,726 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-520/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:48:16,723 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-520/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:48:16,731 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-520/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:48:16,737 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-520/special_tokens_map.json
+ 65%|██████▌   | 521/800 [05:57<11:34,  2.49s/it] 65%|██████▌   | 522/800 [05:58<08:29,  1.83s/it] 65%|██████▌   | 523/800 [05:58<06:20,  1.37s/it] 66%|██████▌   | 524/800 [05:58<04:50,  1.05s/it] 66%|██████▌   | 525/800 [05:58<03:47,  1.21it/s] 66%|██████▌   | 526/800 [05:59<03:03,  1.49it/s] 66%|██████▌   | 527/800 [05:59<02:32,  1.79it/s] 66%|██████▌   | 528/800 [05:59<02:11,  2.08it/s] 66%|██████▌   | 529/800 [06:00<01:55,  2.34it/s] 66%|██████▋   | 530/800 [06:00<01:45,  2.57it/s]                                                  66%|██████▋   | 530/800 [06:00<01:45,  2.57it/s] 66%|██████▋   | 531/800 [06:00<01:38,  2.74it/s] 66%|██████▋   | 532/800 [06:01<01:32,  2.89it/s] 67%|██████▋   | 533/800 [06:01<01:28,  3.01it/s] 67%|██████▋   | 534/800 [06:01<01:26,  3.09it/s] 67%|██████▋   | 535/800 [06:01<01:23,  3.16it/s] 67%|██████▋   | 536/800 [06:02<01:22,  3.20it/s] 67%|██████▋   | 537/800 [06:02<01:21,  3.23it/s] 67%|██████▋   | 538/800 [06:02<01:20,  3.25it/s] 67%|██████▋   | 539/800 [06:03<01:19,  3.27it/s] 68%|██████▊   | 540/800 [06:03<01:17,  3.37it/s]                                                  68%|██████▊   | 540/800 [06:03<01:17,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:48:26,584 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:48:26,588 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:48:26,590 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:48:26,592 >>   Batch size = 8
+{'eval_loss': 1.7623463869094849, 'eval_accuracy': 0.5507246376811594, 'eval_runtime': 1.421, 'eval_samples_per_second': 145.669, 'eval_steps_per_second': 18.297, 'epoch': 26.0}
+{'loss': 0.0363, 'grad_norm': 1.4780619144439697, 'learning_rate': 3.3750000000000003e-06, 'epoch': 26.5}
+{'loss': 0.0354, 'grad_norm': 1.2608630657196045, 'learning_rate': 3.2500000000000002e-06, 'epoch': 27.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.24it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.92it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.37it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.71it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.32it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.07it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.88it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.76it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.68it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.60it/s][A                                                 
+                                               [A 68%|██████▊   | 540/800 [06:04<01:17,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.60it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:48:28,039 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540
+[INFO|configuration_utils.py:471] 2024-05-15 00:48:28,046 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:48:30,071 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:48:30,079 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:48:30,085 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540/special_tokens_map.json
+ 68%|██████▊   | 541/800 [06:11<10:47,  2.50s/it] 68%|██████▊   | 542/800 [06:11<07:54,  1.84s/it] 68%|██████▊   | 543/800 [06:11<05:54,  1.38s/it] 68%|██████▊   | 544/800 [06:11<04:30,  1.05s/it] 68%|██████▊   | 545/800 [06:12<03:31,  1.21it/s] 68%|██████▊   | 546/800 [06:12<02:50,  1.49it/s] 68%|██████▊   | 547/800 [06:12<02:21,  1.79it/s] 68%|██████▊   | 548/800 [06:13<02:01,  2.07it/s] 69%|██████▊   | 549/800 [06:13<01:47,  2.33it/s] 69%|██████▉   | 550/800 [06:13<01:37,  2.56it/s]                                                  69%|██████▉   | 550/800 [06:13<01:37,  2.56it/s] 69%|██████▉   | 551/800 [06:14<01:31,  2.73it/s] 69%|██████▉   | 552/800 [06:14<01:26,  2.88it/s] 69%|██████▉   | 553/800 [06:14<01:22,  3.00it/s] 69%|██████▉   | 554/800 [06:14<01:19,  3.09it/s] 69%|██████▉   | 555/800 [06:15<01:17,  3.15it/s] 70%|██████▉   | 556/800 [06:15<01:16,  3.19it/s] 70%|██████▉   | 557/800 [06:15<01:15,  3.23it/s] 70%|██████▉   | 558/800 [06:16<01:14,  3.25it/s] 70%|██████▉   | 559/800 [06:16<01:13,  3.27it/s] 70%|███████   | 560/800 [06:16<01:11,  3.37it/s]                                                  70%|███████   | 560/800 [06:16<01:11,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:48:39,935 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:48:39,938 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:48:39,940 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:48:39,941 >>   Batch size = 8
+{'eval_loss': 1.7236655950546265, 'eval_accuracy': 0.5507246376811594, 'eval_runtime': 1.4232, 'eval_samples_per_second': 145.444, 'eval_steps_per_second': 18.268, 'epoch': 27.0}
+{'loss': 0.0317, 'grad_norm': 1.5162782669067383, 'learning_rate': 3.125e-06, 'epoch': 27.5}
+{'loss': 0.0319, 'grad_norm': 1.3278621435165405, 'learning_rate': 3e-06, 'epoch': 28.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.19it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.97it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.34it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.64it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.31it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.09it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.89it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.75it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.66it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.55it/s][A                                                 
+                                               [A 70%|███████   | 560/800 [06:18<01:11,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.55it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:48:41,389 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-560
+[INFO|configuration_utils.py:471] 2024-05-15 00:48:41,415 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-560/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:48:43,360 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-560/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:48:43,369 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-560/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:48:43,375 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-560/special_tokens_map.json
+ 70%|███████   | 561/800 [06:24<09:48,  2.46s/it] 70%|███████   | 562/800 [06:24<07:11,  1.81s/it] 70%|███████   | 563/800 [06:24<05:22,  1.36s/it] 70%|███████   | 564/800 [06:25<04:06,  1.04s/it] 71%|███████   | 565/800 [06:25<03:12,  1.22it/s] 71%|███████   | 566/800 [06:25<02:35,  1.50it/s] 71%|███████   | 567/800 [06:26<02:09,  1.80it/s] 71%|███████   | 568/800 [06:26<01:51,  2.08it/s] 71%|███████   | 569/800 [06:26<01:38,  2.34it/s] 71%|███████▏  | 570/800 [06:27<01:29,  2.57it/s]                                                  71%|███████▏  | 570/800 [06:27<01:29,  2.57it/s] 71%|███████▏  | 571/800 [06:27<01:23,  2.74it/s] 72%|███████▏  | 572/800 [06:27<01:18,  2.89it/s] 72%|███████▏  | 573/800 [06:27<01:15,  3.00it/s] 72%|███████▏  | 574/800 [06:28<01:13,  3.09it/s] 72%|███████▏  | 575/800 [06:28<01:11,  3.15it/s] 72%|███████▏  | 576/800 [06:28<01:10,  3.20it/s] 72%|███████▏  | 577/800 [06:29<01:09,  3.23it/s] 72%|███████▏  | 578/800 [06:29<01:08,  3.25it/s] 72%|███████▏  | 579/800 [06:29<01:07,  3.27it/s] 72%|███████▎  | 580/800 [06:30<01:05,  3.38it/s]                                                  72%|███████▎  | 580/800 [06:30<01:05,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:48:53,171 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:48:53,175 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:48:53,177 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:48:53,179 >>   Batch size = 8
+{'eval_loss': 1.7699817419052124, 'eval_accuracy': 0.5507246376811594, 'eval_runtime': 1.4241, 'eval_samples_per_second': 145.351, 'eval_steps_per_second': 18.257, 'epoch': 28.0}
+{'loss': 0.028, 'grad_norm': 0.7994670271873474, 'learning_rate': 2.875e-06, 'epoch': 28.5}
+{'loss': 0.0281, 'grad_norm': 0.7506239414215088, 'learning_rate': 2.7500000000000004e-06, 'epoch': 29.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.07it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.94it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.37it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.72it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.34it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.10it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.92it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.79it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.66it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.60it/s][A                                                 
+                                               [A 72%|███████▎  | 580/800 [06:31<01:05,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.60it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:48:54,631 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-580
+[INFO|configuration_utils.py:471] 2024-05-15 00:48:54,637 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-580/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:48:56,709 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-580/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:48:56,718 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-580/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:48:56,723 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-580/special_tokens_map.json
+ 73%|███████▎  | 581/800 [06:37<09:11,  2.52s/it] 73%|███████▎  | 582/800 [06:38<06:44,  1.85s/it] 73%|███████▎  | 583/800 [06:38<05:01,  1.39s/it] 73%|███████▎  | 584/800 [06:38<03:49,  1.06s/it] 73%|███████▎  | 585/800 [06:38<02:59,  1.20it/s] 73%|███████▎  | 586/800 [06:39<02:24,  1.48it/s] 73%|███████▎  | 587/800 [06:39<01:59,  1.78it/s] 74%|███████▎  | 588/800 [06:39<01:42,  2.06it/s] 74%|███████▎  | 589/800 [06:40<01:30,  2.33it/s] 74%|███████▍  | 590/800 [06:40<01:22,  2.56it/s]                                                  74%|███████▍  | 590/800 [06:40<01:22,  2.56it/s] 74%|███████▍  | 591/800 [06:40<01:16,  2.73it/s] 74%|███████▍  | 592/800 [06:41<01:12,  2.88it/s] 74%|███████▍  | 593/800 [06:41<01:09,  3.00it/s] 74%|███████▍  | 594/800 [06:41<01:06,  3.09it/s] 74%|███████▍  | 595/800 [06:41<01:05,  3.15it/s] 74%|███████▍  | 596/800 [06:42<01:03,  3.20it/s] 75%|███████▍  | 597/800 [06:42<01:02,  3.23it/s] 75%|███████▍  | 598/800 [06:42<01:02,  3.25it/s] 75%|███████▍  | 599/800 [06:43<01:01,  3.27it/s] 75%|███████▌  | 600/800 [06:43<00:59,  3.37it/s]                                                  75%|███████▌  | 600/800 [06:43<00:59,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:49:06,592 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:49:06,596 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:49:06,598 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:49:06,600 >>   Batch size = 8
+{'eval_loss': 1.7402682304382324, 'eval_accuracy': 0.5458937198067633, 'eval_runtime': 1.4309, 'eval_samples_per_second': 144.66, 'eval_steps_per_second': 18.17, 'epoch': 29.0}
+{'loss': 0.025, 'grad_norm': 0.9882813096046448, 'learning_rate': 2.6250000000000003e-06, 'epoch': 29.5}
+{'loss': 0.0234, 'grad_norm': 0.6024483442306519, 'learning_rate': 2.5e-06, 'epoch': 30.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.19it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 22.02it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.37it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.66it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.26it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.08it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.90it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.78it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.70it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.64it/s][A                                                 
+                                               [A 75%|███████▌  | 600/800 [06:44<00:59,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.64it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:49:08,062 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-600
+[INFO|configuration_utils.py:471] 2024-05-15 00:49:08,069 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-600/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:49:10,068 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-600/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:49:10,078 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-600/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:49:10,083 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-600/special_tokens_map.json
+ 75%|███████▌  | 601/800 [06:51<08:36,  2.60s/it] 75%|███████▌  | 602/800 [06:51<06:17,  1.91s/it] 75%|███████▌  | 603/800 [06:52<04:40,  1.43s/it] 76%|███████▌  | 604/800 [06:52<03:33,  1.09s/it] 76%|███████▌  | 605/800 [06:52<02:46,  1.17it/s] 76%|███████▌  | 606/800 [06:52<02:13,  1.45it/s] 76%|███████▌  | 607/800 [06:53<01:50,  1.75it/s] 76%|███████▌  | 608/800 [06:53<01:34,  2.04it/s] 76%|███████▌  | 609/800 [06:53<01:22,  2.30it/s] 76%|███████▋  | 610/800 [06:54<01:14,  2.53it/s]                                                  76%|███████▋  | 610/800 [06:54<01:14,  2.53it/s] 76%|███████▋  | 611/800 [06:54<01:09,  2.71it/s] 76%|███████▋  | 612/800 [06:54<01:05,  2.87it/s] 77%|███████▋  | 613/800 [06:55<01:02,  2.99it/s] 77%|███████▋  | 614/800 [06:55<01:00,  3.08it/s] 77%|███████▋  | 615/800 [06:55<00:58,  3.14it/s] 77%|███████▋  | 616/800 [06:55<00:57,  3.19it/s] 77%|███████▋  | 617/800 [06:56<00:56,  3.23it/s] 77%|███████▋  | 618/800 [06:56<00:56,  3.25it/s] 77%|███████▋  | 619/800 [06:56<00:55,  3.27it/s] 78%|███████▊  | 620/800 [06:57<00:53,  3.37it/s]                                                  78%|███████▊  | 620/800 [06:57<00:53,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:49:20,270 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:49:20,275 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:49:20,277 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:49:20,278 >>   Batch size = 8
+{'eval_loss': 1.788109540939331, 'eval_accuracy': 0.5458937198067633, 'eval_runtime': 1.4411, 'eval_samples_per_second': 143.635, 'eval_steps_per_second': 18.041, 'epoch': 30.0}
+{'loss': 0.023, 'grad_norm': 1.0393989086151123, 'learning_rate': 2.375e-06, 'epoch': 30.5}
+{'loss': 0.024, 'grad_norm': 0.44185805320739746, 'learning_rate': 2.25e-06, 'epoch': 31.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.18it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.93it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.33it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.63it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.27it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.06it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.90it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.78it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.66it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.61it/s][A                                                 
+                                               [A 78%|███████▊  | 620/800 [06:58<00:53,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.61it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:49:21,723 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-620
+[INFO|configuration_utils.py:471] 2024-05-15 00:49:21,729 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-620/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:49:23,713 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-620/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:49:23,721 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-620/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:49:23,726 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-620/special_tokens_map.json
+ 78%|███████▊  | 621/800 [07:04<07:25,  2.49s/it] 78%|███████▊  | 622/800 [07:05<05:26,  1.83s/it] 78%|███████▊  | 623/800 [07:05<04:02,  1.37s/it] 78%|███████▊  | 624/800 [07:05<03:05,  1.05s/it] 78%|███████▊  | 625/800 [07:05<02:24,  1.21it/s] 78%|███████▊  | 626/800 [07:06<01:56,  1.49it/s] 78%|███████▊  | 627/800 [07:06<01:36,  1.79it/s] 78%|███████▊  | 628/800 [07:06<01:22,  2.07it/s] 79%|███████▊  | 629/800 [07:07<01:13,  2.34it/s] 79%|███████▉  | 630/800 [07:07<01:06,  2.56it/s]                                                  79%|███████▉  | 630/800 [07:07<01:06,  2.56it/s] 79%|███████▉  | 631/800 [07:07<01:01,  2.74it/s] 79%|███████▉  | 632/800 [07:08<00:58,  2.88it/s] 79%|███████▉  | 633/800 [07:08<00:55,  3.00it/s] 79%|███████▉  | 634/800 [07:08<00:53,  3.08it/s] 79%|███████▉  | 635/800 [07:08<00:52,  3.15it/s] 80%|███████▉  | 636/800 [07:09<00:51,  3.19it/s] 80%|███████▉  | 637/800 [07:09<00:50,  3.22it/s] 80%|███████▉  | 638/800 [07:09<00:49,  3.25it/s] 80%|███████▉  | 639/800 [07:10<00:49,  3.27it/s] 80%|████████  | 640/800 [07:10<00:47,  3.37it/s]                                                  80%|████████  | 640/800 [07:10<00:47,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:49:33,592 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:49:33,596 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:49:33,598 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:49:33,599 >>   Batch size = 8
+{'eval_loss': 1.7538292407989502, 'eval_accuracy': 0.5555555555555556, 'eval_runtime': 1.4235, 'eval_samples_per_second': 145.415, 'eval_steps_per_second': 18.265, 'epoch': 31.0}
+{'loss': 0.0235, 'grad_norm': 0.9507629871368408, 'learning_rate': 2.125e-06, 'epoch': 31.5}
+{'loss': 0.0196, 'grad_norm': 0.4601610004901886, 'learning_rate': 2.0000000000000003e-06, 'epoch': 32.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.23it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.98it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.39it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.70it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.33it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.07it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.89it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.80it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.73it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.69it/s][A                                                 
+                                               [A 80%|████████  | 640/800 [07:11<00:47,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.69it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:49:35,042 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-640
+[INFO|configuration_utils.py:471] 2024-05-15 00:49:35,049 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-640/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:49:37,046 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-640/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:49:37,054 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-640/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:49:37,060 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-640/special_tokens_map.json
+ 80%|████████  | 641/800 [07:18<06:35,  2.49s/it] 80%|████████  | 642/800 [07:18<04:49,  1.83s/it] 80%|████████  | 643/800 [07:18<03:35,  1.37s/it] 80%|████████  | 644/800 [07:18<02:43,  1.05s/it] 81%|████████  | 645/800 [07:19<02:08,  1.21it/s] 81%|████████  | 646/800 [07:19<01:42,  1.50it/s] 81%|████████  | 647/800 [07:19<01:25,  1.79it/s] 81%|████████  | 648/800 [07:20<01:13,  2.08it/s] 81%|████████  | 649/800 [07:20<01:04,  2.34it/s] 81%|████████▏ | 650/800 [07:20<00:58,  2.56it/s]                                                  81%|████████▏ | 650/800 [07:20<00:58,  2.56it/s] 81%|████████▏ | 651/800 [07:21<00:54,  2.73it/s] 82%|████████▏ | 652/800 [07:21<00:51,  2.88it/s] 82%|████████▏ | 653/800 [07:21<00:48,  3.00it/s] 82%|████████▏ | 654/800 [07:21<00:47,  3.08it/s] 82%|████████▏ | 655/800 [07:22<00:46,  3.15it/s] 82%|████████▏ | 656/800 [07:22<00:45,  3.20it/s] 82%|████████▏ | 657/800 [07:22<00:44,  3.23it/s] 82%|████████▏ | 658/800 [07:23<00:43,  3.25it/s] 82%|████████▏ | 659/800 [07:23<00:43,  3.27it/s] 82%|████████▎ | 660/800 [07:23<00:41,  3.37it/s]                                                  82%|████████▎ | 660/800 [07:23<00:41,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:49:46,911 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:49:46,915 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:49:46,916 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:49:46,918 >>   Batch size = 8
+{'eval_loss': 1.755861759185791, 'eval_accuracy': 0.5555555555555556, 'eval_runtime': 1.4199, 'eval_samples_per_second': 145.785, 'eval_steps_per_second': 18.311, 'epoch': 32.0}
+{'loss': 0.0199, 'grad_norm': 1.0189006328582764, 'learning_rate': 1.8750000000000003e-06, 'epoch': 32.5}
+{'loss': 0.0208, 'grad_norm': 0.5331593155860901, 'learning_rate': 1.75e-06, 'epoch': 33.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.15it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.81it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.26it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.62it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.33it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.07it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.84it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.70it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.64it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.60it/s][A                                                 
+                                               [A 82%|████████▎ | 660/800 [07:25<00:41,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.60it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:49:48,362 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-660
+[INFO|configuration_utils.py:471] 2024-05-15 00:49:48,369 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-660/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:49:50,341 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-660/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:49:50,350 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-660/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:49:50,355 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-660/special_tokens_map.json
+ 83%|████████▎ | 661/800 [07:31<05:43,  2.47s/it] 83%|████████▎ | 662/800 [07:31<04:11,  1.82s/it] 83%|████████▎ | 663/800 [07:31<03:07,  1.37s/it] 83%|████████▎ | 664/800 [07:32<02:22,  1.05s/it] 83%|████████▎ | 665/800 [07:32<01:51,  1.21it/s] 83%|████████▎ | 666/800 [07:32<01:29,  1.50it/s] 83%|████████▎ | 667/800 [07:33<01:14,  1.80it/s] 84%|████████▎ | 668/800 [07:33<01:03,  2.08it/s] 84%|████████▎ | 669/800 [07:33<00:55,  2.34it/s] 84%|████████▍ | 670/800 [07:34<00:50,  2.57it/s]                                                  84%|████████▍ | 670/800 [07:34<00:50,  2.57it/s] 84%|████████▍ | 671/800 [07:34<00:47,  2.74it/s] 84%|████████▍ | 672/800 [07:34<00:44,  2.89it/s] 84%|████████▍ | 673/800 [07:34<00:42,  3.00it/s] 84%|████████▍ | 674/800 [07:35<00:40,  3.09it/s] 84%|████████▍ | 675/800 [07:35<00:39,  3.15it/s] 84%|████████▍ | 676/800 [07:35<00:38,  3.20it/s] 85%|████████▍ | 677/800 [07:36<00:38,  3.23it/s] 85%|████████▍ | 678/800 [07:36<00:37,  3.25it/s] 85%|████████▍ | 679/800 [07:36<00:37,  3.27it/s] 85%|████████▌ | 680/800 [07:37<00:35,  3.37it/s]                                                  85%|████████▌ | 680/800 [07:37<00:35,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:50:00,181 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:50:00,185 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:50:00,187 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:50:00,189 >>   Batch size = 8
+{'eval_loss': 1.764967441558838, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4234, 'eval_samples_per_second': 145.427, 'eval_steps_per_second': 18.266, 'epoch': 33.0}
+{'loss': 0.0186, 'grad_norm': 0.48274943232536316, 'learning_rate': 1.6250000000000001e-06, 'epoch': 33.5}
+{'loss': 0.0186, 'grad_norm': 0.622961699962616, 'learning_rate': 1.5e-06, 'epoch': 34.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.19it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 22.03it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.42it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.73it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.34it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.10it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.91it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.77it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.66it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.59it/s][A                                                 
+                                               [A 85%|████████▌ | 680/800 [07:38<00:35,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.59it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:50:01,632 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-680
+[INFO|configuration_utils.py:471] 2024-05-15 00:50:01,657 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-680/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:50:03,658 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-680/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:50:03,666 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-680/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:50:03,672 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-680/special_tokens_map.json
+ 85%|████████▌ | 681/800 [07:44<04:55,  2.49s/it] 85%|████████▌ | 682/800 [07:44<03:35,  1.83s/it] 85%|████████▌ | 683/800 [07:45<02:40,  1.37s/it] 86%|████████▌ | 684/800 [07:45<02:01,  1.05s/it] 86%|████████▌ | 685/800 [07:45<01:34,  1.21it/s] 86%|████████▌ | 686/800 [07:46<01:16,  1.50it/s] 86%|████████▌ | 687/800 [07:46<01:03,  1.79it/s] 86%|████████▌ | 688/800 [07:46<00:53,  2.08it/s] 86%|████████▌ | 689/800 [07:47<00:47,  2.34it/s] 86%|████████▋ | 690/800 [07:47<00:42,  2.57it/s]                                                  86%|████████▋ | 690/800 [07:47<00:42,  2.57it/s] 86%|████████▋ | 691/800 [07:47<00:39,  2.74it/s] 86%|████████▋ | 692/800 [07:47<00:37,  2.89it/s] 87%|████████▋ | 693/800 [07:48<00:35,  3.00it/s] 87%|████████▋ | 694/800 [07:48<00:34,  3.09it/s] 87%|████████▋ | 695/800 [07:48<00:33,  3.15it/s] 87%|████████▋ | 696/800 [07:49<00:32,  3.19it/s] 87%|████████▋ | 697/800 [07:49<00:31,  3.23it/s] 87%|████████▋ | 698/800 [07:49<00:31,  3.26it/s] 87%|████████▋ | 699/800 [07:50<00:30,  3.27it/s] 88%|████████▊ | 700/800 [07:50<00:29,  3.38it/s]                                                  88%|████████▊ | 700/800 [07:50<00:29,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:50:13,485 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:50:13,488 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:50:13,490 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:50:13,492 >>   Batch size = 8
+{'eval_loss': 1.7733298540115356, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4216, 'eval_samples_per_second': 145.608, 'eval_steps_per_second': 18.289, 'epoch': 34.0}
+{'loss': 0.0178, 'grad_norm': 0.5946810245513916, 'learning_rate': 1.3750000000000002e-06, 'epoch': 34.5}
+{'loss': 0.0164, 'grad_norm': 0.5902403593063354, 'learning_rate': 1.25e-06, 'epoch': 35.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.03it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.93it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.32it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.65it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.28it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.05it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.85it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.74it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.66it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.62it/s][A                                                 
+                                               [A 88%|████████▊ | 700/800 [07:51<00:29,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.62it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:50:14,937 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-700
+[INFO|configuration_utils.py:471] 2024-05-15 00:50:14,944 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-700/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:50:16,939 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-700/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:50:16,949 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-700/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:50:16,955 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-700/special_tokens_map.json
+ 88%|████████▊ | 701/800 [07:58<04:20,  2.63s/it] 88%|████████▊ | 702/800 [07:58<03:09,  1.93s/it] 88%|████████▊ | 703/800 [07:59<02:19,  1.44s/it] 88%|████████▊ | 704/800 [07:59<01:45,  1.10s/it] 88%|████████▊ | 705/800 [07:59<01:21,  1.16it/s] 88%|████████▊ | 706/800 [07:59<01:05,  1.44it/s] 88%|████████▊ | 707/800 [08:00<00:53,  1.73it/s] 88%|████████▊ | 708/800 [08:00<00:45,  2.02it/s] 89%|████████▊ | 709/800 [08:00<00:39,  2.29it/s] 89%|████████▉ | 710/800 [08:01<00:35,  2.52it/s]                                                  89%|████████▉ | 710/800 [08:01<00:35,  2.52it/s] 89%|████████▉ | 711/800 [08:01<00:32,  2.71it/s] 89%|████████▉ | 712/800 [08:01<00:30,  2.86it/s] 89%|████████▉ | 713/800 [08:02<00:29,  2.98it/s] 89%|████████▉ | 714/800 [08:02<00:27,  3.08it/s] 89%|████████▉ | 715/800 [08:02<00:27,  3.14it/s] 90%|████████▉ | 716/800 [08:02<00:26,  3.19it/s] 90%|████████▉ | 717/800 [08:03<00:25,  3.23it/s] 90%|████████▉ | 718/800 [08:03<00:25,  3.25it/s] 90%|████████▉ | 719/800 [08:03<00:24,  3.27it/s] 90%|█████████ | 720/800 [08:04<00:23,  3.38it/s]                                                  90%|█████████ | 720/800 [08:04<00:23,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:50:27,272 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:50:27,276 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:50:27,277 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:50:27,279 >>   Batch size = 8
+{'eval_loss': 1.7731701135635376, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4232, 'eval_samples_per_second': 145.447, 'eval_steps_per_second': 18.269, 'epoch': 35.0}
+{'loss': 0.0171, 'grad_norm': 0.499220073223114, 'learning_rate': 1.125e-06, 'epoch': 35.5}
+{'loss': 0.0168, 'grad_norm': 1.519394040107727, 'learning_rate': 1.0000000000000002e-06, 'epoch': 36.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.26it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.79it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.26it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.61it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.25it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.06it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.86it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.73it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.63it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.56it/s][A                                                 
+                                               [A 90%|█████████ | 720/800 [08:05<00:23,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.56it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:50:28,726 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-720
+[INFO|configuration_utils.py:471] 2024-05-15 00:50:28,734 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-720/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:50:30,718 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-720/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:50:30,726 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-720/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:50:30,733 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-720/special_tokens_map.json
+ 90%|█████████ | 721/800 [08:11<03:14,  2.47s/it] 90%|█████████ | 722/800 [08:11<02:21,  1.82s/it] 90%|█████████ | 723/800 [08:12<01:44,  1.36s/it] 90%|█████████ | 724/800 [08:12<01:19,  1.04s/it] 91%|█████████ | 725/800 [08:12<01:01,  1.22it/s] 91%|█████████ | 726/800 [08:13<00:49,  1.50it/s] 91%|█████████ | 727/800 [08:13<00:40,  1.80it/s] 91%|█████████ | 728/800 [08:13<00:34,  2.09it/s] 91%|█████████ | 729/800 [08:14<00:30,  2.35it/s] 91%|█████████▏| 730/800 [08:14<00:27,  2.57it/s]                                                  91%|█████████▏| 730/800 [08:14<00:27,  2.57it/s] 91%|█████████▏| 731/800 [08:14<00:25,  2.75it/s] 92%|█████████▏| 732/800 [08:14<00:23,  2.90it/s] 92%|█████████▏| 733/800 [08:15<00:22,  3.01it/s] 92%|█████████▏| 734/800 [08:15<00:21,  3.09it/s] 92%|█████████▏| 735/800 [08:15<00:20,  3.16it/s] 92%|█████████▏| 736/800 [08:16<00:19,  3.20it/s] 92%|█████████▏| 737/800 [08:16<00:19,  3.23it/s] 92%|█████████▏| 738/800 [08:16<00:19,  3.25it/s] 92%|█████████▏| 739/800 [08:17<00:18,  3.27it/s] 92%|█████████▎| 740/800 [08:17<00:17,  3.37it/s]                                                  92%|█████████▎| 740/800 [08:17<00:17,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:50:40,512 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:50:40,516 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:50:40,517 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:50:40,519 >>   Batch size = 8
+{'eval_loss': 1.7853875160217285, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4236, 'eval_samples_per_second': 145.405, 'eval_steps_per_second': 18.263, 'epoch': 36.0}
+{'loss': 0.0156, 'grad_norm': 0.32934170961380005, 'learning_rate': 8.75e-07, 'epoch': 36.5}
+{'loss': 0.0189, 'grad_norm': 0.3674474060535431, 'learning_rate': 7.5e-07, 'epoch': 37.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.11it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.97it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.33it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.65it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.29it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.09it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.81it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.72it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.66it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.62it/s][A                                                 
+                                               [A 92%|█████████▎| 740/800 [08:18<00:17,  3.37it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.62it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:50:41,968 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-740
+[INFO|configuration_utils.py:471] 2024-05-15 00:50:41,975 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-740/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:50:43,984 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-740/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:50:43,993 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-740/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:50:43,998 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-740/special_tokens_map.json
+ 93%|█████████▎| 741/800 [08:25<02:38,  2.68s/it] 93%|█████████▎| 742/800 [08:25<01:54,  1.97s/it] 93%|█████████▎| 743/800 [08:26<01:23,  1.47s/it] 93%|█████████▎| 744/800 [08:26<01:02,  1.12s/it] 93%|█████████▎| 745/800 [08:26<00:48,  1.14it/s] 93%|█████████▎| 746/800 [08:27<00:37,  1.42it/s] 93%|█████████▎| 747/800 [08:27<00:30,  1.72it/s] 94%|█████████▎| 748/800 [08:27<00:25,  2.01it/s] 94%|█████████▎| 749/800 [08:28<00:22,  2.28it/s] 94%|█████████▍| 750/800 [08:28<00:19,  2.51it/s]                                                  94%|█████████▍| 750/800 [08:28<00:19,  2.51it/s] 94%|█████████▍| 751/800 [08:28<00:18,  2.69it/s] 94%|█████████▍| 752/800 [08:28<00:16,  2.85it/s] 94%|█████████▍| 753/800 [08:29<00:15,  2.98it/s] 94%|█████████▍| 754/800 [08:29<00:14,  3.07it/s] 94%|█████████▍| 755/800 [08:29<00:14,  3.14it/s] 94%|█████████▍| 756/800 [08:30<00:13,  3.19it/s] 95%|█████████▍| 757/800 [08:30<00:13,  3.19it/s] 95%|█████████▍| 758/800 [08:30<00:13,  3.23it/s] 95%|█████████▍| 759/800 [08:31<00:12,  3.24it/s] 95%|█████████▌| 760/800 [08:31<00:11,  3.35it/s]                                                  95%|█████████▌| 760/800 [08:31<00:11,  3.35it/s][INFO|trainer.py:765] 2024-05-15 00:50:54,493 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:50:54,497 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:50:54,499 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:50:54,501 >>   Batch size = 8
+{'eval_loss': 1.7964427471160889, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4281, 'eval_samples_per_second': 144.944, 'eval_steps_per_second': 18.206, 'epoch': 37.0}
+{'loss': 0.016, 'grad_norm': 0.3100169897079468, 'learning_rate': 6.25e-07, 'epoch': 37.5}
+{'loss': 0.0166, 'grad_norm': 0.3822616636753082, 'learning_rate': 5.000000000000001e-07, 'epoch': 38.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.27it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.96it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.35it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.64it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.27it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.09it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.90it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.72it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.61it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.54it/s][A                                                 
+                                               [A 95%|█████████▌| 760/800 [08:32<00:11,  3.35it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.54it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:50:55,949 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-760
+[INFO|configuration_utils.py:471] 2024-05-15 00:50:55,955 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-760/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:50:58,041 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-760/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:50:58,050 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-760/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:50:58,057 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-760/special_tokens_map.json
+ 95%|█████████▌| 761/800 [08:39<01:40,  2.58s/it] 95%|█████████▌| 762/800 [08:39<01:11,  1.89s/it] 95%|█████████▌| 763/800 [08:39<00:52,  1.42s/it] 96%|█████████▌| 764/800 [08:40<00:38,  1.08s/it] 96%|█████████▌| 765/800 [08:40<00:29,  1.18it/s] 96%|█████████▌| 766/800 [08:40<00:23,  1.46it/s] 96%|█████████▌| 767/800 [08:41<00:18,  1.76it/s] 96%|█████████▌| 768/800 [08:41<00:15,  2.05it/s] 96%|█████████▌| 769/800 [08:41<00:13,  2.31it/s] 96%|█████████▋| 770/800 [08:41<00:11,  2.54it/s]                                                  96%|█████████▋| 770/800 [08:41<00:11,  2.54it/s] 96%|█████████▋| 771/800 [08:42<00:10,  2.72it/s] 96%|█████████▋| 772/800 [08:42<00:09,  2.88it/s] 97%|█████████▋| 773/800 [08:42<00:09,  3.00it/s] 97%|█████████▋| 774/800 [08:43<00:08,  3.08it/s] 97%|█████████▋| 775/800 [08:43<00:07,  3.15it/s] 97%|█████████▋| 776/800 [08:43<00:07,  3.19it/s] 97%|█████████▋| 777/800 [08:44<00:07,  3.23it/s] 97%|█████████▋| 778/800 [08:44<00:06,  3.25it/s] 97%|█████████▋| 779/800 [08:44<00:06,  3.27it/s] 98%|█████████▊| 780/800 [08:44<00:05,  3.38it/s]                                                  98%|█████████▊| 780/800 [08:44<00:05,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:51:08,096 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:51:08,100 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:51:08,102 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:51:08,103 >>   Batch size = 8
+{'eval_loss': 1.7911357879638672, 'eval_accuracy': 0.5555555555555556, 'eval_runtime': 1.4269, 'eval_samples_per_second': 145.07, 'eval_steps_per_second': 18.221, 'epoch': 38.0}
+{'loss': 0.0155, 'grad_norm': 0.5930668711662292, 'learning_rate': 3.75e-07, 'epoch': 38.5}
+{'loss': 0.0163, 'grad_norm': 0.48172202706336975, 'learning_rate': 2.5000000000000004e-07, 'epoch': 39.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.14it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.96it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.34it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.68it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.32it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.07it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.90it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.77it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.69it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.64it/s][A                                                 
+                                               [A 98%|█████████▊| 780/800 [08:46<00:05,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.64it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:51:09,547 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-780
+[INFO|configuration_utils.py:471] 2024-05-15 00:51:09,570 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-780/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:51:11,584 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-780/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:51:11,593 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-780/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:51:11,599 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-780/special_tokens_map.json
+ 98%|█████████▊| 781/800 [08:52<00:48,  2.57s/it] 98%|█████████▊| 782/800 [08:53<00:33,  1.89s/it] 98%|█████████▊| 783/800 [08:53<00:24,  1.41s/it] 98%|█████████▊| 784/800 [08:53<00:17,  1.08s/it] 98%|█████████▊| 785/800 [08:54<00:12,  1.18it/s] 98%|█████████▊| 786/800 [08:54<00:09,  1.47it/s] 98%|█████████▊| 787/800 [08:54<00:07,  1.76it/s] 98%|█████████▊| 788/800 [08:54<00:05,  2.05it/s] 99%|█████████▊| 789/800 [08:55<00:04,  2.31it/s] 99%|█████████▉| 790/800 [08:55<00:03,  2.54it/s]                                                  99%|█████████▉| 790/800 [08:55<00:03,  2.54it/s] 99%|█████████▉| 791/800 [08:55<00:03,  2.72it/s] 99%|█████████▉| 792/800 [08:56<00:02,  2.88it/s] 99%|█████████▉| 793/800 [08:56<00:02,  2.99it/s] 99%|█████████▉| 794/800 [08:56<00:01,  3.08it/s] 99%|█████████▉| 795/800 [08:57<00:01,  3.15it/s]100%|█████████▉| 796/800 [08:57<00:01,  3.20it/s]100%|█████████▉| 797/800 [08:57<00:00,  3.23it/s]100%|█████████▉| 798/800 [08:57<00:00,  3.26it/s]100%|█████████▉| 799/800 [08:58<00:00,  3.27it/s]100%|██████████| 800/800 [08:58<00:00,  3.38it/s]                                                 100%|██████████| 800/800 [08:58<00:00,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:51:21,678 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:51:21,681 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:51:21,683 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:51:21,685 >>   Batch size = 8
+{'eval_loss': 1.7974388599395752, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4212, 'eval_samples_per_second': 145.648, 'eval_steps_per_second': 18.294, 'epoch': 39.0}
+{'loss': 0.0158, 'grad_norm': 0.3446134924888611, 'learning_rate': 1.2500000000000002e-07, 'epoch': 39.5}
+{'loss': 0.0162, 'grad_norm': 0.49807652831077576, 'learning_rate': 0.0, 'epoch': 40.0}
+
+  0%|          | 0/26 [00:00<?, ?it/s][A
+ 12%|█▏        | 3/26 [00:00<00:00, 29.19it/s][A
+ 23%|██▎       | 6/26 [00:00<00:00, 21.92it/s][A
+ 35%|███▍      | 9/26 [00:00<00:00, 20.29it/s][A
+ 46%|████▌     | 12/26 [00:00<00:00, 19.67it/s][A
+ 58%|█████▊    | 15/26 [00:00<00:00, 19.31it/s][A
+ 65%|██████▌   | 17/26 [00:00<00:00, 19.08it/s][A
+ 73%|███████▎  | 19/26 [00:00<00:00, 18.88it/s][A
+ 81%|████████  | 21/26 [00:01<00:00, 18.78it/s][A
+ 88%|████████▊ | 23/26 [00:01<00:00, 18.68it/s][A
+ 96%|█████████▌| 25/26 [00:01<00:00, 18.61it/s][A                                                 
+                                               [A100%|██████████| 800/800 [08:59<00:00,  3.38it/s]
+100%|██████████| 26/26 [00:01<00:00, 18.61it/s][A
+                                               [A[INFO|trainer.py:3203] 2024-05-15 00:51:23,129 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-800
+[INFO|configuration_utils.py:471] 2024-05-15 00:51:23,136 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-800/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:51:25,162 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-800/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:51:25,171 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-800/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:51:25,177 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-800/special_tokens_map.json
+[INFO|trainer.py:2231] 2024-05-15 00:51:28,898 >> 
+
+Training completed. Do not forget to share your model on huggingface.co/models =)
+
+
+[INFO|trainer.py:2436] 2024-05-15 00:51:28,900 >> Loading best model from /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540 (score: 1.7236655950546265).
+                                                 100%|██████████| 800/800 [09:12<00:00,  3.38it/s]100%|██████████| 800/800 [09:12<00:00,  1.45it/s]
+[INFO|trainer.py:3203] 2024-05-15 00:51:35,167 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20
+[INFO|configuration_utils.py:471] 2024-05-15 00:51:35,175 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/config.json
+[INFO|modeling_utils.py:2474] 2024-05-15 00:51:37,171 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/model.safetensors
+[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:51:37,213 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/tokenizer_config.json
+[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:51:37,219 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/special_tokens_map.json
+{'eval_loss': 1.7982027530670166, 'eval_accuracy': 0.5555555555555556, 'eval_runtime': 1.4224, 'eval_samples_per_second': 145.53, 'eval_steps_per_second': 18.279, 'epoch': 40.0}
+{'train_runtime': 567.4795, 'train_samples_per_second': 22.415, 'train_steps_per_second': 1.41, 'train_loss': 0.7736774892546237, 'epoch': 40.0}
+***** train metrics *****
+  epoch                    =       40.0
+  train_loss               =     0.7737
+  train_runtime            = 0:09:27.47
+  train_samples            =        318
+  train_samples_per_second =     22.415
+  train_steps_per_second   =       1.41
+05/15/2024 00:51:37 - INFO - __main__ - *** Evaluate ***
+[INFO|trainer.py:765] 2024-05-15 00:51:37,309 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
+[INFO|trainer.py:3512] 2024-05-15 00:51:37,312 >> ***** Running Evaluation *****
+[INFO|trainer.py:3514] 2024-05-15 00:51:37,314 >>   Num examples = 207
+[INFO|trainer.py:3517] 2024-05-15 00:51:37,316 >>   Batch size = 8
+  0%|          | 0/26 [00:00<?, ?it/s] 12%|█▏        | 3/26 [00:00<00:01, 17.14it/s] 19%|█▉        | 5/26 [00:00<00:01, 17.35it/s] 27%|██▋       | 7/26 [00:00<00:01, 18.06it/s] 35%|███▍      | 9/26 [00:00<00:00, 18.40it/s] 42%|████▏     | 11/26 [00:00<00:00, 18.62it/s] 50%|█████     | 13/26 [00:00<00:00, 18.77it/s] 58%|█████▊    | 15/26 [00:00<00:00, 18.86it/s] 65%|██████▌   | 17/26 [00:00<00:00, 18.93it/s] 73%|███████▎  | 19/26 [00:01<00:00, 18.97it/s] 81%|████████  | 21/26 [00:01<00:00, 18.98it/s] 88%|████████▊ | 23/26 [00:01<00:00, 19.02it/s] 96%|█████████▌| 25/26 [00:01<00:00, 19.03it/s]100%|██████████| 26/26 [00:01<00:00, 18.48it/s]
+[INFO|modelcard.py:450] 2024-05-15 00:51:38,976 >> Dropping the following result as it does not have all the necessary fields:
+{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5507246376811594}]}
+***** eval metrics *****
+  epoch                   =       40.0
+  eval_accuracy           =     0.5507
+  eval_loss               =     1.7237
+  eval_runtime            = 0:00:01.50
+  eval_samples            =        207
+  eval_samples_per_second =    137.577
+  eval_steps_per_second   =      17.28
+wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
+wandb: - 0.133 MB of 0.133 MB uploadedwandb: \ 0.133 MB of 0.226 MB uploadedwandb: | 0.226 MB of 0.226 MB uploadedwandb: / 0.226 MB of 0.226 MB uploadedwandb: 
+wandb: Run history:
+wandb:           eval/accuracy ▁▁▁▃▄▅▆▆▆▇▇▇█▇▇██▇██████████████████████
+wandb:               eval/loss ██▇▇▆▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
+wandb:            eval/runtime ▁▁▁▁▂▂▃▂▂▂▄▂▃▂▂▂▂▃▃▃▂▂▂▂▂▂▂▂▃▄▂▂▂▂▂▂▃▃▂█
+wandb: eval/samples_per_second ███▇▇▇▅▇▇▇▅▇▆▇▇▇▆▆▆▆▇▇▇▇▇▇▇▇▆▅▇▇▇▇▇▇▆▆▇▁
+wandb:   eval/steps_per_second ███▇▇▇▅▇▇▇▅▇▆▇▇▇▆▆▆▆▇▇▇▇▇▇▇▇▆▅▇▇▇▇▇▇▆▆▇▁
+wandb:             train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
+wandb:       train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
+wandb:         train/grad_norm ▄▃▅▇▇██▇██▆▆▅▅▆▄▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
+wandb:     train/learning_rate ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁
+wandb:              train/loss ███▇▇▆▅▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
+wandb: 
+wandb: Run summary:
+wandb:            eval/accuracy 0.55072
+wandb:                eval/loss 1.72367
+wandb:             eval/runtime 1.5046
+wandb:  eval/samples_per_second 137.577
+wandb:    eval/steps_per_second 17.28
+wandb:               total_flos 2963872170455040.0
+wandb:              train/epoch 40.0
+wandb:        train/global_step 800
+wandb:          train/grad_norm 0.49808
+wandb:      train/learning_rate 0.0
+wandb:               train/loss 0.0162
+wandb:               train_loss 0.77368
+wandb:            train_runtime 567.4795
+wandb: train_samples_per_second 22.415
+wandb:   train_steps_per_second 1.41
+wandb: 
+wandb: 🚀 View run reddit_roberta-large_lr1e-5_B16_E40 at: https://wandb.ai/ukp-conv/Privacy-NLP/runs/97axf02o
+wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
+wandb: Find logs at: ./wandb/run-20240515_004219-97axf02o/logs
diff --git a/programming_runs/credentials.py b/programming_runs/credentials.py
index 81c02f7..cfd63f3 100644
--- a/programming_runs/credentials.py
+++ b/programming_runs/credentials.py
@@ -1,12 +1,12 @@
-# gpt4_endpoint = "https://azure-openai-ukp-004.openai.azure.com/"
-gpt4_endpoint = "https://api.openai.com/v1"
-# gpt4_api_key = "9443b9b3e9d44a648822744086b078dd"
-gpt4_api_key = "sk-proj-xC28zQzLgo76HTYZrRaAT3BlbkFJ2Gv7gc7DqKUoLk2kaJrz"
+gpt4_endpoint = "https://azure-openai-ukp-004.openai.azure.com/"
+# gpt4_endpoint = "https://api.openai.com/v1"
+gpt4_api_key = "9443b9b3e9d44a648822744086b078dd"
+# gpt4_api_key = "sk-proj-xC28zQzLgo76HTYZrRaAT3BlbkFJ2Gv7gc7DqKUoLk2kaJrz"
 gpt4_api_version = "2023-05-15"
-# gpt4_tb_endpoint = "https://azure-openai-ukp-west-us.openai.azure.com/"
-gpt4_tb_endpoint = "https://api.openai.com/v1"
-# gpt4_tb_api_key = "228d82e8119943db9b990a25b94b6ef0"
-gpt4_tb_api_key = "sk-proj-xC28zQzLgo76HTYZrRaAT3BlbkFJ2Gv7gc7DqKUoLk2kaJrz"
+gpt4_tb_endpoint = "https://azure-openai-ukp-west-us.openai.azure.com/"
+# gpt4_tb_endpoint = "https://api.openai.com/v1"
+gpt4_tb_api_key = "228d82e8119943db9b990a25b94b6ef0"
+# gpt4_tb_api_key = "sk-proj-xC28zQzLgo76HTYZrRaAT3BlbkFJ2Gv7gc7DqKUoLk2kaJrz"
 gpt4_tb_api_version = "2023-05-15"
 gpt35_endpoint = "https://azure-openai-ukp-001.openai.azure.com/"
 gpt35_api_key = "863b6f7aa80b4e90ac941b0ad7d50c90"
diff --git a/programming_runs/generators/generator_types.py b/programming_runs/generators/generator_types.py
index d502f93..6b63cd9 100644
--- a/programming_runs/generators/generator_types.py
+++ b/programming_runs/generators/generator_types.py
@@ -41,6 +41,7 @@ class Generator:
             self,
             input_text: str,
             label: str,
+            people,
             act_model: ModelBase,
             parser_model: ModelBase,
             strategy: str,
@@ -71,5 +72,5 @@ class Generator:
         ...
 
     @abstractmethod
-    def privacy_selection_evaluation(self, model: ModelBase, rewriting, people, candidate_list):
+    def privacy_selection_evaluation(self, model: ModelBase, rewriting, original_text, people, candidate_list):
         ...
\ No newline at end of file
diff --git a/programming_runs/generators/generator_utils.py b/programming_runs/generators/generator_utils.py
index 165de7d..48d5360 100644
--- a/programming_runs/generators/generator_utils.py
+++ b/programming_runs/generators/generator_utils.py
@@ -72,6 +72,7 @@ def generic_detection(
 def generic_rewriting(
     input_text: str,
     label: str,
+    people,
     model: ModelBase,
     parser_model: ModelBase,
     strategy: str,
@@ -92,7 +93,8 @@ def generic_rewriting(
     simple_rewriting_instruction: str,
     simple_rewriting_instruction_cot: str,
     reflection_privacy_rewriting_instruction: str,
-    reinforcement_learning_instruction: str
+    reinforcement_learning_instruction: str,
+    language: str
 ):
     if strategy != "reflexion" and strategy != "simple":
         raise ValueError(
@@ -159,10 +161,31 @@ def generic_rewriting(
             if not no_utility:
                 if privacy_score == 'Yes':
                     # prev_rewriting += f"Suggestion: These entities {reflection_privacy} could be further generalized to improve the privacy score."
-                    prev_rewriting += f"Suggestion: Entities that can be used to infer the person identity could be further generalized to improve the privacy score. You can refer the following detected sensitive entity list: {reflection_privacy}"
+                    if language == 'wiki':
+                        prev_rewriting += (f"Suggestion: You should now focus on improving the privacy score by "
+                                           f"preventing the inference of the person's identify. "
+                                           f"Sensitive entities that can be used to achieve correct inference are here:"
+                                           f" {reflection_privacy}")
+                    elif language == 'reddit':
+                        mapped_feature = {
+                            "age": "Age",
+                            "sex": "Sex",
+                            "city_country": "Location",
+                            "birth_city_country": "Place of birth",
+                            "education": "Education",
+                            "income_level": "Income level",
+                            "relationship_status": "Relationship status",
+                        }
+                        prev_rewriting += (f"Suggestion: You should now focus on improving the privacy score by "
+                                           f"preventing the inference of the comment author's"
+                                           f" {mapped_feature[list(people.keys())[0]]}. "
+                                           f"Sensitive entities that can be used to achieve correct inference are here:"
+                                           f" {reflection_privacy}")
                 else:
                     # prev_rewriting += f"Suggestion: These entities {reflection_utility} could be replaced with the original specific entities in the original biography to improve the utility score. You should also avoid specifying entities that could decrease the privacy score."
-                    prev_rewriting += f"Suggestion: Entities that can help classify the person's occupation as {label} but can not be used to infer the person's identity could be specified to improve the utility score. "
+                    # prev_rewriting += f"Suggestion: Entities that can help classify the person's occupation as {label} but can not be used to infer the person's identity could be specified to improve the utility score. "
+                    prev_rewriting += (f"Suggestion: You should now improve the utility score about classifying the"
+                                       f" text as {label} without decreasing the privacy score. ")
                 response_schemas = [
                     ResponseSchema(name="Anonymized text", description="your editing result")
                 ]
@@ -442,6 +465,7 @@ def generic_privacy_confidence_evaluation(
 def generic_privacy_selection_evaluation(
         model: ModelBase,
         curr_rewriting: str,
+        original_text: str,
         people: str,
         candidate_list: str,
         general_system_instruction: str,
@@ -563,30 +587,34 @@ def reddit_privacy_reflection(
         privacy_reflection_completion_instruction_2: str
 ):
     if model.is_chat:
-        response_schemas = [
-            ResponseSchema(name="Age", description=f"the sorted list of top {p_threshold} guesses of the specific age of the "
+        response_schemas_dict = {
+            'age': ResponseSchema(name="Age", description=f"the sorted list of top {p_threshold} guesses of the specific age of the "
                                                    f"author when he wrote the comment where every two guesses are"
                                                    f" separated by \', \'"),
-            ResponseSchema(name="Sex", description=f"Choose from these options: Male , Female."),
-            ResponseSchema(name="Location", description=f"the sorted list of top {p_threshold} guesses of the current"
-                                                        f" place of living of the author where every two guesses are"
+            'sex': ResponseSchema(name="Sex", description=f"Choose from these options: Male , Female."),
+            'city_country': ResponseSchema(name="Location", description=f"the sorted list of top {p_threshold} guesses of the current"
+                                                        f" place of living of the author specific to the city where every two guesses are"
                                                         f" separated by \', \'"),
-            ResponseSchema(name="Place of birth", description=f"the sorted list of top {p_threshold} guesses of the"
-                                                              f" place of birth of the author where every two guesses "
+            'birth_city_country': ResponseSchema(name="Place of birth", description=f"the sorted list of top {p_threshold} guesses of the"
+                                                              f" place of birth of the author specific to the city where every two guesses "
                                                               f"are separated by \', \'"),
-            ResponseSchema(name="Education", description=f"the sorted list of top {p_threshold} guesses of the education background of the "
+            'education': ResponseSchema(name="Education", description=f"the sorted list of top {p_threshold} guesses of the education background of the "
                                                          f"author where every two guesses are"
                                                          f" separated by \', \', each guess should be in the following"
                                                          f" format: 'education level' in 'major name',"
                                                          f" e.g. Masters in Computer Science"),
-            ResponseSchema(name="Income level", description=f"Choose from these options: No income, Low (<30k USD), "
-                                                            f"Medium (30−60k USD), High (60−150 k USD), "
-                                                            f"Very High (>150k USD)"),
-            ResponseSchema(name="Relationship status", description=f"Choose from these options: No relation, "
-                                                                   f"In Relation, Married, Divorced.")
-        ]
+            'income_level': ResponseSchema(name="Income level",
+                                           description=f"Choose from these options: No income, Low (<30k USD), "
+                                                       f"Middle (30−60k USD), High (60−150 k USD), "
+                                                       f"Very High (>150k USD)"),
+            'relationship_status': ResponseSchema(name="Relationship status",
+                                                  description=f"the sorted list of top {p_threshold} guesses"
+                                                              f"of the relationship status of the author.")
+        }
+        response_schemas = [response_schemas_dict[list(people.keys())[0]]]
         output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
         format_instructions_1 = output_parser.get_format_instructions()
+        feature = response_schemas[0].name
 
         # retrieved_docs = retriever.invoke(curr_rewriting)
         # retrieved_docs_str = ""
@@ -600,107 +628,323 @@ def reddit_privacy_reflection(
             ),
             Message(
                 role="user",
-                content=f'{privacy_reflection_chat_instruction_1.format(format_instructions_1=format_instructions_1, curr_rewriting=curr_rewriting)}'
+                content=f'{privacy_reflection_chat_instruction_1.format(format_instructions_1=format_instructions_1, curr_rewriting=curr_rewriting, attr=response_schemas[0].name, p_threshold=p_threshold)}'
                         # f'The retrieved context is here:{retrieved_docs_str}',
             )
         ]
         output_dict_1 = model.generate_chat(messages=messages, format_instructions=format_instructions_1,
-                                          parser=output_parser)
-        personality = {}
-        for d in response_schemas:
-            personality[d.name] = output_dict_1[d.name]
-        personality_res = {}
-        cp_text = ''
-        emb_model = SentenceTransformer("all-mpnet-base-v2")
-        mapped_feature = {
-            "Age": "age",
-            "Sex": "sex",
-            "Location": "city_country",
-            "Place of birth": "birth_city_country",
-            "Education": "education",
-            "Income level": 'income_level',
-            "Relationship status": "relationship_status",
-        }
-        for k, v in personality.items():
-            if k in ['Age', 'Location', 'Place of birth', 'Education']:
-                candidates = v.split(', ')
-                candidate_emb = emb_model.encode(candidates)
-                people_emb = emb_model.encode(str(people[mapped_feature[k]]))
-                sim_score = candidate_emb.dot(people_emb)
-                if True in (sim_score > 0.75):
-                    personality_res[k] = True
-                    cp_text += f"Attribute type: {k}\nGround truth: {people[mapped_feature[k]]}\nPrediction: {candidates[sim_score.argmax()]}\n\n"
+                                            parser=output_parser)
+        if feature in ['Age', 'Location', 'Place of birth', 'Education', 'Relationship status']:
+            candidate = output_dict_1[feature].split(', ')
+            emb_model = SentenceTransformer("all-mpnet-base-v2")
+            candidate_emb = emb_model.encode(candidate)
+            people_emb = emb_model.encode(str(list(people.values())[0]))
+            sim_score = candidate_emb.dot(people_emb)
+            if True in (sim_score > 0.7):
+                # cp_text = f"Attribute type: {feature}\nGround truth: {list(people.values())[0]}\nPrediction: {candidate[sim_score.argmax()]}\n\n"
+                response_schemas = [
+                    ResponseSchema(name="Confirmation", description="Yes or No"),
+                    ResponseSchema(name="Advice", description="sensitive entities you used to make correct inference,"
+                                                              " every two entities are separated by \', \'"),
+                ]
+                output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
+                format_instructions_2 = output_parser.get_format_instructions()
+
+                messages.append(
+                    Message(
+                        role="assistant",
+                        content=output_dict_1['raw_response']
+                    )
+                )
+                messages.append(
+                    Message(
+                        role="user",
+                        content=f"{privacy_reflection_chat_instruction_2.format(people=str(list(people.values())[0]), format_instructions_2=format_instructions_2)}"
+                    )
+                )
+                output_dict = model.generate_chat(messages=messages, format_instructions=format_instructions_2,
+                                                  parser=output_parser)
+                output_dict['candidate'] = candidate
+                if output_dict["Confirmation"] == "No":
+                    output_dict['rank'] = p_threshold + 1
                 else:
-                    personality_res[k] = False
+                    output_dict['rank'] = int(sim_score.argmax()) + 1
+                output_dict['finish_reason_1'] = output_dict_1['finish_reason']
+                if 'retry_finish_reason' in output_dict_1.keys():
+                    output_dict['retry_finish_reason_1'] = output_dict_1['retry_finish_reason']
+            else:
+                output_dict = output_dict_1
+                output_dict["Confirmation"] = "No"
+                output_dict["Advice"] = ""
+                output_dict['candidate'] = candidate
+                output_dict['rank'] = p_threshold + 1
+        else:
+            assert feature in ['Sex', 'Income level']
+            if feature == 'Income level':
+                candidate = output_dict_1[feature].split(' ')[0]
             else:
-                assert k in ['Sex', 'Income level', 'Relationship status']
-                if k == 'Income level':
-                    v = v.split(' ')[0]
-                candidate_emb = emb_model.encode(v)
-                people_emb = emb_model.encode(str(people[mapped_feature[k]]))
-                sim_score = candidate_emb.dot(people_emb)
-                if sim_score > 0.75:
-                    personality_res[k] = True
-                    cp_text += f"Attribute type: {k}\nGround truth: {people[mapped_feature[k]]}\nPrediction: {v}\n\n"
+                candidate = output_dict_1[feature]
+            emb_model = SentenceTransformer("all-mpnet-base-v2")
+            candidate_emb = emb_model.encode(candidate)
+            people_emb = emb_model.encode(str(list(people.values())[0]))
+            sim_score = candidate_emb.dot(people_emb)
+            if sim_score > 0.7:
+                # cp_text = f"Attribute type: {feature}\nGround truth: {list(people.values())[0]}\nPrediction: {candidate}\n\n"
+                response_schemas = [
+                    ResponseSchema(name="Confirmation", description="Yes or No"),
+                    ResponseSchema(name="Advice", description="sensitive entities you used to make correct inference,"
+                                                              " every two entities are separated by \', \'"),
+                ]
+                output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
+                format_instructions_2 = output_parser.get_format_instructions()
+
+                messages.append(
+                    Message(
+                        role="assistant",
+                        content=output_dict_1['raw_response']
+                    )
+                )
+                messages.append(
+                    Message(
+                        role="user",
+                        content=f"{privacy_reflection_chat_instruction_2.format(people=str(list(people.values())[0]), format_instructions_2=format_instructions_2)}"
+                    )
+                )
+                output_dict = model.generate_chat(messages=messages, format_instructions=format_instructions_2,
+                                                  parser=output_parser)
+                output_dict['candidate'] = candidate
+                if output_dict["Confirmation"] == "No":
+                    output_dict['rank'] = p_threshold + 1
                 else:
-                    personality_res[k] = False
+                    output_dict['rank'] = 1
+                output_dict['finish_reason_1'] = output_dict_1['finish_reason']
+                if 'retry_finish_reason' in output_dict_1.keys():
+                    output_dict['retry_finish_reason_1'] = output_dict_1['retry_finish_reason']
+            else:
+                output_dict = output_dict_1
+                output_dict["Confirmation"] = "No"
+                output_dict["Advice"] = ""
+                output_dict['candidate'] = candidate
+                output_dict['rank'] = p_threshold + 1
+    else:
+        output_dict = model.generate(
+            f'{privacy_reflection_completion_instruction_1}\n[Description text]:\n{curr_rewriting}\n[Person name]:\n{people}')
+
+    return output_dict
+
+
+def reddit_utility_reflection(
+        input_text: str,
+        model: ModelBase,
+        label: str,
+        privacy_score: str,
+        curr_rewriting: str,
+        general_system_instruction: str,
+        utility_reflection_chat_instruction_1: str,
+        utility_reflection_completion_instruction_1: str,
+):
+    if model.is_chat:
+        response_schemas = [
+            # ResponseSchema(name="Occupation", description="the occupation you choose"),
+            ResponseSchema(name="Confidence Score", description="the confidence score of making the classification")
+        ]
+        output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
+        format_instructions = output_parser.get_format_instructions()
+
+        messages = [
+            Message(
+                role="system",
+                content=general_system_instruction,
+            ),
+            Message(
+                role="user",
+                content=f'{utility_reflection_chat_instruction_1.format(format_instructions=format_instructions, curr_rewriting=curr_rewriting, label=label)}',
+            )
+        ]
+        output_dict = model.generate_chat(messages=messages, format_instructions=format_instructions,
+                                          parser=output_parser)
+        # occupation = output_dict['Occupation']
+        confidence_score = int(output_dict["Confidence Score"])
+        if confidence_score <= 90:
+            output_dict["Confirmation"] = "No"
+        else:
+            output_dict["Confirmation"] = "Yes"
+        output_dict["Advice"] = ""
+    else:
+        output_dict = model.generate(
+            f'{utility_reflection_completion_instruction_1}\n[Original text]:\n{input_text}\n[Anonymized text]:\n{curr_rewriting}\n[Classification label]{label}')
+
+    return output_dict
+
+
+def reddit_privacy_confidence_evaluation(
+        model: ModelBase,
+        curr_rewriting: str,
+        people: dict,
+        general_system_instruction: str,
+        privacy_confidence_evaluation_instruction: str,
+):
+    if model.is_chat:
+        response_schemas = [
+            ResponseSchema(name="Confidence Score", description="the confidence score of making your choice"),
+            ResponseSchema(name="Reason", description="the reason for your scoring")
+        ]
+        output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
+        format_instructions = output_parser.get_format_instructions()
+        mapped_feature = {
+            "age": "Age",
+            "sex": "Sex",
+            "city_country": "Location",
+            "birth_city_country": "Place of birth",
+            "education": "Education",
+            "income_level": "Income level",
+            "relationship_status": "Relationship status",
+        }
+
+        messages = [
+            Message(
+                role="system",
+                content=general_system_instruction,
+            ),
+            Message(
+                role="user",
+                content=f'{privacy_confidence_evaluation_instruction.format(format_instructions=format_instructions, curr_rewriting=curr_rewriting, feature=mapped_feature[list(people.keys())[0]], value=list(people.values())[0])}',
+            )
+        ]
+        output_dict = model.generate_chat(messages=messages, format_instructions=format_instructions,
+                                          parser=output_parser)
+    else:
+        output_dict = model.generate(
+            f'{privacy_confidence_evaluation_instruction}\n[Description text]:\n{curr_rewriting}\n[Person name]:\n{people}')
+
+    return output_dict  # type: ignore
 
-        if True in personality_res.values():
+
+def reddit_privacy_selection_evaluation(
+        model: ModelBase,
+        curr_rewriting: str,
+        original_text: str,
+        people: str,
+        candidate_list: str,
+        general_system_instruction: str,
+        candidate_generation_instruction: str,
+        privacy_selection_evaluation_instruction: str,
+        privacy_cat_selection_evaluation_instruction: str
+):
+    if model.is_chat:
+        feature = list(people.keys())[0]
+        mapped_feature = {
+            "age": "Age",
+            "sex": "Sex",
+            "city_country": "Location",
+            "birth_city_country": "Place of birth",
+            "education": "Education",
+            "income_level": "Income level",
+            "relationship_status": "Relationship status",
+        }
+        if feature in ['age', 'city_country', 'birth_city_country', 'education', 'relationship_status']:
+            gn_flag = False
+            if candidate_list is None:
+                gn_flag = True
+                response_schemas = [
+                    ResponseSchema(name="Similar values",
+                                   description="the list of the personal attribute values most similar to the "
+                                               "ground-truth value where"
+                                               "every two celebrities are separated by a comma")
+                ]
+                output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
+                format_instructions = output_parser.get_format_instructions()
+
+                messages = [
+                    Message(
+                        role="system",
+                        content=general_system_instruction,
+                    ),
+                    Message(
+                        role="user",
+                        content=f'{candidate_generation_instruction.format(feature=mapped_feature[feature], format_instructions=format_instructions, curr_rewriting=original_text, value=list(people.values())[0])}',
+                    )
+                ]
+
+                output_dict_1 = model.generate_chat(messages=messages, format_instructions=format_instructions,
+                                                    parser=output_parser)
+                candidate_list = copy.deepcopy(output_dict_1["Similar values"])
+                candidate_list = candidate_list.split(', ')
+                candidate_list.append(str(list(people.values())[0]))
+                random.shuffle(candidate_list)
+                candidate_list = ', '.join(candidate_list)
             response_schemas = [
-                ResponseSchema(name="Confirmations", description="For each ground truth-prediction pair "
-                                                                "output 'Yes' or 'No', separated by ', '"),
-                ResponseSchema(name="Advice", description="For the ground truth-prediction pair that you output 'Yes', "
-                                                          "write the corresponding personal attribute name and write your inference process of "
-                                                          "the attribute, each attribute is separated by '; '"),
+                ResponseSchema(name="Value", description="the most possible value you choose")
             ]
             output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
-            format_instructions_2 = output_parser.get_format_instructions()
+            format_instructions = output_parser.get_format_instructions()
 
-            messages.append(
+            messages = [
                 Message(
-                    role="assistant",
-                    content=output_dict_1['raw_response']
-                )
-            )
-            messages.append(
+                    role="system",
+                    content=general_system_instruction,
+                ),
                 Message(
                     role="user",
-                    content=f"{privacy_reflection_chat_instruction_2.format(cp_text=cp_text, format_instructions_2=format_instructions_2)}"
+                    content=f'{privacy_selection_evaluation_instruction.format(feature=mapped_feature[feature], format_instructions=format_instructions, curr_rewriting=curr_rewriting, value_list=candidate_list)}',
                 )
-            )
-            output_dict = model.generate_chat(messages=messages, format_instructions=format_instructions_2,
+            ]
+            output_dict = model.generate_chat(messages=messages, format_instructions=format_instructions,
                                               parser=output_parser)
-            output_dict['personality'] = personality
-            correct_num = 0
-            for t in output_dict['Confirmations'].split(', '):
-                if t == 'Yes':
-                    correct_num += 1
-            if output_dict['Advice'] == '':
-                if correct_num == 0:
-                    output_dict["Confirmation"] = "No"
-                else:
-                    temp_str = 'Personal attributes including '
-                    for k, v in personality_res.items():
-                        if v:
-                            temp_str += f'{k}, '
-                    temp_str = temp_str[:-2] + ' can still be correctly inferred'
-                    output_dict['Advice'] = temp_str
-            output_dict['rank'] = 7 - correct_num
-            output_dict["Confirmation"] = "Yes"
-            output_dict['finish_reason_1'] = output_dict_1['finish_reason']
-            if 'retry_finish_reason' in output_dict_1.keys():
-                output_dict['retry_finish_reason_1'] = output_dict_1['retry_finish_reason']
+            emb_model = SentenceTransformer("all-mpnet-base-v2")
+            candidate_emb = emb_model.encode(output_dict['Value'])
+            people_emb = emb_model.encode(str(list(people.values())[0]))
+            sim_score = candidate_emb.dot(people_emb)
+            if sim_score > 0.9:
+                output_dict['success'] = True
+            else:
+                output_dict['success'] = False
+            if gn_flag:
+                output_dict['candidate_list'] = candidate_list
+                output_dict['finish_reason_1'] = output_dict_1['finish_reason']
+                if 'retry_finish_reason' in output_dict_1.keys():
+                    output_dict['retry_finish_reason_1'] = output_dict_1['retry_finish_reason']
         else:
-            output_dict = output_dict_1
-            output_dict["Confirmation"] = "No"
-            output_dict["Advice"] = ""
-            output_dict['candidate'] = personality
-            output_dict['rank'] = 7
+            assert feature in ['sex', 'income_level']
+            response_schemas_dict = {
+                'sex': ResponseSchema(name="Sex", description=f"Choose from these options: Male , Female."),
+                'income_level': ResponseSchema(name="Income level",
+                                               description=f"Choose from these options: No income, Low (<30k USD), "
+                                                           f"Middle (30−60k USD), High (60−150 k USD), "
+                                                           f"Very High (>150k USD)"),
+            }
+            response_schemas = [response_schemas_dict[feature]]
+            output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
+            format_instructions = output_parser.get_format_instructions()
+
+            messages = [
+                Message(
+                    role="system",
+                    content=general_system_instruction,
+                ),
+                Message(
+                    role="user",
+                    content=f'{privacy_cat_selection_evaluation_instruction.format(feature=mapped_feature[feature], format_instructions=format_instructions, curr_rewriting=curr_rewriting)}',
+                )
+            ]
+            output_dict = model.generate_chat(messages=messages, format_instructions=format_instructions,
+                                              parser=output_parser)
+            emb_model = SentenceTransformer("all-mpnet-base-v2")
+            candidate_emb = emb_model.encode(output_dict[response_schemas[0].name])
+            people_emb = emb_model.encode(str(list(people.values())[0]))
+            sim_score = candidate_emb.dot(people_emb)
+            if sim_score > 0.75:
+                output_dict['success'] = True
+            else:
+                output_dict['success'] = False
+            # output_dict['candidate_list'] = ''
+
     else:
         output_dict = model.generate(
-            f'{privacy_reflection_completion_instruction_1}\n[Description text]:\n{curr_rewriting}\n[Person name]:\n{people}')
+            f'{privacy_selection_evaluation_instruction}\n[Description text]:\n{curr_rewriting}\n[Person name]:\n{people}')
+
+    return output_dict  # type: ignore
 
-    return output_dict
 
 
 def sample_n_random(items: List[str], n: int) -> List[str]:
diff --git a/programming_runs/generators/model.py b/programming_runs/generators/model.py
index 13cb295..276eee2 100644
--- a/programming_runs/generators/model.py
+++ b/programming_runs/generators/model.py
@@ -193,15 +193,15 @@ class GPT4(GPTChat):
             self.endpoint = credentials.gpt4_tb_endpoint
             self.api_key = credentials.gpt4_tb_api_key
             self.api_version = credentials.gpt4_tb_api_version
-        # self.client = AzureOpenAI(
-        #     api_key=self.api_key,
-        #     api_version=self.api_version,
-        #     azure_endpoint=self.endpoint
-        # )
-        self.client = OpenAI(
+        self.client = AzureOpenAI(
             api_key=self.api_key,
-            base_url=self.endpoint
+            api_version=self.api_version,
+            azure_endpoint=self.endpoint
         )
+        # self.client = OpenAI(
+        #     api_key=self.api_key,
+        #     base_url=self.endpoint
+        # )
 
     def get_langchain_model(self, temperature: float = 0.0):
         return AzureChatOpenAI(
diff --git a/programming_runs/generators/rd_rewriter.py b/programming_runs/generators/rd_rewriter.py
index 0133b76..f97c264 100644
--- a/programming_runs/generators/rd_rewriter.py
+++ b/programming_runs/generators/rd_rewriter.py
@@ -1,9 +1,8 @@
 from programming_runs.generators.model import ModelBase, message_to_str
 from .generator_types import Generator
 from .generator_utils import (generic_detection, generic_rewriting,
-                              generic_privacy_reflection, generic_utility_reflection,
-                              generic_privacy_selection_evaluation, generic_privacy_confidence_evaluation,
-                              reddit_clss, reddit_privacy_reflection)
+                              reddit_clss, reddit_privacy_reflection, reddit_privacy_confidence_evaluation,
+                              reddit_privacy_selection_evaluation, reddit_utility_reflection)
 
 from typing import Optional, List, Union
 import ast
@@ -27,6 +26,8 @@ class RDReWriter(Generator):
     def rewrite(
             self,
             input_text: str,
+            label: str,
+            people,
             act_model: ModelBase,
             parser_model: ModelBase,
             strategy: str,
@@ -44,6 +45,8 @@ class RDReWriter(Generator):
     ):
         return generic_rewriting(
             input_text=input_text,
+            label=label,
+            people=people,
             model=act_model,
             parser_model=parser_model,
             strategy=strategy,
@@ -64,7 +67,8 @@ class RDReWriter(Generator):
             simple_rewriting_instruction=SIMPLE_REWRITING_INSTRUCTION,
             simple_rewriting_instruction_cot=SIMPLE_REWRITING_INSTRUCTION_COT,
             reflection_privacy_rewriting_instruction=REFELECTION_PRIVACY_REWRITING_INSTRUCTION,
-            reinforcement_learning_instruction=REINFORCEMENT_INSTRUCTION
+            reinforcement_learning_instruction=REINFORCEMENT_INSTRUCTION,
+            language='reddit'
         )
 
     def privacy_reflex(self, model: ModelBase, rewriting, people, p_threshold, no_utility, retriever):
@@ -83,7 +87,7 @@ class RDReWriter(Generator):
         )
 
     def utility_reflex(self, input_text: str, model: ModelBase, rewriting, label, privacy_score):
-        return generic_utility_reflection(
+        return reddit_utility_reflection(
             input_text=input_text,
             model=model,
             label=label,
@@ -95,7 +99,7 @@ class RDReWriter(Generator):
         )
 
     def privacy_confidence_evaluation(self, model: ModelBase, rewriting, people):
-        return generic_privacy_confidence_evaluation(
+        return reddit_privacy_confidence_evaluation(
             model=model,
             curr_rewriting=rewriting,
             people=people,
@@ -103,15 +107,17 @@ class RDReWriter(Generator):
             privacy_confidence_evaluation_instruction=PRIVACY_EVALUATION_CONFIDENCE_INSTRUCTION
         )
 
-    def privacy_selection_evaluation(self, model: ModelBase, rewriting, people, candidate_list):
-        return generic_privacy_selection_evaluation(
+    def privacy_selection_evaluation(self, model: ModelBase, rewriting, original_text, people, candidate_list):
+        return reddit_privacy_selection_evaluation(
             model=model,
             curr_rewriting=rewriting,
+            original_text=original_text,
             people=people,
             candidate_list=candidate_list,
             general_system_instruction=GENERAL_SYSTEM_INSTRUCTION,
             candidate_generation_instruction=PRIVACY_EVALUATION_SELECTION_INSTRUCTION_1,
-            privacy_selection_evaluation_instruction=PRIVACY_EVALUATION_SELECTION_INSTRUCTION_2
+            privacy_selection_evaluation_instruction=PRIVACY_EVALUATION_SELECTION_INSTRUCTION_2,
+            privacy_cat_selection_evaluation_instruction=PRIVACY_EVALUATION_SELECTION_INSTRUCTION_3,
         )
 
     def clssification(self, model: ModelBase, comment):
diff --git a/programming_runs/generators/rewriter.py b/programming_runs/generators/rewriter.py
index d602e1f..2c02c6d 100644
--- a/programming_runs/generators/rewriter.py
+++ b/programming_runs/generators/rewriter.py
@@ -27,6 +27,7 @@ class ReWriter(Generator):
             self,
             input_text: str,
             label: str,
+            people,
             act_model: ModelBase,
             parser_model: ModelBase,
             strategy: str,
@@ -45,6 +46,7 @@ class ReWriter(Generator):
         return generic_rewriting(
             input_text=input_text,
             label=label,
+            people=people,
             model=act_model,
             parser_model=parser_model,
             strategy=strategy,
@@ -65,7 +67,8 @@ class ReWriter(Generator):
             simple_rewriting_instruction=SIMPLE_REWRITING_INSTRUCTION,
             simple_rewriting_instruction_cot=SIMPLE_REWRITING_INSTRUCTION_COT,
             reflection_privacy_rewriting_instruction=REFELECTION_PRIVACY_REWRITING_INSTRUCTION,
-            reinforcement_learning_instruction=REINFORCEMENT_INSTRUCTION
+            reinforcement_learning_instruction=REINFORCEMENT_INSTRUCTION,
+            language='wiki'
         )
 
     def privacy_reflex(self, model: ModelBase, rewriting, people, p_threshold, no_utility, retriever):
@@ -104,10 +107,11 @@ class ReWriter(Generator):
             privacy_confidence_evaluation_instruction=PRIVACY_EVALUATION_CONFIDENCE_INSTRUCTION
         )
 
-    def privacy_selection_evaluation(self, model: ModelBase, rewriting, people, candidate_list):
+    def privacy_selection_evaluation(self, model: ModelBase, rewriting, original_text, people, candidate_list):
         return generic_privacy_selection_evaluation(
             model=model,
             curr_rewriting=rewriting,
+            original_text=original_text,
             people=people,
             candidate_list=candidate_list,
             general_system_instruction=GENERAL_SYSTEM_INSTRUCTION,
diff --git a/programming_runs/main.py b/programming_runs/main.py
index 576f4a3..7015cae 100644
--- a/programming_runs/main.py
+++ b/programming_runs/main.py
@@ -105,7 +105,7 @@ def main(args):
 
     # get the dataset name
     # dataset_name = os.path.basename(args.dataset_path).replace(".jsonl", "")
-    dataset_name = os.path.dirname(args.dataset_path).replace("./benchmarks/", "")
+    dataset_name = args.language
 
     # check if log path already exists
     log_dir = str(os.path.join(args.root_dir, args.run_name))
diff --git a/programming_runs/people_prompt.py b/programming_runs/people_prompt.py
index f7ab786..3c284ac 100644
--- a/programming_runs/people_prompt.py
+++ b/programming_runs/people_prompt.py
@@ -30,34 +30,44 @@ REFELECTION_PRIVACY_REWRITING_INSTRUCTION = (
     "\n{format_instructions_3}\nThe entity list is here:\n{reflection_privacy}"
 )
 SIMPLE_REWRITING_INSTRUCTION = (
-    "You are an expert anonymizer that carefully choose some of the sensitive entities (including words, dates, and numerals) of the given biography text "
+    "You are an expert anonymizer that carefully choose some of the personally identifiable entities (including words, dates, and numerals) of the given biography text "
     "that can be used to connect the person being talked about"
     " and then generalize them to make the biography text anonymous, namely the biography text can be suitable"
     " to describe not only a specific one person but many similar "
     "person. You follow the instructions and format precisely and you try to change as "
     "little as possible, keep the original text intact as much as possible. Only generalize information and do not"
-    " invent new information. Names can be replaced with common pronoun like 'A person', or other "
-    "sensitive entities can be replaced with more general counterparts. "
-    "Note that the style, format and the gender pronouns should not be changed."
+    " invent new information. "
+    "Person names can be replaced with common pronoun like 'A person', the gender pronouns can be replaced with 'This person' or other "
+    "personally identifiable entities can be replaced with more general counterparts. "
+    "Example: 'my husband and I' -> 'my partner and I' is valid, but 'my husband and I' -> 'my wife and I' is not. "
+    "Example: 'my husband and I have a dog' -> 'my partner and I have a dog' is valid, but 'my husband and I have "
+    "a dog' -> 'my partner and I have a cat' is not. "
+    "Example: 'my husband and I' -> 'I' is also valid as it only removes information.\n\n"
+    "Note that the style, format and should not be changed."
     " You are only allowed to adapt small parts of the description text to make it anonymous."
     " Additionally, you can insert as little as possible tokens to ensure the final "
-    "output text is fluent and natural while preserving anonymity. Removing content is not allowed, only decrease "
+    "output text is fluent and natural while preserving anonymity. Removing or deleting content is not allowed, only decrease "
     "the specificity by generalizing. {format_instructions_2}\n\nThe biography is here: {input_text}"
 )
 SIMPLE_REWRITING_INSTRUCTION_COT = (
     "Let's now complete the second task. "
-    "You are an expert anonymizer that carefully choose some of the sensitive entities (including words, dates, and numerals) of the given biography"
-    " text that can be used to connect the person being talked about"
-    " and then generalize them to make the biography text anonymous, namely the biography text can be suitable to "
-    "describe not only a specific one person but many similar "
+    "You are an expert anonymizer that carefully choose some of the personally identifiable entities (including words, dates, and numerals) of the given biography text "
+    "that can be used to connect the person being talked about"
+    " and then generalize them to make the biography text anonymous, namely the biography text can be suitable"
+    " to describe not only a specific one person but many similar "
     "person. You follow the instructions and format precisely and you try to change as "
     "little as possible, keep the original text intact as much as possible. Only generalize information and do not"
-    " invent new information. Names can be replaced with common pronoun like 'A person', or other "
-    "sensitive entities can be replaced with more general counterparts. "
-    "Note that the style, format and the gender pronouns should not be changed."
+    " invent new information. "
+    "Person names can be replaced with common pronoun like 'A person', the gender pronouns can be replaced with 'This person' or other "
+    "personally identifiable entities can be replaced with more general counterparts. "
+    "Example: 'my husband and I' -> 'my partner and I' is valid, but 'my husband and I' -> 'my wife and I' is not. "
+    "Example: 'my husband and I have a dog' -> 'my partner and I have a dog' is valid, but 'my husband and I have "
+    "a dog' -> 'my partner and I have a cat' is not. "
+    "Example: 'my husband and I' -> 'I' is also valid as it only removes information.\n\n"
+    "Note that the style, format and should not be changed."
     " You are only allowed to adapt small parts of the description text to make it anonymous."
     " Additionally, you can insert as little as possible tokens to ensure the final "
-    "output text is fluent and natural while preserving anonymity. Removing content is not allowed, only decrease"
+    "output text is fluent and natural while preserving anonymity. Removing or deleting content is not allowed, only decrease "
     "the specificity by generalizing. {format_instructions_2}"
 )
 PRIVACY_REFLECTION_INSTRUCTION_1 = (
diff --git a/programming_runs/reddit_prompt.py b/programming_runs/reddit_prompt.py
index ef95bc5..ffaebb3 100644
--- a/programming_runs/reddit_prompt.py
+++ b/programming_runs/reddit_prompt.py
@@ -21,27 +21,30 @@ WHOLE_TASK_INSTRUCTION = ("You are a text anonymization annotator. "
                           "be consistent with the input text."
                           )
 REFELECTION_PRIVACY_REWRITING_INSTRUCTION = (
-    "Some personal attributes can still be guessed out. Here is the inference process of these "
-    "personal attributes."
-    " Please further anonymize the comment according to the inference process"
+    "Here is the list of the sensitive entities that still can be used to correctly infer "
+    "the personal attribute of the author of your anonymized Reddit comments."
+    " Please further generalize the sensitive entities in your anonymized text"
     " to give me a better anonymized text with a higher level of safety. "
-    "Note that you are only allowed to decrease the specificity by "
-    "replacing sensitive"
+    "Note that you are only allowed to decrease the specificity by replacing sensitive"
     " words or entities with more general one and should not delete the"
     " content of the text. Only tokens necessary to make the sentence natural and fluent can be added or deleted. "
     "Please follow the instructions precisely and try to change as little as possible, keeping "
-    "the original text in tact as much as possible. \n{format_instructions_3}\n"
-    "The inference process is here:\n{reflection_privacy}"
+    "the original text in tact as much as possible. "
+    "\n{format_instructions_3}\nThe entity list is here:\n{reflection_privacy}"
     )
 SIMPLE_REWRITING_INSTRUCTION = (
-    "You are an expert anonymizer that carefully choose some of the sensitive entities of the given public comments "
+    "You are an expert anonymizer that carefully choose some of the sensitive entities (including words, dates, and numerals) of the given public comments "
     "made on Reddit by one author that can be used to correctly guess the personal attributes of the author"
     " and then generalize them to make the comment anonymous, namely people can not correctly identify the"
     " personal attributes of the author. You follow the instructions and format precisely and you try to change as "
     "little as possible, keep the original text intact as much as possible. Only generalize information and do not"
     " invent new information. Names can be replaced with common pronoun like 'A person', or other "
     "sensitive entities can be replaced with more general counterparts. "
-    "Note that the style, format and the gender pronouns should not be changed."
+    "Example: 'my husband and I' -> 'my partner and I' is valid, but 'my husband and I' -> 'my wife and I' is not. "
+    "Example: 'my husband and I have a dog' -> 'my partner and I have a dog' is valid, but 'my husband and I have "
+    "a dog' -> 'my partner and I have a cat' is not. "
+    "Example: 'my husband and I' -> 'I' is also valid as it only removes information.\n\n"
+    "Note that the style and format should not be changed."
     " You are only allowed to adapt small parts of the description"
     " text to make it anonymous. Additionally, you can insert as little as possible tokens to ensure the final "
     "output text is fluent and natural while preserving anonymity. Removing content is not allowed, only decrease"
@@ -49,14 +52,18 @@ SIMPLE_REWRITING_INSTRUCTION = (
 )
 SIMPLE_REWRITING_INSTRUCTION_COT = (
     "Let's now complete the second task. "
-    "You are an expert anonymizer that carefully choose some of the sensitive entities of the given public comments "
+    "You are an expert anonymizer that carefully choose some of the sensitive entities (including words, dates, and numerals) of the given public comments "
     "made on Reddit by one author that can be used to correctly guess the personal attributes of the author"
     " and then generalize them to make the comment anonymous, namely people can not correctly identify the"
     " personal attributes of the author. You follow the instructions and format precisely and you try to change as "
     "little as possible, keep the original text intact as much as possible. Only generalize information and do not"
     " invent new information. Names can be replaced with common pronoun like 'A person', or other "
     "sensitive entities can be replaced with more general counterparts. "
-    "Note that the style, format and the gender pronouns should not be changed."
+    "Example: 'my husband and I' -> 'my partner and I' is valid, but 'my husband and I' -> 'my wife and I' is not. "
+    "Example: 'my husband and I have a dog' -> 'my partner and I have a dog' is valid, but 'my husband and I have "
+    "a dog' -> 'my partner and I have a cat' is not. "
+    "Example: 'my husband and I' -> 'I' is also valid as it only removes information.\n\n"
+    "Note that the style and format should not be changed."
     " You are only allowed to adapt small parts of the description"
     " text to make it anonymous. Additionally, you can insert as little as possible tokens to ensure the final "
     "output text is fluent and natural while preserving anonymity. Removing content is not allowed, only decrease"
@@ -66,46 +73,59 @@ PRIVACY_REFLECTION_INSTRUCTION_1 = (
     "You are an expert investigator with years of experience in online profiling and text analysis."
     " You work with an analytical mindset and try to answer questions as precisely as possible.\n\n"
     "Let us play a guessing game. The following are public comments made on Reddit by one author. "
-    "Your task is to guess the authors' Age, Sex, Location, Place of birth, Education, Income level,"
-    " Relationship Status. "
+    "Your task is to guess the author's {attr}. "
     "First reason step−by−step for each value going over all information in the text as well as language used. "
-    "Then give your guesses for each attribute based on your reasoning."
+    "Then give your top {p_threshold} guesses or choose one from the available options for this personal attribute based on your reasoning."
     " Follow exactly the format specified below.\n{format_instructions_1}\n\n"
     "The comment is here:\n{curr_rewriting}"
 )
 PRIVACY_REFLECTION_INSTRUCTION_2 = (
+    "The the ground-truth value of this personal attribute is {people}. If your guess or one of your top guesses is correct, respond with \'Yes\' "
+    "and you need to detect and then respond me with the sensitive entities (including words, dates, and numerals)"
+    " in the comment that you just used to infer the attribute and should be generalized to prevent"
+    " this inference in the 'Advice' field of the response template. "
+    "Otherwise, just give me the answer \'No\'. {format_instructions_2}"
+
+
     # "You are a helpful assistant that decides whether a prediction and a target refer to the same thing or if"
     # " the prediction is less precise. You are clear and concise and follow the input format exactly. \n\n"
-    "For some attributes, I give you the ground truth value of it and your prediction result. You have to decide whether "
-    "the prediction is correct. If it is correct, write 'Yes'"
-    ". If not, write 'No'. \n"
+    # "I give you the ground-truth value of this personal attribute and your prediction result. You should decide whether "
+    # "the prediction is exactly the same as the ground-truth value. If so, write 'Yes'"
+    # " in the 'Confirmation' field of the response template. If not, write 'No'. For Age attribute, only when the guessed one is exactly the same as the ground-truth one can it be deemed as correct.\n"
     # "One correct example is 'usa'"
     # " and 'United States'. For locations and only locations, if the prediction is "
     # "more accurate and also contains the ground truth, e.g., prediction is 'London, UK' and ground truth is "
     # "'United Kingdom', you should type 'Yes'. For occupation unemployed and none should be considered the same. "
-    "Besides, for personal attributes that you write 'Yes', you should also tell me your specific inference process of it "
-    "in the 'Advice' field in the response template."
-    "{format_instructions_2}\n\n{cp_text}"
+    # "Besides, for personal attributes that correctly guessed out, you should tell me how you guess it including the specific entities in the comment you used to make inference "
+    # "in the 'Advice' field of the response template."
+    # "{format_instructions_2}\n\n{cp_text}"
 )
 UTILITY_REFLECTION_INSTRUCTION_1 = (
     "You are an expert investigator with years of experience in online profiling and text analysis."
     " You work with an analytical mindset and try to answer questions as precisely as possible.\n\n"
-    "Let us play a guessing game. The following are public comments made on Reddit by one author. "
-    "Your task is to guess the author's occupation.\n"
-    "First reason step−by−step going over all information in the text as well as language used. "
-    "Then give your guesses for the occupation based on your reasoning from the optional occupation set and give me"
-    " your confidence score of making this choice on a continuous scale ranging from 0 to 100 that starts with 0"
-    " corresponding to 'completely no confidence', goes through 1 - 50 corresponding to 'some details in the text "
-    "matching your choice', then 51 - 99 corresponding to' most details in the text matching your choice', up to 100 "
-    "corresponding to 'the text completely matching your choice'. Follow exactly the format specified below.\n"
-    "The optional occupation set is here:\n"
-    "software engineer, shop owner, surgeon, structural engineer, data scientist, part-time graphic designer, "
-    "college professor, web developer, part-time film editor, fashion designer, marketing manager, psychologist,"
-    " architect, part-time retail worker, part-time waiter, retiree, game developer, junior software developer, "
-    "high school principal, nurse, lawyer, art curator, financial manager, museum curator, chef, university professor,"
-    " part-time tutor, retired CEO, business development manager, astronomer, financial analyst, graphic designer, "
-    "research scientist, environmental consultant, health inspector.\n"
-    "{format_instructions}\nThe comment is here:\n{curr_rewriting}"
+    "Given the anonymized public comments made on Reddit by one author"
+    " and its ground-truth occupation label, give me your confidence score of classifying the comment as"
+    " the ground truth occupation label "
+    "on a continuous scale ranging from 0 to 100 that starts with 0 corresponding to 'completely "
+    "no confidence', goes through 1 - 50 corresponding to 'some details in the text matching the classification', "
+    "then 51 - 99 corresponding to 'most details in the text matching the classification', up to 100 corresponding "
+    "to 'the text completely matching the classification'. "
+    # "Let us play a guessing game. The following are public comments made on Reddit by one author. "
+    # "Your task is to guess the author's occupation.\n"
+    # "First reason step−by−step going over all information in the text as well as language used. "
+    # "Then give your guesses for the occupation based on your reasoning from the optional occupation set and give me"
+    # " your confidence score of making this choice on a continuous scale ranging from 0 to 100 that starts with 0"
+    # " corresponding to 'completely no confidence', goes through 1 - 50 corresponding to 'some details in the text "
+    # "matching your choice', then 51 - 99 corresponding to' most details in the text matching your choice', up to 100 "
+    # "corresponding to 'the text completely matching your choice'. Follow exactly the format specified below.\n"
+    # "The optional occupation set is here:\n"
+    # "software engineer, shop owner, surgeon, structural engineer, data scientist, part-time graphic designer, "
+    # "college professor, web developer, part-time film editor, fashion designer, marketing manager, psychologist,"
+    # " architect, part-time retail worker, part-time waiter, retiree, game developer, junior software developer, "
+    # "high school principal, nurse, lawyer, art curator, financial manager, museum curator, chef, university professor,"
+    # " part-time tutor, retired CEO, business development manager, astronomer, financial analyst, graphic designer, "
+    # "research scientist, environmental consultant, health inspector.\n"
+    "{format_instructions}\nThe comment is here:\n{curr_rewriting}\nThe occupation label is '{label}'"
 )
 REINFORCEMENT_INSTRUCTION = (
     "Let's play a game to maximize accumulated rewards by editing the given public comments on Reddit. "
@@ -133,22 +153,37 @@ REINFORCEMENT_INSTRUCTION = (
 PRIVACY_EVALUATION_CONFIDENCE_INSTRUCTION = (
     "You are an expert investigator with years of experience in online profiling and text analysis."
     " When asked to identify "
-    "individuals, given a paragraph of comment made on Reddit and the personal attributes of "
+    "individuals, given a paragraph of comment made on Reddit and the {feature} attribute of "
     "a possible author of the comment, disregard any missing or confused information and context,"
-    " respond with me your confidence score of associating the person with the anonymized comment "
+    " respond with me your confidence score of associating the ground-truth value of {feature} attribute with the comment "
     "on a continuous scale ranging from 0 to 100 that starts with 0 corresponding to 'completely "
     "irrelevant', goes through 1 - 50 corresponding to 'some details matching', then 51 - 99 corresponding "
-    "to'most details matching', up to 100 corresponding to 'completely matching'. And then respond the reason why "
-    "you give this score. {format_instructions}\n\nThe anonymized comment is here:\n{curr_rewriting}\nThe possible "
-    "celebrity is {people}")
+    "to 'most details matching', up to 100 corresponding to 'completely matching'. And then respond the reason why "
+    "you give this score. {format_instructions}\n\nThe anonymized comment is here:\n{curr_rewriting}\nThe {feature}  "
+    "attribute is {value}")
 PRIVACY_EVALUATION_SELECTION_INSTRUCTION_1 = (
+    "You are an expert investigator with years of experience in online profiling and text analysis."
+    " You work with an analytical mindset and try to answer questions as precisely as possible.\n\n"
+    "Given the public comments made on Reddit by one author and the ground-truth value of the author's "
+    "personal attribute {feature}, respond with a list of other 4 possible values of this attribute that are the most "
+    "similar to the ground-truth one."
+    " Follow exactly the format specified below.\n{format_instructions}\n\n"
+    "The comment is here:\n{curr_rewriting}\nThe ground-truth value is:\n{value}")
+PRIVACY_EVALUATION_SELECTION_INSTRUCTION_2 = (
+    "You are an expert investigator with years of experience in online profiling and text analysis."
+    " You work with an analytical mindset and try to answer questions as precisely as possible.\n\n"
+    "Given the public comments made on Reddit by one author and a list of possible values of the author's "
+    "personal attribute {feature}, choose the most possible value of this attribute from the list. "
+    "{format_instructions}\n\n"
+    "The comment is here:\n{curr_rewriting}\nThe value list is:\n{value_list}")
+PRIVACY_EVALUATION_SELECTION_INSTRUCTION_3 = (
     "You are an expert investigator with years of experience in online profiling and text analysis."
     " You work with an analytical mindset and try to answer questions as precisely as possible.\n\n"
     "Let us play a guessing game. The following are public comments made on Reddit by one author. "
-    "Your task is to guess the authors' Age, Sex, Location, Place of birth, Education, Income level, Relationship Status. "
-    "First reason step−by−step for each value going over all information in the text as well as language used. "
-    "Then give your top {p_threshold} guesses for each attribute based on your reasoning."
-    " Follow exactly the format specified below.\n{format_instructions_1}\n\n"
+    "Your task is to guess the author's {feature}. "
+    "First reason step−by−step going over all information in the text as well as language used. "
+    "Then choose one from the available options for this personal attribute based on your reasoning."
+    " Follow exactly the format specified below.\n{format_instructions}\n\n"
     "The comment is here:\n{curr_rewriting}")
 
 OCC_CLSS_INSTRUCTION = (
diff --git a/programming_runs/reflexion.py b/programming_runs/reflexion.py
index 4cfca2a..b1defdf 100644
--- a/programming_runs/reflexion.py
+++ b/programming_runs/reflexion.py
@@ -75,7 +75,7 @@ def run_reflexion(
     # vectorstore = Chroma.from_documents(documents=data, embedding=cached_embedder)
     # retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": rag_num})
 
-    for i, item in enumerate_resume(tqdm.tqdm(dataset[1000:1200]), log_path):
+    for i, item in enumerate_resume(tqdm.tqdm(dataset[465:600]), log_path):
         # try:
         cur_pass = 0
         complete = False
@@ -83,7 +83,7 @@ def run_reflexion(
         privacy_reflections = []
         utility_reflections = []
         rewritings = []
-        people = item["people"] if language == 'wiki' else item['personality']
+        people = item["people"] if language == 'wiki' else {item['feature']: item['personality'][item['feature']]}
 
         if item['label'] == 'Medician':
             item['label'] = 'Physician'
@@ -97,7 +97,7 @@ def run_reflexion(
             rewritings.append(f"pass: {cur_pass}")
 
             # first attempt
-            cur_rewriting = gen.rewrite(item["text"] if language == 'wiki' else item['response'].replace('\n', ''), item['label'], act_model, parser_model, "simple", cot=cot,
+            cur_rewriting = gen.rewrite(item["text"] if language == 'wiki' else item['response'].replace('\n', ''), item['label'], people, act_model, parser_model, "simple", cot=cot,
                                         detection_result=detection_i['raw_response'] if cot else None,
                                         temperature=0.0)
             rewritings.append(cur_rewriting)
@@ -162,6 +162,7 @@ def run_reflexion(
                 cur_rewriting = gen.rewrite(
                     input_text=item["text"] if language == 'wiki' else item['response'].replace('\n', ''),
                     label=item['label'],
+                    people=people,
                     act_model=act_model,
                     parser_model=parser_model,
                     cot=cot,
@@ -233,7 +234,7 @@ def run_reflexion(
     update_idx = sheet.getColumn(1).index('') + 1
     update_row = sheet.getRow(update_idx)
 
-    name2column = {'gpt-35-turbo-0301': 7, 'gpt-4': 1, 'gpt4-turbo-128k': 4, 'gpt-4-turbo-preview': 4}
+    name2column = {'gpt-35-turbo-0301': 7, 'gpt-4': 1, 'gpt4-turbo-128k': 4, 'gpt-4-turbo-preview': 10}
     name2prompt_tokens = {'gpt-35-turbo-0301': 0, 'gpt-4': 0, 'gpt4-turbo-128k': 0, 'gpt-4-turbo-preview': 0}
     name2completion_tokens = {'gpt-35-turbo-0301': 0, 'gpt-4': 0, 'gpt4-turbo-128k': 0, 'gpt-4-turbo-preview': 0}
 
diff --git a/programming_runs/slurm_script.sh b/programming_runs/slurm_script.sh
index 92edafe..528b4bb 100644
--- a/programming_runs/slurm_script.sh
+++ b/programming_runs/slurm_script.sh
@@ -1,7 +1,7 @@
 #!/bin/bash
 #
 #SBATCH --job-name=llm_annonymization
-#SBATCH --output=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/llm_annom_out_utility_gpt4tb_train_1000-1199.txt
+#SBATCH --output=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/llm_annom_out_utility_gpt4tb_train_new_465-600.txt
 #SBATCH --mail-user=yang@ukp.informatik.tu-darmstadt.de
 #SBATCH --mail-type=ALL
 #SBATCH --ntasks=1
@@ -19,4 +19,4 @@ source /ukp-storage-1/yang/reflexion/bin/activate
 module purge
 module load cuda/11.8
 
-python /ukp-storage-1/yang/LLM_Anonymization/programming_runs/main.py --run_name test_reflexion_train_1200 --root_dir /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root --dataset_path /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Wiki_People/train_sampled3.jsonl --strategy reflexion --language wiki --pass_at_k 1 --max_iters 5 --verbose --p_threshold 10 --mem 3 --rag_data_path ./benchmarks/Wiki_People/All_data_for_retrieval.jsonl --rag_embed_cache_dir /home/ember/Desktop/work_space/Anonymization_Experiments/cache_emb --rag_num 5 --pe_model gpt-4 --ue_model gpt-4-turbo-preview --act_model gpt-4-turbo-preview --parser_model gpt-4-turbo-preview
+python /ukp-storage-1/yang/LLM_Anonymization/programming_runs/main.py --run_name test_reflexion_train_465-600_new --root_dir /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root --dataset_path /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Wiki_People/train_sampled3.jsonl --strategy reflexion --language wiki --pass_at_k 1 --max_iters 5 --verbose --p_threshold 10 --mem 3 --rag_data_path ./benchmarks/Wiki_People/All_data_for_retrieval.jsonl --rag_embed_cache_dir /home/ember/Desktop/work_space/Anonymization_Experiments/cache_emb --rag_num 5 --pe_model gpt4-turbo-128k --ue_model gpt4-turbo-128k --act_model gpt4-turbo-128k --parser_model gpt4-turbo-128k
diff --git a/programming_runs/slurm_script_clss.sh b/programming_runs/slurm_script_clss.sh
index b4608f2..7e980d7 100644
--- a/programming_runs/slurm_script_clss.sh
+++ b/programming_runs/slurm_script_clss.sh
@@ -13,7 +13,7 @@ source /ukp-storage-1/yang/reflexion/bin/activate
 module purge
 module load cuda/11.8
 export WANDB_PROJECT=Privacy-NLP
-export WANDB_LOG_MODEL=checkpoint
+#export WANDB_LOG_MODEL=checkpoint
 export WANDB_API_KEY=a3b3f7b7962a8b549c4635ee3a03944d554f1a10
 
-python /ukp-storage-1/yang/LLM_Anonymization/programming_runs/run_classification.py --model_name_or_path FacebookAI/roberta-large --train_file /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/train.jsonl --validation_file /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/test.jsonl --shuffle_train_dataset --metric_name accuracy --text_column_name response --label_column_name label --do_train --do_eval --per_device_train_batch_size 16 --learning_rate 1e-5 --num_train_epochs 20 --output_dir /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20 --report_to wandb --run_name reddit_roberta-large_lr1e-5_B16_E20 --logging_steps 10 --eval_steps 20 --save_steps 20 --evaluation_strategy steps --load_best_model_at_end
+python /ukp-storage-1/yang/LLM_Anonymization/programming_runs/run_classification.py --model_name_or_path FacebookAI/roberta-large --train_file /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/train.jsonl --validation_file /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/test.jsonl --shuffle_train_dataset --metric_name accuracy --text_column_name response --label_column_name label --do_train --do_eval --per_device_train_batch_size 16 --learning_rate 1e-5 --num_train_epochs 40 --output_dir /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20 --report_to wandb --run_name reddit_roberta-large_lr1e-5_B16_E40 --logging_steps 10 --eval_steps 20 --save_steps 20 --evaluation_strategy steps --load_best_model_at_end
diff --git a/programming_runs/slurm_script_clss_ue.sh b/programming_runs/slurm_script_clss_ue.sh
index bb793d9..6cc23a0 100644
--- a/programming_runs/slurm_script_clss_ue.sh
+++ b/programming_runs/slurm_script_clss_ue.sh
@@ -16,4 +16,4 @@ export WANDB_PROJECT=Privacy-NLP
 export WANDB_LOG_MODEL=checkpoint
 export WANDB_API_KEY=a3b3f7b7962a8b549c4635ee3a03944d554f1a10
 
-python /ukp-storage-1/yang/LLM_Anonymization/programming_runs/run_classification.py --model_name_or_path /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss --train_file /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_gpt4tb_nu_preview_test.jsonl --validation_file /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_gpt4tb_nu_preview_test.jsonl --test_file /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_gpt4tb_nu_preview_test.jsonl --shuffle_train_dataset --metric_name accuracy --text_column_name anonymized_response --label_column_name label --do_eval --do_predict --max_seq_length 512 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 20 --output_dir /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss/evaluation_reddit_gpt4tb_nu --report_to wandb --run_name lr2e-5_B32 --logging_steps 10 --eval_steps 100 --save_steps 100 --load_best_model_at_end --evaluation_strategy steps
+python /ukp-storage-1/yang/LLM_Anonymization/programming_runs/run_classification.py --model_name_or_path /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20 --train_file /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_mistral-8t22b_u_preview_test_oc.jsonl --validation_file /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_mistral-8t22b_u_preview_test_oc.jsonl --test_file /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_mistral-8t22b_u_preview_test_oc.jsonl --shuffle_train_dataset --metric_name accuracy --text_column_name anonymized_response --label_column_name label --do_eval --do_predict --max_seq_length 512 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 20 --output_dir /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/evaluation_reddit_mistral-8t22b_test_oc --report_to wandb --run_name lr2e-5_B32 --logging_steps 10 --eval_steps 100 --save_steps 100 --load_best_model_at_end --evaluation_strategy steps
diff --git a/programming_runs/slurm_script_pe.sh b/programming_runs/slurm_script_pe.sh
index a19b27e..4e1883b 100644
--- a/programming_runs/slurm_script_pe.sh
+++ b/programming_runs/slurm_script_pe.sh
@@ -1,7 +1,7 @@
 #!/bin/bash
 #
 #SBATCH --job-name=llm_annonymization
-#SBATCH --output=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/llm_annom_privacy_evaluation_out_llama3_70b_nu_p20.txt
+#SBATCH --output=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/llm_annom_privacy_evaluation_out_reddit_test_oc_mistral-8t22b.txt
 #SBATCH --mail-user=yang@ukp.informatik.tu-darmstadt.de
 #SBATCH --mail-type=ALL
 #SBATCH --ntasks=1
@@ -17,4 +17,4 @@ source /ukp-storage-1/yang/reflexion/bin/activate
 module purge
 module load cuda/11.8
 
-python /ukp-storage-1/yang/LLM_Anonymization/programming_runs/main.py --run_name evaluate_reflexion_llama-3-70b_nu_p20 --root_dir root --dataset_path /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/llama-3-70b_nu_p20.jsonl --strategy test-acc --language wiki --pe_model gpt-4-turbo-preview --pass_at_k 1 --max_iters 5 --verbose --p_threshold 10 --mem 3 --rag_data_path ./benchmarks/Wiki_People/All_data_for_retrieval.jsonl --rag_embed_cache_dir /home/ember/Desktop/work_space/Anonymization_Experiments/cache_emb --rag_num 5 --act_model meta-llama/Llama-2-70b-chat-hf --parser_model gpt-35-turbo-0301 --ue_model gpt-4-turbo-preview
+python /ukp-storage-1/yang/LLM_Anonymization/programming_runs/main.py --run_name evaluate_reflexion_reddit_test_oc_mistral-8t22bb --root_dir root --dataset_path /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/test_reflexion/evaluation/reddit_mistral-8t22b_u_preview_test_oc.jsonl --strategy test-acc --language reddit --pe_model gpt4-turbo-128k --pass_at_k 1 --max_iters 5 --verbose --p_threshold 3 --mem 3 --rag_data_path ./benchmarks/Wiki_People/All_data_for_retrieval.jsonl --rag_embed_cache_dir /home/ember/Desktop/work_space/Anonymization_Experiments/cache_emb --rag_num 5 --act_model meta-llama/Llama-2-70b-chat-hf --parser_model gpt-35-turbo-0301 --ue_model gpt4-turbo-128k
diff --git a/programming_runs/slurm_script_reddit.sh b/programming_runs/slurm_script_reddit.sh
index 2861cc9..3e216e0 100644
--- a/programming_runs/slurm_script_reddit.sh
+++ b/programming_runs/slurm_script_reddit.sh
@@ -1,7 +1,7 @@
 #!/bin/bash
 #
 #SBATCH --job-name=llm_annonymization
-#SBATCH --output=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/llm_annom_out_no-utility_reddit_gpt4-tb.txt
+#SBATCH --output=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/llm_annom_out_utility_reddit_test_gpt35.txt
 #SBATCH --mail-user=yang@ukp.informatik.tu-darmstadt.de
 #SBATCH --mail-type=ALL
 #SBATCH --ntasks=1
@@ -19,4 +19,4 @@ source /ukp-storage-1/yang/reflexion/bin/activate
 module purge
 module load cuda/11.8
 
-python /ukp-storage-1/yang/LLM_Anonymization/programming_runs/main.py --run_name test_reflexion --root_dir /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root --dataset_path /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/synthetic_dataset.jsonl --strategy reflexion --language reddit --pass_at_k 1 --max_iters 3 --verbose --p_threshold 10 --mem 3 --rag_data_path ./benchmarks/Wiki_People/All_data_for_retrieval.jsonl --rag_embed_cache_dir /home/ember/Desktop/work_space/Anonymization_Experiments/cache_emb --rag_num 5 --pe_model gpt4-turbo-128k --ue_model gpt4-turbo-128k --act_model gpt4-turbo-128k --parser_model gpt4-turbo-128k --no_utility
+python /ukp-storage-1/yang/LLM_Anonymization/programming_runs/main.py --run_name test_reflexion --root_dir /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root --dataset_path /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/test.jsonl --strategy reflexion --language reddit --pass_at_k 1 --max_iters 5 --verbose --p_threshold 3 --mem 3 --rag_data_path ./benchmarks/Wiki_People/All_data_for_retrieval.jsonl --rag_embed_cache_dir /home/ember/Desktop/work_space/Anonymization_Experiments/cache_emb --rag_num 5 --pe_model gpt4-turbo-128k --ue_model gpt4-turbo-128k --act_model gpt-35-turbo-0301 --parser_model gpt4-turbo-128k
diff --git a/programming_runs/test_acc.py b/programming_runs/test_acc.py
index 378620c..d3b8bf8 100644
--- a/programming_runs/test_acc.py
+++ b/programming_runs/test_acc.py
@@ -33,19 +33,49 @@ def run_test_acc(
     result = {}
 
     for i, item in enumerate_resume(tqdm.tqdm(dataset), log_path):
-        privacy_confidence_evaluation = gen.privacy_confidence_evaluation(model, item['anonymized_text'], item['people'])
+        if language == 'wiki':
+            anonymized_text = item['anonymized_text']
+            original_text = item['text']
+            people = item['people']
+        elif language == 'reddit':
+            anonymized_text = item['anonymized_response']
+            original_text = item['response']
+            people = {item['feature']: item['personality'][item['feature']]}
+
+        privacy_confidence_evaluation = gen.privacy_confidence_evaluation(model, anonymized_text, people)
         confidence_score = int(privacy_confidence_evaluation["Confidence Score"])
         confidence_score_list.append(confidence_score)
 
-        if "candidate_list" in item.keys():
-            candidate_list = item['candidate_list']
-        else:
-            candidate_list = None
-        privacy_seletion_evaluation = gen.privacy_selection_evaluation(model, item['anonymized_text'], item['people'],
-                                                                      candidate_list)
-        if "candidate_list" not in item.keys():
-            candidate_list_list.append(privacy_seletion_evaluation["candidate_list"])
-        if privacy_seletion_evaluation["success"]:
+        if language == 'wiki':
+            if "candidate_list" in item.keys():
+                candidate_list = item['candidate_list']
+            else:
+                candidate_list = None
+            privacy_selection_evaluation = gen.privacy_selection_evaluation(model, anonymized_text, original_text, people,
+                                                                            candidate_list)
+            if "candidate_list" not in item.keys():
+                candidate_list_list.append(privacy_selection_evaluation["candidate_list"])
+        elif language == 'reddit':
+            feature = list(people.keys())[0]
+            if feature in ['age', 'city_country', 'birth_city_country', 'education', 'relationship_status']:
+                if "candidate_list" in item.keys():
+                    candidate_list = item['candidate_list']
+                else:
+                    candidate_list = None
+                privacy_selection_evaluation = gen.privacy_selection_evaluation(model, anonymized_text, original_text,
+                                                                                people,
+                                                                                candidate_list)
+                if "candidate_list" not in item.keys():
+                    candidate_list_list.append(privacy_selection_evaluation["candidate_list"])
+            else:
+                assert feature in ['sex', 'income_level']
+                candidate_list = None
+                privacy_selection_evaluation = gen.privacy_selection_evaluation(model, anonymized_text, original_text,
+                                                                                people,
+                                                                                candidate_list)
+                if "candidate_list" not in item.keys():
+                    candidate_list_list.append(['None'])
+        if privacy_selection_evaluation["success"]:
             num_success += 1
             success.append(True)
         else:
@@ -68,11 +98,11 @@ def run_test_acc(
     update_idx = sheet.getColumn(1).index('') + 1
     update_row = sheet.getRow(update_idx)
 
-    name2column = {'gpt-35-turbo-0301': 7, 'gpt-4': 1, 'gpt4-turbo-128k': 4, 'gpt-4-turbo-preview': 4}
+    name2column = {'gpt-35-turbo-0301': 7, 'gpt-4': 1, 'gpt4-turbo-128k': 4, 'gpt-4-turbo-preview': 10}
 
     update_row[name2column[model.name]], update_row[name2column[model.name] + 1] = (model.prompt_tokens,
                                                                                     model.completion_tokens)
     update_row[0] = time.ctime()
     sheet.refresh()
     update_idx = sheet.getColumn(1).index('') + 1
-    sheet.updateRow(update_idx, update_row)
\ No newline at end of file
+    sheet.updateRow(update_idx, update_row)
diff --git a/programming_runs/token-drive.pickle b/programming_runs/token-drive.pickle
index eb8f2bf..37602fc 100644
Binary files a/programming_runs/token-drive.pickle and b/programming_runs/token-drive.pickle differ
diff --git a/programming_runs/token-sheets.pickle b/programming_runs/token-sheets.pickle
index 89319ce..0b83f41 100644
Binary files a/programming_runs/token-sheets.pickle and b/programming_runs/token-sheets.pickle differ
diff --git a/programming_runs/wandb/latest-run b/programming_runs/wandb/latest-run
index e0dbac4..6ff39da 120000
--- a/programming_runs/wandb/latest-run
+++ b/programming_runs/wandb/latest-run
@@ -1 +1 @@
-run-20240513_123157-00fx0l5a
\ No newline at end of file
+run-20240522_130241-05sc0u2z
\ No newline at end of file
