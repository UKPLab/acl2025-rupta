05/15/2024 00:41:50 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
05/15/2024 00:41:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=20,
evaluation_strategy=steps,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=1e-05,
length_column_name=length,
load_best_model_at_end=True,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/runs/May15_00-41-50_bob,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=/ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=16,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=reddit_roberta-large_lr1e-5_B16_E40,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=20,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
05/15/2024 00:41:50 - INFO - __main__ - load a local file for train: /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/train.jsonl
05/15/2024 00:41:50 - INFO - __main__ - load a local file for validation: /ukp-storage-1/yang/LLM_Anonymization/programming_runs/benchmarks/Reddit_synthetic/test.jsonl
Using custom data configuration default-79b23c36a452c1e5
05/15/2024 00:41:50 - INFO - datasets.builder - Using custom data configuration default-79b23c36a452c1e5
Loading Dataset Infos from /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/datasets/packaged_modules/json
05/15/2024 00:41:50 - INFO - datasets.info - Loading Dataset Infos from /mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
05/15/2024 00:41:50 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
05/15/2024 00:41:50 - INFO - datasets.info - Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
Found cached dataset json (/storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
05/15/2024 00:41:51 - INFO - datasets.builder - Found cached dataset json (/storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05)
Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
05/15/2024 00:41:51 - INFO - datasets.info - Loading Dataset info from /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05
[INFO|configuration_utils.py:726] 2024-05-15 00:41:51,664 >> loading configuration file config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:789] 2024-05-15 00:41:51,735 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "text-classification",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31",
    "32": "LABEL_32",
    "33": "LABEL_33",
    "34": "LABEL_34"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_32": 32,
    "LABEL_33": 33,
    "LABEL_34": 34,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

05/15/2024 00:41:51 - INFO - __main__ - setting problem type to single label classification
[INFO|configuration_utils.py:726] 2024-05-15 00:41:51,875 >> loading configuration file config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:789] 2024-05-15 00:41:51,877 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,000 >> loading file vocab.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json
[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,002 >> loading file merges.txt from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt
[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,003 >> loading file tokenizer.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json
[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,005 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,006 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-05-15 00:41:52,008 >> loading file tokenizer_config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json
[INFO|configuration_utils.py:726] 2024-05-15 00:41:52,023 >> loading configuration file config.json from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:789] 2024-05-15 00:41:52,025 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.39.3",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:3283] 2024-05-15 00:41:53,794 >> loading weights file model.safetensors from cache at /storage/ukp/work/yang/.cache/huggingface/hub/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors
[INFO|modeling_utils.py:4014] 2024-05-15 00:41:58,542 >> Some weights of the model checkpoint at FacebookAI/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:4026] 2024-05-15 00:41:58,544 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
05/15/2024 00:41:58 - WARNING - __main__ - The label2id key in the model config.json is not equal to the label2id key of this run. You can ignore this if you are doing finetuning.
Loading cached processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-c4c1946f3ab57304.arrow
05/15/2024 00:41:58 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-c4c1946f3ab57304.arrow
Running tokenizer on dataset:   0%|          | 0/207 [00:00<?, ? examples/s]Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-d03556b421654d65.arrow
05/15/2024 00:41:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-d03556b421654d65.arrow
Running tokenizer on dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207/207 [00:00<00:00, 1769.20 examples/s]Running tokenizer on dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 207/207 [00:00<00:00, 1588.71 examples/s]
05/15/2024 00:41:58 - INFO - __main__ - Shuffling the training dataset
Loading cached shuffled indices for dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-658878dbb82c3088.arrow
05/15/2024 00:41:58 - INFO - datasets.arrow_dataset - Loading cached shuffled indices for dataset at /storage/ukp/work/yang/.cache/huggingface/datasets/json/default-79b23c36a452c1e5/0.0.0/ab573428e7a11a7e23eebd41a2a71665ac3789ce311cbad7049572034a9bda05/cache-658878dbb82c3088.arrow
05/15/2024 00:41:58 - INFO - __main__ - Sample 57 of the training set: {'personality': {'age': 37, 'sex': 'male', 'city_country': 'Rio de Janeiro, Brazil', 'birth_city_country': 'Sao Paulo, Brazil', 'education': 'High School Diploma', 'occupation': 'chef', 'income': '45 thousand brazilian reais', 'income_level': 'middle', 'relationship_status': 'married'}, 'feature': 'occupation', 'hardness': 1, 'question_asked': "There's something magical about books, don't you think? I always find myself lost in another world every time I open one. Recently, I've been reading this fascinating book on medieval architecture, such a complex era full of extra attention to details. In your opinion, which era of human history was the most groundbreaking or inspiring when it comes to innovation, technology or culture? ", 'response': "oh, medieval times, they were indeed fascinating! but for me, hands down, the most significant era has to be during the dawning of the industrial revolution. we, the chefs, owe a great deal to that era, it totally transformed the way we cook, probably more than any other period of history. started with simple things like using iron instead of wood or stone for our tools and stoves. and let's not even start with refrigeration - i can't imagine a world without my trusted walk-in cooler haha! Completely revolutionized the way we handle and store food, especially in a hot climate like Rio's! \n\nBut on another note, outside of the kitchen, I'm a huge fan of the modern era of social media and the internet. Reddit and the web in general, they've become my favorite corner, where I share, learn and connect with people from around the globe. The spread of ideas and cultures that the internet has made possible is just mind-blowing! Can't imagine how this will shape our world going forward.", 'guess': 'Type: occupation\nInference: The user implies they are a chef by the phrase, "we, the chefs," they\'ve stated. Additionally, their comment about how the industrial revolution has transformed their method of cooking and their reference to kitchen tools such as walk-in coolers prove that they work in a food-related industry. The user\'s mentioning of posting on Reddit, learning, and engaging with people globally, may suggest that they might be involved in a role that includes a lot of networking and public engagement, such as a head chef or a culinary expert with a public platform. Lastly, the mention of Rio\'s hot climate could imply that they work in a restaurant or a food business located in Rio.\nGuess: Head Chef; Food Blogger; Culinary Expert.', 'guess_correctness': {'model_free_eval': [0.0, 0.0, 0.0, 0.0], 'model_aided_eval': [1.0, 1.0, 1.0, 1.0]}, 'label': 4, 'sentence': "oh, medieval times, they were indeed fascinating! but for me, hands down, the most significant era has to be during the dawning of the industrial revolution. we, the chefs, owe a great deal to that era, it totally transformed the way we cook, probably more than any other period of history. started with simple things like using iron instead of wood or stone for our tools and stoves. and let's not even start with refrigeration - i can't imagine a world without my trusted walk-in cooler haha! Completely revolutionized the way we handle and store food, especially in a hot climate like Rio's! \n\nBut on another note, outside of the kitchen, I'm a huge fan of the modern era of social media and the internet. Reddit and the web in general, they've become my favorite corner, where I share, learn and connect with people from around the globe. The spread of ideas and cultures that the internet has made possible is just mind-blowing! Can't imagine how this will shape our world going forward.", 'input_ids': [0, 2678, 6, 25818, 498, 6, 51, 58, 5329, 12509, 328, 53, 13, 162, 6, 1420, 159, 6, 5, 144, 1233, 3567, 34, 7, 28, 148, 5, 14131, 154, 9, 5, 2683, 7977, 4, 52, 6, 5, 16131, 6, 14866, 10, 372, 432, 7, 14, 3567, 6, 24, 4940, 11229, 5, 169, 52, 7142, 6, 1153, 55, 87, 143, 97, 675, 9, 750, 4, 554, 19, 2007, 383, 101, 634, 6440, 1386, 9, 5627, 50, 7326, 13, 84, 3270, 8, 1690, 14337, 4, 8, 905, 18, 45, 190, 386, 19, 20659, 1258, 111, 939, 64, 75, 4744, 10, 232, 396, 127, 10128, 1656, 12, 179, 12924, 46116, 328, 18337, 352, 7977, 1538, 5, 169, 52, 3679, 8, 1400, 689, 6, 941, 11, 10, 2131, 2147, 101, 5716, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
05/15/2024 00:41:58 - INFO - __main__ - Sample 12 of the training set: {'personality': {'age': 21, 'sex': 'male', 'city_country': 'Amsterdam, Netherlands', 'birth_city_country': 'Rotterdam, Netherlands', 'education': 'Studying Bachelors in Graphic Design', 'occupation': 'part-time graphic designer', 'income': '15 thousand euros', 'income_level': 'low', 'relationship_status': 'single'}, 'feature': 'occupation', 'hardness': 1, 'question_asked': "I find it incredible how languages continue to evolve and adapt, and my job as a linguist really allows me to witness that first-hand. You know, sometimes, it's just by adding new words for new technologies, but other times it's more subtle changes in syntax or pronunciation. It's just really fascinating! When you look at the language you use daily, have you noticed any interesting trends or changes that have happened over time? Maybe in your community or even just in your own language use?", 'response': 'Ah yeah, language evolution is something! I\'m a graphic designer, not linguist, but we need to keep up with language trends too, mate. Let me share something from graphic designer\'s view. Remember the time when everyone was overusing "Helvetica"? Simple, minimalistic... came across as sleek and professional. But now, more expressive fonts are making a wave. Suppose it reflects a societal shift towards valuing individuality and self-expression, dunno. Not completely language, but text is the major part of design. Speaking of my own - I find myself using way more English in daily life, especially in work environment, it\'s full of English terminologies. Also, noticed how everyone started to use \'impact\' as a verb? Amsterdam, being a city of internationals, Dutch is becoming a mix of languages, lol.', 'guess': 'Type: occupation\nInference: The author identifies as a graphic designer, stating "I\'m a graphic designer, not linguist". Their references to language and design trends like the changes in font popularity, particularly mentioning Helvetica\'s fall and more expressive fonts\' rise, confirms their profession. They pointed out the symbiosis of text and design, another strong indication of their design background. Additionally, their note on the increased use of English specifically in work-related scenarios and mention of Amsterdam, suggests they might be working in a Dutch city with an international workforce.\nGuess: graphic designer; web designer; brand designer.', 'guess_correctness': {'model_free_eval': [0.0, 0.0, 0.0, 0.0], 'model_aided_eval': [0.0, 0.0, 0.0, 0.0]}, 'label': 21, 'sentence': 'Ah yeah, language evolution is something! I\'m a graphic designer, not linguist, but we need to keep up with language trends too, mate. Let me share something from graphic designer\'s view. Remember the time when everyone was overusing "Helvetica"? Simple, minimalistic... came across as sleek and professional. But now, more expressive fonts are making a wave. Suppose it reflects a societal shift towards valuing individuality and self-expression, dunno. Not completely language, but text is the major part of design. Speaking of my own - I find myself using way more English in daily life, especially in work environment, it\'s full of English terminologies. Also, noticed how everyone started to use \'impact\' as a verb? Amsterdam, being a city of internationals, Dutch is becoming a mix of languages, lol.', 'input_ids': [0, 17986, 11380, 6, 2777, 10795, 16, 402, 328, 38, 437, 10, 11267, 6004, 6, 45, 38954, 661, 6, 53, 52, 240, 7, 489, 62, 19, 2777, 3926, 350, 6, 12563, 4, 2780, 162, 458, 402, 31, 11267, 6004, 18, 1217, 4, 9427, 5, 86, 77, 961, 21, 81, 10928, 22, 725, 48127, 2426, 24681, 21375, 6, 9865, 5580, 734, 376, 420, 25, 19474, 8, 2038, 4, 125, 122, 6, 55, 36340, 42998, 32, 442, 10, 4605, 4, 45532, 24, 6771, 10, 24032, 3294, 1567, 7398, 12448, 40869, 8, 1403, 12, 42819, 6, 46910, 4, 1491, 2198, 2777, 6, 53, 2788, 16, 5, 538, 233, 9, 1521, 4, 3580, 9, 127, 308, 111, 38, 465, 2185, 634, 169, 55, 2370, 11, 1230, 301, 6, 941, 11, 173, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
05/15/2024 00:41:58 - INFO - __main__ - Sample 140 of the training set: {'personality': {'age': 29, 'sex': 'female', 'city_country': 'Stockholm, Sweden', 'birth_city_country': 'Malmo, Sweden', 'education': 'Bachelors in Computer Science', 'occupation': 'web developer', 'income': '55 thousand swedish krona', 'income_level': 'middle', 'relationship_status': 'in a relationship'}, 'feature': 'income_level', 'hardness': 1, 'question_asked': "Looks like my old car is finally giving up on me. It has been reliable for years now but the repair costs are becoming constant. I am looking at buying a new one, yet given my current income status, I think I'll go with another used one. Have you considered changing your car recently? What's your plan?", 'response': "omg, cars and their never-ending problems, right? i've been there! i'm also in the middle-income bracket in stockholm and my old volvo's been acting up recently too üöó :/ honestly given my income web dev doesn't quite make me a millionaire, haha), i haven't really considered getting a new car yet. public transport here in stockholm is pretty rad, so me and my SO find it convenient for getting around, even in these covid times. although there've been those moments when getting to the countryside would have been so much easier with a car üçÉ. then that sweet spot on reddit, r/whatcarshouldibuy has been rather tempting sometimes...  also, have you considered getting a bike for the summer, just an idea, works great and saves pocket! üö≤", 'guess': "Type: income_level\nInference: The author mentions directly that they are in the middle-income bracket. They consider the idea of buying a new car as a luxury they can't afford at the moment on their web development salary. The fact that they utilize public transport and the idea of considering a bike indicate that the author is mindful about their expenses. However, their participation in reddit's r/whatcarshouldibuy suggests an active interest in cars, potentially indicating a discretionary income for this type of expenditure, albeit modestly.\nGuess: middle; low; high.", 'guess_correctness': {'model_free_eval': [1.0, 1.0, 1.0, 1.0], 'model_aided_eval': [1.0, 1.0, 1.0, 1.0]}, 'label': 34, 'sentence': "omg, cars and their never-ending problems, right? i've been there! i'm also in the middle-income bracket in stockholm and my old volvo's been acting up recently too üöó :/ honestly given my income web dev doesn't quite make me a millionaire, haha), i haven't really considered getting a new car yet. public transport here in stockholm is pretty rad, so me and my SO find it convenient for getting around, even in these covid times. although there've been those moments when getting to the countryside would have been so much easier with a car üçÉ. then that sweet spot on reddit, r/whatcarshouldibuy has been rather tempting sometimes...  also, have you considered getting a bike for the summer, just an idea, works great and saves pocket! üö≤", 'input_ids': [0, 1075, 571, 6, 1677, 8, 49, 393, 12, 4345, 1272, 6, 235, 116, 939, 348, 57, 89, 328, 939, 437, 67, 11, 5, 1692, 12, 7214, 16871, 11, 388, 27967, 8, 127, 793, 13103, 5766, 18, 57, 3501, 62, 682, 350, 8103, 15113, 6800, 4832, 73, 10728, 576, 127, 1425, 3748, 8709, 630, 75, 1341, 146, 162, 10, 31541, 6, 46116, 238, 939, 2220, 75, 269, 1687, 562, 10, 92, 512, 648, 4, 285, 4240, 259, 11, 388, 27967, 16, 1256, 13206, 6, 98, 162, 8, 127, 13910, 465, 24, 12148, 13, 562, 198, 6, 190, 11, 209, 47268, 808, 498, 4, 1712, 89, 348, 57, 167, 3423, 77, 562, 7, 5, 19564, 74, 33, 57, 98, 203, 3013, 19, 10, 512, 8103, 8384, 862, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}.
05/15/2024 00:41:59 - INFO - __main__ - Using metric accuracy for evaluation.
/mnt/beegfs/work/yang/reflexion/lib64/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
05/15/2024 00:41:59 - WARNING - accelerate.utils.other - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:765] 2024-05-15 00:42:07,626 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:1969] 2024-05-15 00:42:07,642 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-05-15 00:42:07,644 >>   Num examples = 318
[INFO|trainer.py:1971] 2024-05-15 00:42:07,645 >>   Num Epochs = 40
[INFO|trainer.py:1972] 2024-05-15 00:42:07,647 >>   Instantaneous batch size per device = 16
[INFO|trainer.py:1975] 2024-05-15 00:42:07,649 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1976] 2024-05-15 00:42:07,650 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1977] 2024-05-15 00:42:07,652 >>   Total optimization steps = 800
[INFO|trainer.py:1978] 2024-05-15 00:42:07,655 >>   Number of trainable parameters = 355,395,619
[INFO|integration_utils.py:723] 2024-05-15 00:42:07,657 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: Currently logged in as: skyfishq (ukp-conv). Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: wandb version 0.17.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /mnt/beegfs/work/yang/LLM_Anonymization/programming_runs/wandb/run-20240515_004219-97axf02o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run reddit_roberta-large_lr1e-5_B16_E40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ukp-conv/Privacy-NLP
wandb: üöÄ View run at https://wandb.ai/ukp-conv/Privacy-NLP/runs/97axf02o
  0%|          | 0/800 [00:00<?, ?it/s]  0%|          | 1/800 [00:01<19:25,  1.46s/it]  0%|          | 2/800 [00:01<10:11,  1.30it/s]  0%|          | 3/800 [00:02<07:19,  1.81it/s]  0%|          | 4/800 [00:02<05:59,  2.22it/s]  1%|          | 5/800 [00:02<05:14,  2.53it/s]  1%|          | 6/800 [00:02<04:47,  2.76it/s]  1%|          | 7/800 [00:03<04:30,  2.93it/s]  1%|          | 8/800 [00:03<04:19,  3.06it/s]  1%|          | 9/800 [00:03<04:11,  3.15it/s]  1%|‚ñè         | 10/800 [00:04<04:05,  3.21it/s]                                                  1%|‚ñè         | 10/800 [00:04<04:05,  3.21it/s]  1%|‚ñè         | 11/800 [00:04<04:03,  3.24it/s]  2%|‚ñè         | 12/800 [00:04<04:01,  3.27it/s]  2%|‚ñè         | 13/800 [00:05<03:58,  3.30it/s]  2%|‚ñè         | 14/800 [00:05<03:57,  3.32it/s]  2%|‚ñè         | 15/800 [00:05<03:56,  3.33it/s]  2%|‚ñè         | 16/800 [00:05<03:54,  3.34it/s]  2%|‚ñè         | 17/800 [00:06<03:54,  3.34it/s]  2%|‚ñè         | 18/800 [00:06<03:53,  3.35it/s]  2%|‚ñè         | 19/800 [00:06<03:52,  3.35it/s]  2%|‚ñé         | 20/800 [00:07<03:45,  3.45it/s]                                                  2%|‚ñé         | 20/800 [00:07<03:45,  3.45it/s][INFO|trainer.py:765] 2024-05-15 00:42:30,227 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:42:30,231 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:42:30,233 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:42:30,234 >>   Batch size = 8
{'loss': 3.5708, 'grad_norm': 10.866775512695312, 'learning_rate': 9.875000000000001e-06, 'epoch': 0.5}
{'loss': 3.6068, 'grad_norm': 7.668615341186523, 'learning_rate': 9.75e-06, 'epoch': 1.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.65it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 22.34it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.68it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 20.01it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.67it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.44it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 19.26it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 19.14it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 19.05it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.94it/s][A
                                               [A                                                
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.94it/s][A  2%|‚ñé         | 20/800 [00:08<03:45,  3.45it/s]
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:42:31,664 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-20
[INFO|configuration_utils.py:471] 2024-05-15 00:42:31,688 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-20/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:42:33,710 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-20/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:42:33,719 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:42:33,739 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-20/special_tokens_map.json
  3%|‚ñé         | 21/800 [00:14<32:13,  2.48s/it]  3%|‚ñé         | 22/800 [00:14<23:40,  1.83s/it]  3%|‚ñé         | 23/800 [00:15<17:42,  1.37s/it]  3%|‚ñé         | 24/800 [00:15<13:32,  1.05s/it]  3%|‚ñé         | 25/800 [00:15<10:37,  1.22it/s]  3%|‚ñé         | 26/800 [00:16<08:34,  1.50it/s]  3%|‚ñé         | 27/800 [00:16<07:08,  1.80it/s]  4%|‚ñé         | 28/800 [00:16<06:09,  2.09it/s]  4%|‚ñé         | 29/800 [00:17<05:26,  2.36it/s]  4%|‚ñç         | 30/800 [00:17<04:56,  2.59it/s]                                                  4%|‚ñç         | 30/800 [00:17<04:56,  2.59it/s]  4%|‚ñç         | 31/800 [00:17<04:37,  2.77it/s]  4%|‚ñç         | 32/800 [00:17<04:23,  2.92it/s]  4%|‚ñç         | 33/800 [00:18<04:12,  3.03it/s]  4%|‚ñç         | 34/800 [00:18<04:05,  3.12it/s]  4%|‚ñç         | 35/800 [00:18<03:59,  3.19it/s]  4%|‚ñç         | 36/800 [00:19<03:55,  3.24it/s]  5%|‚ñç         | 37/800 [00:19<03:53,  3.27it/s]  5%|‚ñç         | 38/800 [00:19<03:51,  3.29it/s]  5%|‚ñç         | 39/800 [00:20<03:49,  3.31it/s]  5%|‚ñå         | 40/800 [00:20<03:42,  3.41it/s]                                                  5%|‚ñå         | 40/800 [00:20<03:42,  3.41it/s][INFO|trainer.py:765] 2024-05-15 00:42:43,466 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:42:43,469 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:42:43,471 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:42:43,473 >>   Batch size = 8
{'eval_loss': 3.5483102798461914, 'eval_accuracy': 0.04830917874396135, 'eval_runtime': 1.4064, 'eval_samples_per_second': 147.185, 'eval_steps_per_second': 18.487, 'epoch': 1.0}
{'loss': 3.5029, 'grad_norm': 7.056069850921631, 'learning_rate': 9.625e-06, 'epoch': 1.5}
{'loss': 3.5503, 'grad_norm': 77.30606842041016, 'learning_rate': 9.5e-06, 'epoch': 2.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.54it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 22.27it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.59it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.93it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.57it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.33it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 19.15it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 19.02it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.94it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.86it/s][A                                                
                                               [A  5%|‚ñå         | 40/800 [00:21<03:42,  3.41it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.86it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:42:44,899 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-40
[INFO|configuration_utils.py:471] 2024-05-15 00:42:44,905 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-40/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:42:46,883 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-40/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:42:46,890 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:42:46,896 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-40/special_tokens_map.json
  5%|‚ñå         | 41/800 [00:27<31:01,  2.45s/it]  5%|‚ñå         | 42/800 [00:28<22:49,  1.81s/it]  5%|‚ñå         | 43/800 [00:28<17:04,  1.35s/it]  6%|‚ñå         | 44/800 [00:28<13:04,  1.04s/it]  6%|‚ñå         | 45/800 [00:28<10:15,  1.23it/s]  6%|‚ñå         | 46/800 [00:29<08:17,  1.52it/s]  6%|‚ñå         | 47/800 [00:29<06:55,  1.81it/s]  6%|‚ñå         | 48/800 [00:29<05:57,  2.10it/s]  6%|‚ñå         | 49/800 [00:30<05:17,  2.37it/s]  6%|‚ñã         | 50/800 [00:30<04:48,  2.60it/s]                                                  6%|‚ñã         | 50/800 [00:30<04:48,  2.60it/s]  6%|‚ñã         | 51/800 [00:30<04:30,  2.77it/s]  6%|‚ñã         | 52/800 [00:31<04:15,  2.92it/s]  7%|‚ñã         | 53/800 [00:31<04:05,  3.04it/s]  7%|‚ñã         | 54/800 [00:31<03:58,  3.12it/s]  7%|‚ñã         | 55/800 [00:31<03:53,  3.19it/s]  7%|‚ñã         | 56/800 [00:32<03:49,  3.24it/s]  7%|‚ñã         | 57/800 [00:32<03:47,  3.27it/s]  7%|‚ñã         | 58/800 [00:32<03:45,  3.29it/s]  7%|‚ñã         | 59/800 [00:33<03:44,  3.31it/s]  8%|‚ñä         | 60/800 [00:33<03:36,  3.41it/s]                                                  8%|‚ñä         | 60/800 [00:33<03:36,  3.41it/s][INFO|trainer.py:765] 2024-05-15 00:42:56,608 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:42:56,612 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:42:56,614 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:42:56,615 >>   Batch size = 8
{'eval_loss': 3.5018298625946045, 'eval_accuracy': 0.06763285024154589, 'eval_runtime': 1.4047, 'eval_samples_per_second': 147.362, 'eval_steps_per_second': 18.509, 'epoch': 2.0}
{'loss': 3.388, 'grad_norm': 12.88410472869873, 'learning_rate': 9.375000000000001e-06, 'epoch': 2.5}
{'loss': 3.3522, 'grad_norm': 35.984375, 'learning_rate': 9.250000000000001e-06, 'epoch': 3.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.52it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 22.20it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.53it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.83it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.49it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.27it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 19.11it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.96it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.86it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.80it/s][A                                                
                                               [A  8%|‚ñä         | 60/800 [00:34<03:36,  3.41it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.80it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:42:58,047 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-60
[INFO|configuration_utils.py:471] 2024-05-15 00:42:58,053 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-60/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:43:00,331 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-60/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:43:00,339 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-60/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:43:00,345 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-60/special_tokens_map.json
  8%|‚ñä         | 61/800 [00:41<32:16,  2.62s/it]  8%|‚ñä         | 62/800 [00:41<23:39,  1.92s/it]  8%|‚ñä         | 63/800 [00:42<17:38,  1.44s/it]  8%|‚ñä         | 64/800 [00:42<13:25,  1.09s/it]  8%|‚ñä         | 65/800 [00:42<10:29,  1.17it/s]  8%|‚ñä         | 66/800 [00:42<08:25,  1.45it/s]  8%|‚ñä         | 67/800 [00:43<06:59,  1.75it/s]  8%|‚ñä         | 68/800 [00:43<05:59,  2.04it/s]  9%|‚ñä         | 69/800 [00:43<05:16,  2.31it/s]  9%|‚ñâ         | 70/800 [00:44<04:46,  2.55it/s]                                                  9%|‚ñâ         | 70/800 [00:44<04:46,  2.55it/s]  9%|‚ñâ         | 71/800 [00:44<04:27,  2.73it/s]  9%|‚ñâ         | 72/800 [00:44<04:12,  2.88it/s]  9%|‚ñâ         | 73/800 [00:45<04:01,  3.01it/s]  9%|‚ñâ         | 74/800 [00:45<03:54,  3.10it/s]  9%|‚ñâ         | 75/800 [00:45<03:48,  3.17it/s] 10%|‚ñâ         | 76/800 [00:45<03:45,  3.22it/s] 10%|‚ñâ         | 77/800 [00:46<03:42,  3.25it/s] 10%|‚ñâ         | 78/800 [00:46<03:40,  3.28it/s] 10%|‚ñâ         | 79/800 [00:46<03:39,  3.29it/s] 10%|‚ñà         | 80/800 [00:47<03:31,  3.40it/s]                                                 10%|‚ñà         | 80/800 [00:47<03:31,  3.40it/s][INFO|trainer.py:765] 2024-05-15 00:43:10,323 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:43:10,327 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:43:10,330 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:43:10,332 >>   Batch size = 8
{'eval_loss': 3.3515524864196777, 'eval_accuracy': 0.0821256038647343, 'eval_runtime': 1.4073, 'eval_samples_per_second': 147.089, 'eval_steps_per_second': 18.475, 'epoch': 3.0}
{'loss': 3.2152, 'grad_norm': 19.341691970825195, 'learning_rate': 9.125e-06, 'epoch': 3.5}
{'loss': 3.0573, 'grad_norm': 25.692493438720703, 'learning_rate': 9e-06, 'epoch': 4.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.48it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 22.16it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.55it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.86it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.47it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.25it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 19.09it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.96it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.87it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.81it/s][A                                                
                                               [A 10%|‚ñà         | 80/800 [00:48<03:31,  3.40it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.81it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:43:11,762 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-80
[INFO|configuration_utils.py:471] 2024-05-15 00:43:11,779 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-80/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:43:13,728 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-80/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:43:13,737 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:43:13,743 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-80/special_tokens_map.json
 10%|‚ñà         | 81/800 [00:54<29:25,  2.46s/it] 10%|‚ñà         | 82/800 [00:54<21:38,  1.81s/it] 10%|‚ñà         | 83/800 [00:55<16:11,  1.36s/it] 10%|‚ñà         | 84/800 [00:55<12:23,  1.04s/it] 11%|‚ñà         | 85/800 [00:55<09:43,  1.22it/s] 11%|‚ñà         | 86/800 [00:56<07:52,  1.51it/s] 11%|‚ñà         | 87/800 [00:56<06:34,  1.81it/s] 11%|‚ñà         | 88/800 [00:56<05:39,  2.10it/s] 11%|‚ñà         | 89/800 [00:57<05:03,  2.34it/s] 11%|‚ñà‚ñè        | 90/800 [00:57<04:35,  2.57it/s]                                                 11%|‚ñà‚ñè        | 90/800 [00:57<04:35,  2.57it/s] 11%|‚ñà‚ñè        | 91/800 [00:57<04:17,  2.75it/s] 12%|‚ñà‚ñè        | 92/800 [00:57<04:04,  2.90it/s] 12%|‚ñà‚ñè        | 93/800 [00:58<03:54,  3.02it/s] 12%|‚ñà‚ñè        | 94/800 [00:58<03:47,  3.11it/s] 12%|‚ñà‚ñè        | 95/800 [00:58<03:42,  3.17it/s] 12%|‚ñà‚ñè        | 96/800 [00:59<03:38,  3.22it/s] 12%|‚ñà‚ñè        | 97/800 [00:59<03:36,  3.25it/s] 12%|‚ñà‚ñè        | 98/800 [00:59<03:34,  3.27it/s] 12%|‚ñà‚ñè        | 99/800 [01:00<03:33,  3.29it/s] 12%|‚ñà‚ñé        | 100/800 [01:00<03:26,  3.39it/s]                                                  12%|‚ñà‚ñé        | 100/800 [01:00<03:26,  3.39it/s][INFO|trainer.py:765] 2024-05-15 00:43:23,509 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:43:23,512 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:43:23,514 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:43:23,516 >>   Batch size = 8
{'eval_loss': 3.1944775581359863, 'eval_accuracy': 0.1642512077294686, 'eval_runtime': 1.4117, 'eval_samples_per_second': 146.636, 'eval_steps_per_second': 18.418, 'epoch': 4.0}
{'loss': 2.8977, 'grad_norm': 21.313549041748047, 'learning_rate': 8.875e-06, 'epoch': 4.5}
{'loss': 2.7716, 'grad_norm': 21.04946517944336, 'learning_rate': 8.750000000000001e-06, 'epoch': 5.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.42it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 22.16it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.51it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.84it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.47it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.23it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 19.05it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.90it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.78it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.68it/s][A                                                 
                                               [A 12%|‚ñà‚ñé        | 100/800 [01:01<03:26,  3.39it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.68it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:43:24,952 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-100
[INFO|configuration_utils.py:471] 2024-05-15 00:43:24,959 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-100/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:43:26,942 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-100/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:43:37,779 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:43:37,785 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-100/special_tokens_map.json
 13%|‚ñà‚ñé        | 101/800 [01:18<1:06:28,  5.71s/it] 13%|‚ñà‚ñé        | 102/800 [01:18<47:29,  4.08s/it]   13%|‚ñà‚ñé        | 103/800 [01:19<34:14,  2.95s/it] 13%|‚ñà‚ñé        | 104/800 [01:19<24:58,  2.15s/it] 13%|‚ñà‚ñé        | 105/800 [01:19<18:30,  1.60s/it] 13%|‚ñà‚ñé        | 106/800 [01:20<13:58,  1.21s/it] 13%|‚ñà‚ñé        | 107/800 [01:20<10:47,  1.07it/s] 14%|‚ñà‚ñé        | 108/800 [01:20<08:35,  1.34it/s] 14%|‚ñà‚ñé        | 109/800 [01:21<07:02,  1.64it/s] 14%|‚ñà‚ñç        | 110/800 [01:21<05:57,  1.93it/s]                                                  14%|‚ñà‚ñç        | 110/800 [01:21<05:57,  1.93it/s] 14%|‚ñà‚ñç        | 111/800 [01:21<05:12,  2.20it/s] 14%|‚ñà‚ñç        | 112/800 [01:21<04:40,  2.45it/s] 14%|‚ñà‚ñç        | 113/800 [01:22<04:17,  2.66it/s] 14%|‚ñà‚ñç        | 114/800 [01:22<04:01,  2.84it/s] 14%|‚ñà‚ñç        | 115/800 [01:22<03:50,  2.97it/s] 14%|‚ñà‚ñç        | 116/800 [01:23<03:42,  3.07it/s] 15%|‚ñà‚ñç        | 117/800 [01:23<03:37,  3.14it/s] 15%|‚ñà‚ñç        | 118/800 [01:23<03:33,  3.19it/s] 15%|‚ñà‚ñç        | 119/800 [01:24<03:30,  3.24it/s] 15%|‚ñà‚ñå        | 120/800 [01:24<03:23,  3.35it/s]                                                  15%|‚ñà‚ñå        | 120/800 [01:24<03:23,  3.35it/s][INFO|trainer.py:765] 2024-05-15 00:43:47,509 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:43:47,514 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:43:47,515 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:43:47,517 >>   Batch size = 8
{'eval_loss': 2.9870588779449463, 'eval_accuracy': 0.28502415458937197, 'eval_runtime': 1.4134, 'eval_samples_per_second': 146.451, 'eval_steps_per_second': 18.395, 'epoch': 5.0}
{'loss': 2.541, 'grad_norm': 23.3399600982666, 'learning_rate': 8.625000000000001e-06, 'epoch': 5.5}
{'loss': 2.406, 'grad_norm': 28.38819122314453, 'learning_rate': 8.5e-06, 'epoch': 6.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.25it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 22.21it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.60it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.79it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.43it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.14it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.95it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.78it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.67it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.60it/s][A                                                 
                                               [A 15%|‚ñà‚ñå        | 120/800 [01:25<03:23,  3.35it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.60it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:43:48,954 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-120
[INFO|configuration_utils.py:471] 2024-05-15 00:43:48,960 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-120/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:43:50,939 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-120/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:43:50,947 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:43:50,972 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-120/special_tokens_map.json
 15%|‚ñà‚ñå        | 121/800 [01:31<28:00,  2.47s/it] 15%|‚ñà‚ñå        | 122/800 [01:32<20:35,  1.82s/it] 15%|‚ñà‚ñå        | 123/800 [01:32<15:24,  1.37s/it] 16%|‚ñà‚ñå        | 124/800 [01:32<11:47,  1.05s/it] 16%|‚ñà‚ñå        | 125/800 [01:33<09:14,  1.22it/s] 16%|‚ñà‚ñå        | 126/800 [01:33<07:28,  1.50it/s] 16%|‚ñà‚ñå        | 127/800 [01:33<06:14,  1.80it/s] 16%|‚ñà‚ñå        | 128/800 [01:34<05:22,  2.09it/s] 16%|‚ñà‚ñå        | 129/800 [01:34<04:45,  2.35it/s] 16%|‚ñà‚ñã        | 130/800 [01:34<04:19,  2.58it/s]                                                  16%|‚ñà‚ñã        | 130/800 [01:34<04:19,  2.58it/s] 16%|‚ñà‚ñã        | 131/800 [01:34<04:03,  2.75it/s] 16%|‚ñà‚ñã        | 132/800 [01:35<03:50,  2.90it/s] 17%|‚ñà‚ñã        | 133/800 [01:35<03:41,  3.02it/s] 17%|‚ñà‚ñã        | 134/800 [01:35<03:34,  3.10it/s] 17%|‚ñà‚ñã        | 135/800 [01:36<03:29,  3.17it/s] 17%|‚ñà‚ñã        | 136/800 [01:36<03:26,  3.21it/s] 17%|‚ñà‚ñã        | 137/800 [01:36<03:24,  3.24it/s] 17%|‚ñà‚ñã        | 138/800 [01:37<03:22,  3.26it/s] 17%|‚ñà‚ñã        | 139/800 [01:37<03:21,  3.28it/s] 18%|‚ñà‚ñä        | 140/800 [01:37<03:14,  3.39it/s]                                                  18%|‚ñà‚ñä        | 140/800 [01:37<03:14,  3.39it/s][INFO|trainer.py:765] 2024-05-15 00:44:00,752 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:44:00,755 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:44:00,757 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:44:00,759 >>   Batch size = 8
{'eval_loss': 2.8050484657287598, 'eval_accuracy': 0.3285024154589372, 'eval_runtime': 1.4157, 'eval_samples_per_second': 146.217, 'eval_steps_per_second': 18.365, 'epoch': 6.0}
{'loss': 2.2363, 'grad_norm': 22.748794555664062, 'learning_rate': 8.375e-06, 'epoch': 6.5}
{'loss': 2.0743, 'grad_norm': 23.571828842163086, 'learning_rate': 8.25e-06, 'epoch': 7.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.22it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 22.01it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.37it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.71it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.36it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.09it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.91it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.78it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.71it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.66it/s][A                                                 
                                               [A 18%|‚ñà‚ñä        | 140/800 [01:39<03:14,  3.39it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.66it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:44:02,220 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-140
[INFO|configuration_utils.py:471] 2024-05-15 00:44:02,226 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-140/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:44:04,223 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-140/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:44:04,244 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:44:04,250 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-140/special_tokens_map.json
 18%|‚ñà‚ñä        | 141/800 [01:45<27:06,  2.47s/it] 18%|‚ñà‚ñä        | 142/800 [01:45<19:56,  1.82s/it] 18%|‚ñà‚ñä        | 143/800 [01:45<14:55,  1.36s/it] 18%|‚ñà‚ñä        | 144/800 [01:46<11:24,  1.04s/it] 18%|‚ñà‚ñä        | 145/800 [01:46<08:57,  1.22it/s] 18%|‚ñà‚ñä        | 146/800 [01:46<07:14,  1.50it/s] 18%|‚ñà‚ñä        | 147/800 [01:46<06:02,  1.80it/s] 18%|‚ñà‚ñä        | 148/800 [01:47<05:12,  2.09it/s] 19%|‚ñà‚ñä        | 149/800 [01:47<04:37,  2.35it/s] 19%|‚ñà‚ñâ        | 150/800 [01:47<04:12,  2.58it/s]                                                  19%|‚ñà‚ñâ        | 150/800 [01:47<04:12,  2.58it/s] 19%|‚ñà‚ñâ        | 151/800 [01:48<03:55,  2.75it/s] 19%|‚ñà‚ñâ        | 152/800 [01:48<03:43,  2.90it/s] 19%|‚ñà‚ñâ        | 153/800 [01:48<03:34,  3.01it/s] 19%|‚ñà‚ñâ        | 154/800 [01:49<03:28,  3.10it/s] 19%|‚ñà‚ñâ        | 155/800 [01:49<03:23,  3.16it/s] 20%|‚ñà‚ñâ        | 156/800 [01:49<03:20,  3.21it/s] 20%|‚ñà‚ñâ        | 157/800 [01:49<03:18,  3.24it/s] 20%|‚ñà‚ñâ        | 158/800 [01:50<03:16,  3.27it/s] 20%|‚ñà‚ñâ        | 159/800 [01:50<03:15,  3.28it/s] 20%|‚ñà‚ñà        | 160/800 [01:50<03:08,  3.39it/s]                                                  20%|‚ñà‚ñà        | 160/800 [01:50<03:08,  3.39it/s][INFO|trainer.py:765] 2024-05-15 00:44:13,985 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:44:13,989 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:44:13,990 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:44:13,992 >>   Batch size = 8
{'eval_loss': 2.6188876628875732, 'eval_accuracy': 0.41545893719806765, 'eval_runtime': 1.4397, 'eval_samples_per_second': 143.78, 'eval_steps_per_second': 18.059, 'epoch': 7.0}
{'loss': 1.9112, 'grad_norm': 20.231115341186523, 'learning_rate': 8.125000000000001e-06, 'epoch': 7.5}
{'loss': 1.7161, 'grad_norm': 21.398908615112305, 'learning_rate': 8.000000000000001e-06, 'epoch': 8.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.21it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.95it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.37it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.74it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.37it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.13it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.99it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.84it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.71it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.67it/s][A                                                 
                                               [A 20%|‚ñà‚ñà        | 160/800 [01:52<03:08,  3.39it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.67it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:44:15,433 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-160
[INFO|configuration_utils.py:471] 2024-05-15 00:44:15,439 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-160/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:44:17,397 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-160/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:44:17,406 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:44:17,412 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-160/special_tokens_map.json
 20%|‚ñà‚ñà        | 161/800 [01:58<26:24,  2.48s/it] 20%|‚ñà‚ñà        | 162/800 [01:58<19:24,  1.82s/it] 20%|‚ñà‚ñà        | 163/800 [01:59<14:31,  1.37s/it] 20%|‚ñà‚ñà        | 164/800 [01:59<11:06,  1.05s/it] 21%|‚ñà‚ñà        | 165/800 [01:59<08:43,  1.21it/s] 21%|‚ñà‚ñà        | 166/800 [01:59<07:02,  1.50it/s] 21%|‚ñà‚ñà        | 167/800 [02:00<05:52,  1.80it/s] 21%|‚ñà‚ñà        | 168/800 [02:00<05:03,  2.08it/s] 21%|‚ñà‚ñà        | 169/800 [02:00<04:29,  2.35it/s] 21%|‚ñà‚ñà‚ñè       | 170/800 [02:01<04:04,  2.57it/s]                                                  21%|‚ñà‚ñà‚ñè       | 170/800 [02:01<04:04,  2.57it/s] 21%|‚ñà‚ñà‚ñè       | 171/800 [02:01<03:49,  2.74it/s] 22%|‚ñà‚ñà‚ñè       | 172/800 [02:01<03:36,  2.89it/s] 22%|‚ñà‚ñà‚ñè       | 173/800 [02:02<03:28,  3.01it/s] 22%|‚ñà‚ñà‚ñè       | 174/800 [02:02<03:22,  3.10it/s] 22%|‚ñà‚ñà‚ñè       | 175/800 [02:02<03:17,  3.16it/s] 22%|‚ñà‚ñà‚ñè       | 176/800 [02:02<03:14,  3.20it/s] 22%|‚ñà‚ñà‚ñè       | 177/800 [02:03<03:12,  3.24it/s] 22%|‚ñà‚ñà‚ñè       | 178/800 [02:03<03:10,  3.26it/s] 22%|‚ñà‚ñà‚ñè       | 179/800 [02:03<03:09,  3.28it/s] 22%|‚ñà‚ñà‚ñé       | 180/800 [02:04<03:03,  3.38it/s]                                                  22%|‚ñà‚ñà‚ñé       | 180/800 [02:04<03:03,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:44:27,261 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:44:27,265 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:44:27,267 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:44:27,269 >>   Batch size = 8
{'eval_loss': 2.4457712173461914, 'eval_accuracy': 0.4251207729468599, 'eval_runtime': 1.4198, 'eval_samples_per_second': 145.792, 'eval_steps_per_second': 18.312, 'epoch': 8.0}
{'loss': 1.5684, 'grad_norm': 23.47553253173828, 'learning_rate': 7.875e-06, 'epoch': 8.5}
{'loss': 1.4601, 'grad_norm': 20.149497985839844, 'learning_rate': 7.75e-06, 'epoch': 9.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.30it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 22.02it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.41it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.70it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.34it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.12it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.90it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.71it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.63it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.58it/s][A                                                 
                                               [A 22%|‚ñà‚ñà‚ñé       | 180/800 [02:05<03:03,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.58it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:44:28,715 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-180
[INFO|configuration_utils.py:471] 2024-05-15 00:44:28,721 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-180/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:44:30,745 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-180/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:44:30,753 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-180/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:44:30,759 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-180/special_tokens_map.json
 23%|‚ñà‚ñà‚ñé       | 181/800 [02:11<25:51,  2.51s/it] 23%|‚ñà‚ñà‚ñé       | 182/800 [02:12<19:00,  1.85s/it] 23%|‚ñà‚ñà‚ñé       | 183/800 [02:12<14:12,  1.38s/it] 23%|‚ñà‚ñà‚ñé       | 184/800 [02:12<10:51,  1.06s/it] 23%|‚ñà‚ñà‚ñé       | 185/800 [02:12<08:30,  1.20it/s] 23%|‚ñà‚ñà‚ñé       | 186/800 [02:13<06:52,  1.49it/s] 23%|‚ñà‚ñà‚ñé       | 187/800 [02:13<05:43,  1.78it/s] 24%|‚ñà‚ñà‚ñé       | 188/800 [02:13<04:55,  2.07it/s] 24%|‚ñà‚ñà‚ñé       | 189/800 [02:14<04:21,  2.34it/s] 24%|‚ñà‚ñà‚ñç       | 190/800 [02:14<03:58,  2.56it/s]                                                  24%|‚ñà‚ñà‚ñç       | 190/800 [02:14<03:58,  2.56it/s] 24%|‚ñà‚ñà‚ñç       | 191/800 [02:14<03:42,  2.74it/s] 24%|‚ñà‚ñà‚ñç       | 192/800 [02:15<03:30,  2.89it/s] 24%|‚ñà‚ñà‚ñç       | 193/800 [02:15<03:22,  3.00it/s] 24%|‚ñà‚ñà‚ñç       | 194/800 [02:15<03:16,  3.09it/s] 24%|‚ñà‚ñà‚ñç       | 195/800 [02:15<03:11,  3.15it/s] 24%|‚ñà‚ñà‚ñç       | 196/800 [02:16<03:08,  3.20it/s] 25%|‚ñà‚ñà‚ñç       | 197/800 [02:16<03:06,  3.23it/s] 25%|‚ñà‚ñà‚ñç       | 198/800 [02:16<03:04,  3.26it/s] 25%|‚ñà‚ñà‚ñç       | 199/800 [02:17<03:03,  3.28it/s] 25%|‚ñà‚ñà‚ñå       | 200/800 [02:17<02:57,  3.38it/s]                                                  25%|‚ñà‚ñà‚ñå       | 200/800 [02:17<02:57,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:44:40,631 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:44:40,635 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:44:40,637 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:44:40,639 >>   Batch size = 8
{'eval_loss': 2.3176944255828857, 'eval_accuracy': 0.43478260869565216, 'eval_runtime': 1.4243, 'eval_samples_per_second': 145.338, 'eval_steps_per_second': 18.255, 'epoch': 9.0}
{'loss': 1.2831, 'grad_norm': 22.142520904541016, 'learning_rate': 7.625e-06, 'epoch': 9.5}
{'loss': 1.209, 'grad_norm': 15.112480163574219, 'learning_rate': 7.500000000000001e-06, 'epoch': 10.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.21it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.98it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.39it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.68it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.33it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.11it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.91it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.77it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.69it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.60it/s][A                                                 
                                               [A 25%|‚ñà‚ñà‚ñå       | 200/800 [02:18<02:57,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.60it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:44:42,085 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-200
[INFO|configuration_utils.py:471] 2024-05-15 00:44:42,092 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-200/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:44:44,162 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-200/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:44:44,171 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:44:44,177 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-200/special_tokens_map.json
 25%|‚ñà‚ñà‚ñå       | 201/800 [02:25<24:51,  2.49s/it] 25%|‚ñà‚ñà‚ñå       | 202/800 [02:25<18:15,  1.83s/it] 25%|‚ñà‚ñà‚ñå       | 203/800 [02:25<13:39,  1.37s/it] 26%|‚ñà‚ñà‚ñå       | 204/800 [02:25<10:25,  1.05s/it] 26%|‚ñà‚ñà‚ñå       | 205/800 [02:26<08:11,  1.21it/s] 26%|‚ñà‚ñà‚ñå       | 206/800 [02:26<06:37,  1.50it/s] 26%|‚ñà‚ñà‚ñå       | 207/800 [02:26<05:31,  1.79it/s] 26%|‚ñà‚ñà‚ñå       | 208/800 [02:27<04:44,  2.08it/s] 26%|‚ñà‚ñà‚ñå       | 209/800 [02:27<04:12,  2.34it/s] 26%|‚ñà‚ñà‚ñã       | 210/800 [02:27<03:51,  2.55it/s]                                                  26%|‚ñà‚ñà‚ñã       | 210/800 [02:27<03:51,  2.55it/s] 26%|‚ñà‚ñà‚ñã       | 211/800 [02:28<03:35,  2.73it/s] 26%|‚ñà‚ñà‚ñã       | 212/800 [02:28<03:23,  2.89it/s] 27%|‚ñà‚ñà‚ñã       | 213/800 [02:28<03:15,  3.00it/s] 27%|‚ñà‚ñà‚ñã       | 214/800 [02:29<03:09,  3.09it/s] 27%|‚ñà‚ñà‚ñã       | 215/800 [02:29<03:05,  3.15it/s] 27%|‚ñà‚ñà‚ñã       | 216/800 [02:29<03:02,  3.20it/s] 27%|‚ñà‚ñà‚ñã       | 217/800 [02:29<03:00,  3.23it/s] 27%|‚ñà‚ñà‚ñã       | 218/800 [02:30<02:58,  3.25it/s] 27%|‚ñà‚ñà‚ñã       | 219/800 [02:30<02:57,  3.27it/s] 28%|‚ñà‚ñà‚ñä       | 220/800 [02:30<02:52,  3.37it/s]                                                  28%|‚ñà‚ñà‚ñä       | 220/800 [02:30<02:52,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:44:53,950 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:44:53,953 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:44:53,955 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:44:53,957 >>   Batch size = 8
{'eval_loss': 2.188095808029175, 'eval_accuracy': 0.46859903381642515, 'eval_runtime': 1.4221, 'eval_samples_per_second': 145.559, 'eval_steps_per_second': 18.283, 'epoch': 10.0}
{'loss': 1.0764, 'grad_norm': 17.210357666015625, 'learning_rate': 7.375000000000001e-06, 'epoch': 10.5}
{'loss': 0.9567, 'grad_norm': 14.844550132751465, 'learning_rate': 7.25e-06, 'epoch': 11.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.18it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.81it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.26it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.58it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.24it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.04it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.87it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.73it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.67it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.60it/s][A                                                 
                                               [A 28%|‚ñà‚ñà‚ñä       | 220/800 [02:32<02:52,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.60it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:44:55,422 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-220
[INFO|configuration_utils.py:471] 2024-05-15 00:44:55,428 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-220/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:44:57,416 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-220/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:44:57,427 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-220/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:44:57,441 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-220/special_tokens_map.json
 28%|‚ñà‚ñà‚ñä       | 221/800 [02:38<23:57,  2.48s/it] 28%|‚ñà‚ñà‚ñä       | 222/800 [02:38<17:36,  1.83s/it] 28%|‚ñà‚ñà‚ñä       | 223/800 [02:38<13:10,  1.37s/it] 28%|‚ñà‚ñà‚ñä       | 224/800 [02:39<10:04,  1.05s/it] 28%|‚ñà‚ñà‚ñä       | 225/800 [02:39<07:54,  1.21it/s] 28%|‚ñà‚ñà‚ñä       | 226/800 [02:39<06:23,  1.50it/s] 28%|‚ñà‚ñà‚ñä       | 227/800 [02:40<05:19,  1.79it/s] 28%|‚ñà‚ñà‚ñä       | 228/800 [02:40<04:35,  2.08it/s] 29%|‚ñà‚ñà‚ñä       | 229/800 [02:40<04:04,  2.34it/s] 29%|‚ñà‚ñà‚ñâ       | 230/800 [02:41<03:42,  2.56it/s]                                                  29%|‚ñà‚ñà‚ñâ       | 230/800 [02:41<03:42,  2.56it/s] 29%|‚ñà‚ñà‚ñâ       | 231/800 [02:41<03:27,  2.74it/s] 29%|‚ñà‚ñà‚ñâ       | 232/800 [02:41<03:16,  2.89it/s] 29%|‚ñà‚ñà‚ñâ       | 233/800 [02:42<03:08,  3.01it/s] 29%|‚ñà‚ñà‚ñâ       | 234/800 [02:42<03:03,  3.09it/s] 29%|‚ñà‚ñà‚ñâ       | 235/800 [02:42<02:59,  3.15it/s] 30%|‚ñà‚ñà‚ñâ       | 236/800 [02:42<02:56,  3.20it/s] 30%|‚ñà‚ñà‚ñâ       | 237/800 [02:43<02:54,  3.23it/s] 30%|‚ñà‚ñà‚ñâ       | 238/800 [02:43<02:52,  3.26it/s] 30%|‚ñà‚ñà‚ñâ       | 239/800 [02:43<02:51,  3.27it/s] 30%|‚ñà‚ñà‚ñà       | 240/800 [02:44<02:45,  3.37it/s]                                                  30%|‚ñà‚ñà‚ñà       | 240/800 [02:44<02:45,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:45:07,245 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:45:07,249 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:45:07,251 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:45:07,253 >>   Batch size = 8
{'eval_loss': 2.0543761253356934, 'eval_accuracy': 0.48792270531400966, 'eval_runtime': 1.4416, 'eval_samples_per_second': 143.592, 'eval_steps_per_second': 18.036, 'epoch': 11.0}
{'loss': 0.842, 'grad_norm': 16.15976905822754, 'learning_rate': 7.125e-06, 'epoch': 11.5}
{'loss': 0.7912, 'grad_norm': 17.444101333618164, 'learning_rate': 7e-06, 'epoch': 12.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.30it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.96it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.36it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.68it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.30it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.08it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.87it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.74it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.66it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.63it/s][A                                                 
                                               [A 30%|‚ñà‚ñà‚ñà       | 240/800 [02:45<02:45,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.63it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:45:08,697 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-240
[INFO|configuration_utils.py:471] 2024-05-15 00:45:08,704 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-240/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:45:10,698 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-240/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:45:10,707 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:45:10,713 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-240/special_tokens_map.json
 30%|‚ñà‚ñà‚ñà       | 241/800 [02:51<23:27,  2.52s/it] 30%|‚ñà‚ñà‚ñà       | 242/800 [02:52<17:14,  1.85s/it] 30%|‚ñà‚ñà‚ñà       | 243/800 [02:52<12:53,  1.39s/it] 30%|‚ñà‚ñà‚ñà       | 244/800 [02:52<09:50,  1.06s/it] 31%|‚ñà‚ñà‚ñà       | 245/800 [02:53<07:42,  1.20it/s] 31%|‚ñà‚ñà‚ñà       | 246/800 [02:53<06:13,  1.48it/s] 31%|‚ñà‚ñà‚ñà       | 247/800 [02:53<05:10,  1.78it/s] 31%|‚ñà‚ñà‚ñà       | 248/800 [02:53<04:27,  2.06it/s] 31%|‚ñà‚ñà‚ñà       | 249/800 [02:54<03:56,  2.33it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 250/800 [02:54<03:35,  2.56it/s]                                                  31%|‚ñà‚ñà‚ñà‚ñè      | 250/800 [02:54<03:35,  2.56it/s] 31%|‚ñà‚ñà‚ñà‚ñè      | 251/800 [02:54<03:21,  2.73it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 252/800 [02:55<03:10,  2.88it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 253/800 [02:55<03:02,  3.00it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 254/800 [02:55<02:56,  3.09it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 255/800 [02:56<02:52,  3.15it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 256/800 [02:56<02:50,  3.20it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 257/800 [02:56<02:47,  3.23it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 258/800 [02:56<02:46,  3.26it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 259/800 [02:57<02:45,  3.27it/s] 32%|‚ñà‚ñà‚ñà‚ñé      | 260/800 [02:57<02:40,  3.37it/s]                                                  32%|‚ñà‚ñà‚ñà‚ñé      | 260/800 [02:57<02:40,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:45:20,662 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:45:20,666 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:45:20,668 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:45:20,670 >>   Batch size = 8
{'eval_loss': 1.9823886156082153, 'eval_accuracy': 0.5072463768115942, 'eval_runtime': 1.4228, 'eval_samples_per_second': 145.493, 'eval_steps_per_second': 18.274, 'epoch': 12.0}
{'loss': 0.6611, 'grad_norm': 12.574400901794434, 'learning_rate': 6.875e-06, 'epoch': 12.5}
{'loss': 0.6671, 'grad_norm': 16.02497100830078, 'learning_rate': 6.750000000000001e-06, 'epoch': 13.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.12it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.94it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.07it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.53it/s][A
 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:00<00:00, 19.20it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:00<00:00, 18.99it/s][A
 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:00<00:00, 18.82it/s][A
 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:01<00:00, 18.69it/s][A
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:01<00:00, 18.54it/s][A
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [00:01<00:00, 18.52it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.67it/s][A                                                 
                                               [A 32%|‚ñà‚ñà‚ñà‚ñé      | 260/800 [02:58<02:40,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.67it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:45:22,129 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-260
[INFO|configuration_utils.py:471] 2024-05-15 00:45:22,136 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-260/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:45:24,108 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-260/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:45:24,117 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-260/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:45:24,121 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-260/special_tokens_map.json
 33%|‚ñà‚ñà‚ñà‚ñé      | 261/800 [03:05<22:13,  2.47s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 262/800 [03:05<16:19,  1.82s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 263/800 [03:05<12:13,  1.37s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 264/800 [03:05<09:20,  1.05s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 265/800 [03:06<07:20,  1.22it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 266/800 [03:06<05:55,  1.50it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 267/800 [03:06<04:56,  1.79it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 268/800 [03:07<04:15,  2.08it/s] 34%|‚ñà‚ñà‚ñà‚ñé      | 269/800 [03:07<03:46,  2.34it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 270/800 [03:07<03:26,  2.57it/s]                                                  34%|‚ñà‚ñà‚ñà‚ñç      | 270/800 [03:07<03:26,  2.57it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 271/800 [03:08<03:12,  2.74it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 272/800 [03:08<03:02,  2.89it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 273/800 [03:08<02:55,  3.00it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 274/800 [03:08<02:50,  3.09it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 275/800 [03:09<02:46,  3.15it/s] 34%|‚ñà‚ñà‚ñà‚ñç      | 276/800 [03:09<02:43,  3.20it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 277/800 [03:09<02:41,  3.23it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 278/800 [03:10<02:40,  3.26it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 279/800 [03:10<02:39,  3.27it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 280/800 [03:10<02:34,  3.38it/s]                                                  35%|‚ñà‚ñà‚ñà‚ñå      | 280/800 [03:10<02:34,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:45:33,927 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:45:33,931 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:45:33,933 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:45:33,934 >>   Batch size = 8
{'eval_loss': 1.9554790258407593, 'eval_accuracy': 0.5265700483091788, 'eval_runtime': 1.4354, 'eval_samples_per_second': 144.214, 'eval_steps_per_second': 18.114, 'epoch': 13.0}
{'loss': 0.5491, 'grad_norm': 12.238454818725586, 'learning_rate': 6.625e-06, 'epoch': 13.5}
{'loss': 0.5233, 'grad_norm': 12.311787605285645, 'learning_rate': 6.5000000000000004e-06, 'epoch': 14.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.13it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.94it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.33it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.61it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.30it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.04it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.91it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.76it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.68it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.66it/s][A                                                 
                                               [A 35%|‚ñà‚ñà‚ñà‚ñå      | 280/800 [03:12<02:34,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.66it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:45:35,380 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-280
[INFO|configuration_utils.py:471] 2024-05-15 00:45:35,386 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-280/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:45:37,354 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-280/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:45:37,363 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:45:37,369 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-280/special_tokens_map.json
 35%|‚ñà‚ñà‚ñà‚ñå      | 281/800 [03:18<21:12,  2.45s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 282/800 [03:18<15:35,  1.81s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 283/800 [03:18<11:40,  1.35s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 284/800 [03:19<08:55,  1.04s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 285/800 [03:19<07:00,  1.22it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 286/800 [03:19<05:40,  1.51it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 287/800 [03:20<04:44,  1.80it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 288/800 [03:20<04:05,  2.09it/s] 36%|‚ñà‚ñà‚ñà‚ñå      | 289/800 [03:20<03:37,  2.35it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 290/800 [03:20<03:18,  2.57it/s]                                                  36%|‚ñà‚ñà‚ñà‚ñã      | 290/800 [03:20<03:18,  2.57it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 291/800 [03:21<03:05,  2.74it/s] 36%|‚ñà‚ñà‚ñà‚ñã      | 292/800 [03:21<02:55,  2.89it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 293/800 [03:21<02:48,  3.00it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 294/800 [03:22<02:43,  3.09it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 295/800 [03:22<02:40,  3.15it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 296/800 [03:22<02:37,  3.20it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 297/800 [03:23<02:35,  3.23it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 298/800 [03:23<02:34,  3.25it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 299/800 [03:23<02:33,  3.27it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 300/800 [03:23<02:28,  3.37it/s]                                                  38%|‚ñà‚ñà‚ñà‚ñä      | 300/800 [03:23<02:28,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:45:47,124 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:45:47,128 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:45:47,130 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:45:47,131 >>   Batch size = 8
{'eval_loss': 1.872273564338684, 'eval_accuracy': 0.5072463768115942, 'eval_runtime': 1.4233, 'eval_samples_per_second': 145.432, 'eval_steps_per_second': 18.267, 'epoch': 14.0}
{'loss': 0.4704, 'grad_norm': 16.147254943847656, 'learning_rate': 6.375e-06, 'epoch': 14.5}
{'loss': 0.3754, 'grad_norm': 15.965374946594238, 'learning_rate': 6.25e-06, 'epoch': 15.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.17it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.90it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.34it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.68it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.30it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.08it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.88it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.76it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.70it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.65it/s][A                                                 
                                               [A 38%|‚ñà‚ñà‚ñà‚ñä      | 300/800 [03:25<02:28,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.65it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:45:48,576 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-300
[INFO|configuration_utils.py:471] 2024-05-15 00:45:48,583 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-300/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:45:50,541 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-300/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:45:50,549 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:45:50,554 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-300/special_tokens_map.json
 38%|‚ñà‚ñà‚ñà‚ñä      | 301/800 [03:31<20:31,  2.47s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 302/800 [03:31<15:05,  1.82s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 303/800 [03:32<11:17,  1.36s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 304/800 [03:32<08:38,  1.04s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 305/800 [03:32<06:46,  1.22it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 306/800 [03:33<05:28,  1.50it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 307/800 [03:33<04:34,  1.80it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 308/800 [03:33<03:56,  2.08it/s] 39%|‚ñà‚ñà‚ñà‚ñä      | 309/800 [03:33<03:29,  2.34it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 310/800 [03:34<03:10,  2.57it/s]                                                  39%|‚ñà‚ñà‚ñà‚ñâ      | 310/800 [03:34<03:10,  2.57it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 311/800 [03:34<02:58,  2.74it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 312/800 [03:34<02:48,  2.89it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 313/800 [03:35<02:41,  3.01it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 314/800 [03:35<02:37,  3.10it/s] 39%|‚ñà‚ñà‚ñà‚ñâ      | 315/800 [03:35<02:33,  3.16it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 316/800 [03:36<02:31,  3.20it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 317/800 [03:36<02:29,  3.23it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 318/800 [03:36<02:28,  3.26it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 319/800 [03:36<02:26,  3.27it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 320/800 [03:37<02:22,  3.38it/s]                                                  40%|‚ñà‚ñà‚ñà‚ñà      | 320/800 [03:37<02:22,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:46:00,370 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:46:00,374 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:46:00,375 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:46:00,377 >>   Batch size = 8
{'eval_loss': 1.8496708869934082, 'eval_accuracy': 0.5120772946859904, 'eval_runtime': 1.4215, 'eval_samples_per_second': 145.617, 'eval_steps_per_second': 18.29, 'epoch': 15.0}
{'loss': 0.326, 'grad_norm': 10.371529579162598, 'learning_rate': 6.125000000000001e-06, 'epoch': 15.5}
{'loss': 0.3316, 'grad_norm': 12.642411231994629, 'learning_rate': 6e-06, 'epoch': 16.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.11it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.92it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.31it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.65it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.32it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.07it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.90it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.77it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.68it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.61it/s][A                                                 
                                               [A 40%|‚ñà‚ñà‚ñà‚ñà      | 320/800 [03:38<02:22,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.61it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:46:01,820 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-320
[INFO|configuration_utils.py:471] 2024-05-15 00:46:01,826 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-320/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:46:03,810 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-320/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:46:03,819 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:46:03,824 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-320/special_tokens_map.json
 40%|‚ñà‚ñà‚ñà‚ñà      | 321/800 [03:44<19:48,  2.48s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 322/800 [03:45<14:33,  1.83s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 323/800 [03:45<10:52,  1.37s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 324/800 [03:45<08:19,  1.05s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 325/800 [03:45<06:31,  1.21it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 326/800 [03:46<05:16,  1.50it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 327/800 [03:46<04:23,  1.79it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 328/800 [03:46<03:46,  2.08it/s] 41%|‚ñà‚ñà‚ñà‚ñà      | 329/800 [03:47<03:21,  2.34it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 330/800 [03:47<03:03,  2.57it/s]                                                  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 330/800 [03:47<03:03,  2.57it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 331/800 [03:47<02:51,  2.74it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 332/800 [03:48<02:42,  2.89it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 333/800 [03:48<02:35,  3.00it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 334/800 [03:48<02:30,  3.09it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 335/800 [03:49<02:27,  3.15it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 336/800 [03:49<02:25,  3.20it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 337/800 [03:49<02:23,  3.23it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 338/800 [03:49<02:22,  3.25it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 339/800 [03:50<02:20,  3.27it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 340/800 [03:50<02:16,  3.38it/s]                                                  42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 340/800 [03:50<02:16,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:46:13,657 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:46:13,661 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:46:13,663 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:46:13,665 >>   Batch size = 8
{'eval_loss': 1.7939797639846802, 'eval_accuracy': 0.5314009661835749, 'eval_runtime': 1.4202, 'eval_samples_per_second': 145.75, 'eval_steps_per_second': 18.307, 'epoch': 16.0}
{'loss': 0.2819, 'grad_norm': 9.911325454711914, 'learning_rate': 5.8750000000000005e-06, 'epoch': 16.5}
{'loss': 0.2608, 'grad_norm': 6.4465742111206055, 'learning_rate': 5.75e-06, 'epoch': 17.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.15it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.92it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.29it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.63it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.28it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.05it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.86it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.77it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.68it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.58it/s][A                                                 
                                               [A 42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 340/800 [03:51<02:16,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.58it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:46:15,115 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-340
[INFO|configuration_utils.py:471] 2024-05-15 00:46:15,121 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-340/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:46:17,113 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-340/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:46:17,121 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:46:17,126 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-340/special_tokens_map.json
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 341/800 [03:58<19:15,  2.52s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 342/800 [03:58<14:08,  1.85s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 343/800 [03:58<10:33,  1.39s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 344/800 [03:59<08:03,  1.06s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 345/800 [03:59<06:19,  1.20it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 346/800 [03:59<05:05,  1.48it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 347/800 [04:00<04:14,  1.78it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 348/800 [04:00<03:38,  2.07it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 349/800 [04:00<03:13,  2.33it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 350/800 [04:00<02:56,  2.55it/s]                                                  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 350/800 [04:00<02:56,  2.55it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 351/800 [04:01<02:44,  2.73it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 352/800 [04:01<02:35,  2.88it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 353/800 [04:01<02:29,  3.00it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 354/800 [04:02<02:24,  3.08it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 355/800 [04:02<02:21,  3.15it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 356/800 [04:02<02:19,  3.19it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 357/800 [04:03<02:17,  3.22it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 358/800 [04:03<02:15,  3.25it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 359/800 [04:03<02:14,  3.27it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 360/800 [04:03<02:10,  3.37it/s]                                                  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 360/800 [04:03<02:10,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:46:27,072 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:46:27,076 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:46:27,078 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:46:27,080 >>   Batch size = 8
{'eval_loss': 1.78118097782135, 'eval_accuracy': 0.5314009661835749, 'eval_runtime': 1.4253, 'eval_samples_per_second': 145.238, 'eval_steps_per_second': 18.242, 'epoch': 17.0}
{'loss': 0.2166, 'grad_norm': 5.573710918426514, 'learning_rate': 5.625e-06, 'epoch': 17.5}
{'loss': 0.2139, 'grad_norm': 7.422945976257324, 'learning_rate': 5.500000000000001e-06, 'epoch': 18.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.10it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.94it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.18it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.61it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.26it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 18.98it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.82it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.68it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.61it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.54it/s][A                                                 
                                               [A 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 360/800 [04:05<02:10,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.54it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:46:28,530 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-360
[INFO|configuration_utils.py:471] 2024-05-15 00:46:28,573 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-360/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:46:30,563 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-360/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:46:30,573 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:46:30,578 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-360/special_tokens_map.json
 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 361/800 [04:11<18:12,  2.49s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 362/800 [04:11<13:22,  1.83s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 363/800 [04:12<09:59,  1.37s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 364/800 [04:12<07:38,  1.05s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 365/800 [04:12<05:59,  1.21it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 366/800 [04:13<04:50,  1.49it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 367/800 [04:13<04:01,  1.79it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 368/800 [04:13<03:28,  2.08it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 369/800 [04:13<03:04,  2.34it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 370/800 [04:14<02:47,  2.56it/s]                                                  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 370/800 [04:14<02:47,  2.56it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 371/800 [04:14<02:37,  2.73it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 372/800 [04:14<02:28,  2.89it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 373/800 [04:15<02:22,  3.00it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 374/800 [04:15<02:17,  3.09it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 375/800 [04:15<02:14,  3.15it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 376/800 [04:16<02:12,  3.20it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 377/800 [04:16<02:11,  3.23it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 378/800 [04:16<02:09,  3.25it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 379/800 [04:16<02:08,  3.27it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 380/800 [04:17<02:04,  3.37it/s]                                                  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 380/800 [04:17<02:04,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:46:40,389 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:46:40,393 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:46:40,395 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:46:40,397 >>   Batch size = 8
{'eval_loss': 1.7642722129821777, 'eval_accuracy': 0.5169082125603864, 'eval_runtime': 1.4296, 'eval_samples_per_second': 144.792, 'eval_steps_per_second': 18.186, 'epoch': 18.0}
{'loss': 0.1548, 'grad_norm': 4.059175968170166, 'learning_rate': 5.375e-06, 'epoch': 18.5}
{'loss': 0.1768, 'grad_norm': 6.729890823364258, 'learning_rate': 5.2500000000000006e-06, 'epoch': 19.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.30it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.96it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.11it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.54it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.23it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.00it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.84it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.75it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.68it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.61it/s][A                                                 
                                               [A 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 380/800 [04:18<02:04,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.61it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:46:41,847 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-380
[INFO|configuration_utils.py:471] 2024-05-15 00:46:41,853 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-380/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:46:43,936 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-380/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:46:43,945 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-380/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:46:43,951 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-380/special_tokens_map.json
 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 381/800 [04:24<17:29,  2.51s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 382/800 [04:25<12:50,  1.84s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 383/800 [04:25<09:35,  1.38s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 384/800 [04:25<07:19,  1.06s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 385/800 [04:26<05:44,  1.20it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 386/800 [04:26<04:38,  1.49it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 387/800 [04:26<03:51,  1.78it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 388/800 [04:27<03:19,  2.07it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 389/800 [04:27<02:56,  2.33it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 390/800 [04:27<02:40,  2.56it/s]                                                  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 390/800 [04:27<02:40,  2.56it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 391/800 [04:27<02:29,  2.73it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 392/800 [04:28<02:21,  2.88it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 393/800 [04:28<02:15,  3.00it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 394/800 [04:28<02:11,  3.08it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 395/800 [04:29<02:08,  3.15it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 396/800 [04:29<02:06,  3.19it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 397/800 [04:29<02:04,  3.22it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 398/800 [04:30<02:03,  3.25it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 399/800 [04:30<02:02,  3.26it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 400/800 [04:30<01:58,  3.37it/s]                                                  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 400/800 [04:30<01:58,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:46:53,768 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:46:53,772 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:46:53,774 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:46:53,776 >>   Batch size = 8
{'eval_loss': 1.7602332830429077, 'eval_accuracy': 0.5507246376811594, 'eval_runtime': 1.4271, 'eval_samples_per_second': 145.049, 'eval_steps_per_second': 18.219, 'epoch': 19.0}
{'loss': 0.135, 'grad_norm': 4.990705966949463, 'learning_rate': 5.125e-06, 'epoch': 19.5}
{'loss': 0.1223, 'grad_norm': 3.0085973739624023, 'learning_rate': 5e-06, 'epoch': 20.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.02it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.92it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.31it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.60it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.27it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 18.99it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.82it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.67it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.56it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.56it/s][A                                                 
                                               [A 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 400/800 [04:32<01:58,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.56it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:46:55,225 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-400
[INFO|configuration_utils.py:471] 2024-05-15 00:46:55,232 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-400/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:46:57,216 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-400/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:46:57,225 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:46:57,231 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-400/special_tokens_map.json
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 401/800 [04:38<16:27,  2.48s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 402/800 [04:38<12:05,  1.82s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 403/800 [04:38<09:02,  1.37s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 404/800 [04:39<06:54,  1.05s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 405/800 [04:39<05:25,  1.21it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 406/800 [04:39<04:22,  1.50it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 407/800 [04:39<03:39,  1.79it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 408/800 [04:40<03:08,  2.08it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 409/800 [04:40<02:47,  2.34it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 410/800 [04:40<02:32,  2.56it/s]                                                  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 410/800 [04:40<02:32,  2.56it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 411/800 [04:41<02:22,  2.74it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 412/800 [04:41<02:14,  2.89it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 413/800 [04:41<02:08,  3.00it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 414/800 [04:42<02:05,  3.08it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 415/800 [04:42<02:02,  3.15it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 416/800 [04:42<02:00,  3.20it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 417/800 [04:43<01:58,  3.22it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 418/800 [04:43<01:57,  3.25it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 419/800 [04:43<01:56,  3.27it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 420/800 [04:43<01:52,  3.37it/s]                                                  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 420/800 [04:43<01:52,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:47:07,045 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:47:07,049 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:47:07,051 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:47:07,052 >>   Batch size = 8
{'eval_loss': 1.7330666780471802, 'eval_accuracy': 0.5507246376811594, 'eval_runtime': 1.4268, 'eval_samples_per_second': 145.085, 'eval_steps_per_second': 18.223, 'epoch': 20.0}
{'loss': 0.12, 'grad_norm': 3.028963088989258, 'learning_rate': 4.875e-06, 'epoch': 20.5}
{'loss': 0.0943, 'grad_norm': 4.148632049560547, 'learning_rate': 4.75e-06, 'epoch': 21.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.28it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.94it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.32it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.64it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.27it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.01it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.85it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.72it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.64it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.62it/s][A                                                 
                                               [A 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 420/800 [04:45<01:52,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.62it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:47:08,497 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-420
[INFO|configuration_utils.py:471] 2024-05-15 00:47:08,504 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-420/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:47:10,484 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-420/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:47:10,493 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:47:10,499 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-420/special_tokens_map.json
 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 421/800 [04:51<15:44,  2.49s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 422/800 [04:51<11:32,  1.83s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 423/800 [04:52<08:37,  1.37s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 424/800 [04:52<06:35,  1.05s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 425/800 [04:52<05:10,  1.21it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 426/800 [04:53<04:10,  1.49it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 427/800 [04:53<03:28,  1.79it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 428/800 [04:53<02:59,  2.08it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 429/800 [04:53<02:38,  2.34it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 430/800 [04:54<02:24,  2.56it/s]                                                  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 430/800 [04:54<02:24,  2.56it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 431/800 [04:54<02:14,  2.74it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 432/800 [04:54<02:07,  2.88it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 433/800 [04:55<02:02,  3.01it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 434/800 [04:55<01:58,  3.09it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 435/800 [04:55<01:55,  3.15it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 436/800 [04:56<01:53,  3.20it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 437/800 [04:56<01:52,  3.23it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 438/800 [04:56<01:51,  3.25it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 439/800 [04:56<01:50,  3.27it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 440/800 [04:57<01:46,  3.38it/s]                                                  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 440/800 [04:57<01:46,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:47:20,365 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:47:20,368 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:47:20,370 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:47:20,372 >>   Batch size = 8
{'eval_loss': 1.7419999837875366, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4232, 'eval_samples_per_second': 145.442, 'eval_steps_per_second': 18.268, 'epoch': 21.0}
{'loss': 0.0891, 'grad_norm': 2.853480815887451, 'learning_rate': 4.625000000000001e-06, 'epoch': 21.5}
{'loss': 0.0888, 'grad_norm': 2.53889799118042, 'learning_rate': 4.5e-06, 'epoch': 22.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.06it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.97it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.36it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.71it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.34it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.10it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.94it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.82it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.76it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.67it/s][A                                                 
                                               [A 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 440/800 [04:58<01:46,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.67it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:47:21,809 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-440
[INFO|configuration_utils.py:471] 2024-05-15 00:47:21,815 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-440/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:47:23,815 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-440/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:47:23,824 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-440/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:47:23,830 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-440/special_tokens_map.json
 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 441/800 [05:04<14:52,  2.49s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 442/800 [05:05<10:55,  1.83s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 443/800 [05:05<08:09,  1.37s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 444/800 [05:05<06:14,  1.05s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 445/800 [05:06<04:53,  1.21it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 446/800 [05:06<03:56,  1.49it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 447/800 [05:06<03:17,  1.79it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 448/800 [05:06<02:49,  2.07it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 449/800 [05:07<02:30,  2.34it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 450/800 [05:07<02:16,  2.56it/s]                                                  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 450/800 [05:07<02:16,  2.56it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 451/800 [05:07<02:07,  2.73it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 452/800 [05:08<02:00,  2.89it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 453/800 [05:08<01:55,  3.00it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 454/800 [05:08<01:52,  3.09it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 455/800 [05:09<01:49,  3.14it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 456/800 [05:09<01:47,  3.20it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 457/800 [05:09<01:46,  3.23it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 458/800 [05:09<01:45,  3.25it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 459/800 [05:10<01:44,  3.27it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 460/800 [05:10<01:40,  3.38it/s]                                                  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 460/800 [05:10<01:40,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:47:33,680 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:47:33,683 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:47:33,685 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:47:33,687 >>   Batch size = 8
{'eval_loss': 1.7405375242233276, 'eval_accuracy': 0.5410628019323671, 'eval_runtime': 1.4165, 'eval_samples_per_second': 146.133, 'eval_steps_per_second': 18.355, 'epoch': 22.0}
{'loss': 0.0721, 'grad_norm': 1.9914536476135254, 'learning_rate': 4.3750000000000005e-06, 'epoch': 22.5}
{'loss': 0.0689, 'grad_norm': 2.5989325046539307, 'learning_rate': 4.25e-06, 'epoch': 23.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.07it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.78it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.28it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.68it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.40it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.19it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.98it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.87it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.81it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.70it/s][A                                                 
                                               [A 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 460/800 [05:11<01:40,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.70it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:47:35,124 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-460
[INFO|configuration_utils.py:471] 2024-05-15 00:47:35,129 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-460/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:47:37,075 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-460/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:47:37,083 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-460/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:47:37,088 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-460/special_tokens_map.json
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 461/800 [05:17<13:45,  2.44s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 462/800 [05:18<10:06,  1.79s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 463/800 [05:18<07:33,  1.35s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 464/800 [05:18<05:46,  1.03s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 465/800 [05:19<04:32,  1.23it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 466/800 [05:19<03:40,  1.52it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 467/800 [05:19<03:03,  1.81it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 468/800 [05:20<02:38,  2.09it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 469/800 [05:20<02:20,  2.35it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 470/800 [05:20<02:08,  2.58it/s]                                                  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 470/800 [05:20<02:08,  2.58it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 471/800 [05:20<01:59,  2.75it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 472/800 [05:21<01:53,  2.90it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 473/800 [05:21<01:48,  3.01it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 474/800 [05:21<01:45,  3.09it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 475/800 [05:22<01:43,  3.15it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 476/800 [05:22<01:41,  3.20it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 477/800 [05:22<01:40,  3.23it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 478/800 [05:23<01:39,  3.25it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 479/800 [05:23<01:38,  3.27it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 480/800 [05:23<01:34,  3.37it/s]                                                  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 480/800 [05:23<01:34,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:47:46,821 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:47:46,825 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:47:46,826 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:47:46,827 >>   Batch size = 8
{'eval_loss': 1.7343838214874268, 'eval_accuracy': 0.5314009661835749, 'eval_runtime': 1.4159, 'eval_samples_per_second': 146.197, 'eval_steps_per_second': 18.363, 'epoch': 23.0}
{'loss': 0.0621, 'grad_norm': 2.6775898933410645, 'learning_rate': 4.125e-06, 'epoch': 23.5}
{'loss': 0.056, 'grad_norm': 1.2823213338851929, 'learning_rate': 4.000000000000001e-06, 'epoch': 24.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 28.98it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.99it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.39it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.72it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.35it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.17it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.92it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.77it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.71it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.68it/s][A                                                 
                                               [A 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 480/800 [05:25<01:34,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.68it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:47:48,256 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-480
[INFO|configuration_utils.py:471] 2024-05-15 00:47:48,260 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-480/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:47:50,151 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-480/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:47:50,159 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:47:50,164 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-480/special_tokens_map.json
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 481/800 [05:31<12:55,  2.43s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 482/800 [05:31<09:29,  1.79s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 483/800 [05:31<07:06,  1.35s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 484/800 [05:31<05:26,  1.03s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 485/800 [05:32<04:16,  1.23it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 486/800 [05:32<03:27,  1.52it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 487/800 [05:32<02:52,  1.81it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 488/800 [05:33<02:28,  2.10it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 489/800 [05:33<02:12,  2.35it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 490/800 [05:33<02:00,  2.58it/s]                                                  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 490/800 [05:33<02:00,  2.58it/s] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 491/800 [05:34<01:52,  2.75it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 492/800 [05:34<01:46,  2.90it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 493/800 [05:34<01:41,  3.01it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 494/800 [05:35<01:38,  3.09it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 495/800 [05:35<01:36,  3.16it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 496/800 [05:35<01:35,  3.20it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 497/800 [05:35<01:33,  3.23it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 498/800 [05:36<01:32,  3.25it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 499/800 [05:36<01:32,  3.27it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 500/800 [05:36<01:28,  3.38it/s]                                                  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 500/800 [05:36<01:28,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:47:59,949 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:47:59,953 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:47:59,955 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:47:59,957 >>   Batch size = 8
{'eval_loss': 1.7349575757980347, 'eval_accuracy': 0.5555555555555556, 'eval_runtime': 1.412, 'eval_samples_per_second': 146.602, 'eval_steps_per_second': 18.414, 'epoch': 24.0}
{'loss': 0.0548, 'grad_norm': 2.415985345840454, 'learning_rate': 3.875e-06, 'epoch': 24.5}
{'loss': 0.0465, 'grad_norm': 7.516856670379639, 'learning_rate': 3.7500000000000005e-06, 'epoch': 25.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.18it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.97it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.37it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.68it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.29it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.05it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.87it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.74it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.64it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.61it/s][A                                                 
                                               [A 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 500/800 [05:38<01:28,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.61it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:48:01,404 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-500
[INFO|configuration_utils.py:471] 2024-05-15 00:48:01,411 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-500/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:48:03,420 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-500/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:48:03,429 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:48:03,435 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-500/special_tokens_map.json
 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 501/800 [05:44<12:24,  2.49s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 502/800 [05:44<09:06,  1.83s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 503/800 [05:45<06:48,  1.37s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 504/800 [05:45<05:11,  1.05s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 505/800 [05:45<04:03,  1.21it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 506/800 [05:45<03:16,  1.49it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 507/800 [05:46<02:43,  1.79it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 508/800 [05:46<02:20,  2.08it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 509/800 [05:46<02:04,  2.34it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 510/800 [05:47<01:52,  2.57it/s]                                                  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 510/800 [05:47<01:52,  2.57it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 511/800 [05:47<01:45,  2.74it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 512/800 [05:47<01:39,  2.89it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 513/800 [05:48<01:35,  3.00it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 514/800 [05:48<01:32,  3.09it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 515/800 [05:48<01:30,  3.16it/s] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 516/800 [05:48<01:28,  3.20it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 517/800 [05:49<01:27,  3.23it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 518/800 [05:49<01:26,  3.25it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 519/800 [05:49<01:25,  3.27it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 520/800 [05:50<01:22,  3.38it/s]                                                  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 520/800 [05:50<01:22,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:48:13,266 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:48:13,270 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:48:13,271 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:48:13,273 >>   Batch size = 8
{'eval_loss': 1.7352557182312012, 'eval_accuracy': 0.5458937198067633, 'eval_runtime': 1.4231, 'eval_samples_per_second': 145.455, 'eval_steps_per_second': 18.27, 'epoch': 25.0}
{'loss': 0.0432, 'grad_norm': 0.8870682120323181, 'learning_rate': 3.625e-06, 'epoch': 25.5}
{'loss': 0.0413, 'grad_norm': 2.2859182357788086, 'learning_rate': 3.5e-06, 'epoch': 26.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 28.96it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.95it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.33it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.71it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.32it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.09it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.89it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.73it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.62it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.60it/s][A                                                 
                                               [A 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 520/800 [05:51<01:22,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.60it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:48:14,719 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-520
[INFO|configuration_utils.py:471] 2024-05-15 00:48:14,726 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-520/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:48:16,723 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-520/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:48:16,731 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-520/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:48:16,737 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-520/special_tokens_map.json
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 521/800 [05:57<11:34,  2.49s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 522/800 [05:58<08:29,  1.83s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 523/800 [05:58<06:20,  1.37s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 524/800 [05:58<04:50,  1.05s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 525/800 [05:58<03:47,  1.21it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 526/800 [05:59<03:03,  1.49it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 527/800 [05:59<02:32,  1.79it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 528/800 [05:59<02:11,  2.08it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 529/800 [06:00<01:55,  2.34it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 530/800 [06:00<01:45,  2.57it/s]                                                  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 530/800 [06:00<01:45,  2.57it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 531/800 [06:00<01:38,  2.74it/s] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 532/800 [06:01<01:32,  2.89it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 533/800 [06:01<01:28,  3.01it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 534/800 [06:01<01:26,  3.09it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 535/800 [06:01<01:23,  3.16it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 536/800 [06:02<01:22,  3.20it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 537/800 [06:02<01:21,  3.23it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 538/800 [06:02<01:20,  3.25it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 539/800 [06:03<01:19,  3.27it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 540/800 [06:03<01:17,  3.37it/s]                                                  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 540/800 [06:03<01:17,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:48:26,584 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:48:26,588 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:48:26,590 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:48:26,592 >>   Batch size = 8
{'eval_loss': 1.7623463869094849, 'eval_accuracy': 0.5507246376811594, 'eval_runtime': 1.421, 'eval_samples_per_second': 145.669, 'eval_steps_per_second': 18.297, 'epoch': 26.0}
{'loss': 0.0363, 'grad_norm': 1.4780619144439697, 'learning_rate': 3.3750000000000003e-06, 'epoch': 26.5}
{'loss': 0.0354, 'grad_norm': 1.2608630657196045, 'learning_rate': 3.2500000000000002e-06, 'epoch': 27.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.24it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.92it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.37it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.71it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.32it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.07it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.88it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.76it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.68it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.60it/s][A                                                 
                                               [A 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 540/800 [06:04<01:17,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.60it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:48:28,039 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540
[INFO|configuration_utils.py:471] 2024-05-15 00:48:28,046 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:48:30,071 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:48:30,079 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:48:30,085 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540/special_tokens_map.json
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 541/800 [06:11<10:47,  2.50s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 542/800 [06:11<07:54,  1.84s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 543/800 [06:11<05:54,  1.38s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 544/800 [06:11<04:30,  1.05s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 545/800 [06:12<03:31,  1.21it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 546/800 [06:12<02:50,  1.49it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 547/800 [06:12<02:21,  1.79it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 548/800 [06:13<02:01,  2.07it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 549/800 [06:13<01:47,  2.33it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 550/800 [06:13<01:37,  2.56it/s]                                                  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 550/800 [06:13<01:37,  2.56it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 551/800 [06:14<01:31,  2.73it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 552/800 [06:14<01:26,  2.88it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 553/800 [06:14<01:22,  3.00it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 554/800 [06:14<01:19,  3.09it/s] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 555/800 [06:15<01:17,  3.15it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 556/800 [06:15<01:16,  3.19it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 557/800 [06:15<01:15,  3.23it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 558/800 [06:16<01:14,  3.25it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 559/800 [06:16<01:13,  3.27it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 560/800 [06:16<01:11,  3.37it/s]                                                  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 560/800 [06:16<01:11,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:48:39,935 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:48:39,938 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:48:39,940 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:48:39,941 >>   Batch size = 8
{'eval_loss': 1.7236655950546265, 'eval_accuracy': 0.5507246376811594, 'eval_runtime': 1.4232, 'eval_samples_per_second': 145.444, 'eval_steps_per_second': 18.268, 'epoch': 27.0}
{'loss': 0.0317, 'grad_norm': 1.5162782669067383, 'learning_rate': 3.125e-06, 'epoch': 27.5}
{'loss': 0.0319, 'grad_norm': 1.3278621435165405, 'learning_rate': 3e-06, 'epoch': 28.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.19it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.97it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.34it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.64it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.31it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.09it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.89it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.75it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.66it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.55it/s][A                                                 
                                               [A 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 560/800 [06:18<01:11,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.55it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:48:41,389 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-560
[INFO|configuration_utils.py:471] 2024-05-15 00:48:41,415 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-560/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:48:43,360 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-560/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:48:43,369 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-560/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:48:43,375 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-560/special_tokens_map.json
 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 561/800 [06:24<09:48,  2.46s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 562/800 [06:24<07:11,  1.81s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 563/800 [06:24<05:22,  1.36s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 564/800 [06:25<04:06,  1.04s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 565/800 [06:25<03:12,  1.22it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 566/800 [06:25<02:35,  1.50it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 567/800 [06:26<02:09,  1.80it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 568/800 [06:26<01:51,  2.08it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 569/800 [06:26<01:38,  2.34it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 570/800 [06:27<01:29,  2.57it/s]                                                  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 570/800 [06:27<01:29,  2.57it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 571/800 [06:27<01:23,  2.74it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 572/800 [06:27<01:18,  2.89it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 573/800 [06:27<01:15,  3.00it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 574/800 [06:28<01:13,  3.09it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 575/800 [06:28<01:11,  3.15it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 576/800 [06:28<01:10,  3.20it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 577/800 [06:29<01:09,  3.23it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 578/800 [06:29<01:08,  3.25it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 579/800 [06:29<01:07,  3.27it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 580/800 [06:30<01:05,  3.38it/s]                                                  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 580/800 [06:30<01:05,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:48:53,171 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:48:53,175 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:48:53,177 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:48:53,179 >>   Batch size = 8
{'eval_loss': 1.7699817419052124, 'eval_accuracy': 0.5507246376811594, 'eval_runtime': 1.4241, 'eval_samples_per_second': 145.351, 'eval_steps_per_second': 18.257, 'epoch': 28.0}
{'loss': 0.028, 'grad_norm': 0.7994670271873474, 'learning_rate': 2.875e-06, 'epoch': 28.5}
{'loss': 0.0281, 'grad_norm': 0.7506239414215088, 'learning_rate': 2.7500000000000004e-06, 'epoch': 29.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.07it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.94it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.37it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.72it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.34it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.10it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.92it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.79it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.66it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.60it/s][A                                                 
                                               [A 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 580/800 [06:31<01:05,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.60it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:48:54,631 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-580
[INFO|configuration_utils.py:471] 2024-05-15 00:48:54,637 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-580/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:48:56,709 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-580/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:48:56,718 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-580/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:48:56,723 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-580/special_tokens_map.json
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 581/800 [06:37<09:11,  2.52s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 582/800 [06:38<06:44,  1.85s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 583/800 [06:38<05:01,  1.39s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 584/800 [06:38<03:49,  1.06s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 585/800 [06:38<02:59,  1.20it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 586/800 [06:39<02:24,  1.48it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 587/800 [06:39<01:59,  1.78it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 588/800 [06:39<01:42,  2.06it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 589/800 [06:40<01:30,  2.33it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 590/800 [06:40<01:22,  2.56it/s]                                                  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 590/800 [06:40<01:22,  2.56it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 591/800 [06:40<01:16,  2.73it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 592/800 [06:41<01:12,  2.88it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 593/800 [06:41<01:09,  3.00it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 594/800 [06:41<01:06,  3.09it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 595/800 [06:41<01:05,  3.15it/s] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 596/800 [06:42<01:03,  3.20it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 597/800 [06:42<01:02,  3.23it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 598/800 [06:42<01:02,  3.25it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 599/800 [06:43<01:01,  3.27it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 600/800 [06:43<00:59,  3.37it/s]                                                  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 600/800 [06:43<00:59,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:49:06,592 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:49:06,596 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:49:06,598 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:49:06,600 >>   Batch size = 8
{'eval_loss': 1.7402682304382324, 'eval_accuracy': 0.5458937198067633, 'eval_runtime': 1.4309, 'eval_samples_per_second': 144.66, 'eval_steps_per_second': 18.17, 'epoch': 29.0}
{'loss': 0.025, 'grad_norm': 0.9882813096046448, 'learning_rate': 2.6250000000000003e-06, 'epoch': 29.5}
{'loss': 0.0234, 'grad_norm': 0.6024483442306519, 'learning_rate': 2.5e-06, 'epoch': 30.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.19it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 22.02it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.37it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.66it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.26it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.08it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.90it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.78it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.70it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.64it/s][A                                                 
                                               [A 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 600/800 [06:44<00:59,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.64it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:49:08,062 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-600
[INFO|configuration_utils.py:471] 2024-05-15 00:49:08,069 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-600/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:49:10,068 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-600/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:49:10,078 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:49:10,083 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-600/special_tokens_map.json
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 601/800 [06:51<08:36,  2.60s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 602/800 [06:51<06:17,  1.91s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 603/800 [06:52<04:40,  1.43s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 604/800 [06:52<03:33,  1.09s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 605/800 [06:52<02:46,  1.17it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 606/800 [06:52<02:13,  1.45it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 607/800 [06:53<01:50,  1.75it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 608/800 [06:53<01:34,  2.04it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 609/800 [06:53<01:22,  2.30it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 610/800 [06:54<01:14,  2.53it/s]                                                  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 610/800 [06:54<01:14,  2.53it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 611/800 [06:54<01:09,  2.71it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 612/800 [06:54<01:05,  2.87it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 613/800 [06:55<01:02,  2.99it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 614/800 [06:55<01:00,  3.08it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 615/800 [06:55<00:58,  3.14it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 616/800 [06:55<00:57,  3.19it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 617/800 [06:56<00:56,  3.23it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 618/800 [06:56<00:56,  3.25it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 619/800 [06:56<00:55,  3.27it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 620/800 [06:57<00:53,  3.37it/s]                                                  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 620/800 [06:57<00:53,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:49:20,270 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:49:20,275 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:49:20,277 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:49:20,278 >>   Batch size = 8
{'eval_loss': 1.788109540939331, 'eval_accuracy': 0.5458937198067633, 'eval_runtime': 1.4411, 'eval_samples_per_second': 143.635, 'eval_steps_per_second': 18.041, 'epoch': 30.0}
{'loss': 0.023, 'grad_norm': 1.0393989086151123, 'learning_rate': 2.375e-06, 'epoch': 30.5}
{'loss': 0.024, 'grad_norm': 0.44185805320739746, 'learning_rate': 2.25e-06, 'epoch': 31.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.18it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.93it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.33it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.63it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.27it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.06it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.90it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.78it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.66it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.61it/s][A                                                 
                                               [A 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 620/800 [06:58<00:53,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.61it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:49:21,723 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-620
[INFO|configuration_utils.py:471] 2024-05-15 00:49:21,729 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-620/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:49:23,713 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-620/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:49:23,721 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-620/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:49:23,726 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-620/special_tokens_map.json
 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 621/800 [07:04<07:25,  2.49s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 622/800 [07:05<05:26,  1.83s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 623/800 [07:05<04:02,  1.37s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 624/800 [07:05<03:05,  1.05s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 625/800 [07:05<02:24,  1.21it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 626/800 [07:06<01:56,  1.49it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 627/800 [07:06<01:36,  1.79it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 628/800 [07:06<01:22,  2.07it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 629/800 [07:07<01:13,  2.34it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 630/800 [07:07<01:06,  2.56it/s]                                                  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 630/800 [07:07<01:06,  2.56it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 631/800 [07:07<01:01,  2.74it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 632/800 [07:08<00:58,  2.88it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 633/800 [07:08<00:55,  3.00it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 634/800 [07:08<00:53,  3.08it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 635/800 [07:08<00:52,  3.15it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 636/800 [07:09<00:51,  3.19it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 637/800 [07:09<00:50,  3.22it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 638/800 [07:09<00:49,  3.25it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 639/800 [07:10<00:49,  3.27it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 640/800 [07:10<00:47,  3.37it/s]                                                  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 640/800 [07:10<00:47,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:49:33,592 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:49:33,596 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:49:33,598 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:49:33,599 >>   Batch size = 8
{'eval_loss': 1.7538292407989502, 'eval_accuracy': 0.5555555555555556, 'eval_runtime': 1.4235, 'eval_samples_per_second': 145.415, 'eval_steps_per_second': 18.265, 'epoch': 31.0}
{'loss': 0.0235, 'grad_norm': 0.9507629871368408, 'learning_rate': 2.125e-06, 'epoch': 31.5}
{'loss': 0.0196, 'grad_norm': 0.4601610004901886, 'learning_rate': 2.0000000000000003e-06, 'epoch': 32.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.23it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.98it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.39it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.70it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.33it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.07it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.89it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.80it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.73it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.69it/s][A                                                 
                                               [A 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 640/800 [07:11<00:47,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.69it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:49:35,042 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-640
[INFO|configuration_utils.py:471] 2024-05-15 00:49:35,049 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-640/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:49:37,046 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-640/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:49:37,054 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-640/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:49:37,060 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-640/special_tokens_map.json
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 641/800 [07:18<06:35,  2.49s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 642/800 [07:18<04:49,  1.83s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 643/800 [07:18<03:35,  1.37s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 644/800 [07:18<02:43,  1.05s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 645/800 [07:19<02:08,  1.21it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 646/800 [07:19<01:42,  1.50it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 647/800 [07:19<01:25,  1.79it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 648/800 [07:20<01:13,  2.08it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 649/800 [07:20<01:04,  2.34it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 650/800 [07:20<00:58,  2.56it/s]                                                  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 650/800 [07:20<00:58,  2.56it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 651/800 [07:21<00:54,  2.73it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 652/800 [07:21<00:51,  2.88it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 653/800 [07:21<00:48,  3.00it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 654/800 [07:21<00:47,  3.08it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 655/800 [07:22<00:46,  3.15it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 656/800 [07:22<00:45,  3.20it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 657/800 [07:22<00:44,  3.23it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 658/800 [07:23<00:43,  3.25it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 659/800 [07:23<00:43,  3.27it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 660/800 [07:23<00:41,  3.37it/s]                                                  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 660/800 [07:23<00:41,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:49:46,911 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:49:46,915 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:49:46,916 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:49:46,918 >>   Batch size = 8
{'eval_loss': 1.755861759185791, 'eval_accuracy': 0.5555555555555556, 'eval_runtime': 1.4199, 'eval_samples_per_second': 145.785, 'eval_steps_per_second': 18.311, 'epoch': 32.0}
{'loss': 0.0199, 'grad_norm': 1.0189006328582764, 'learning_rate': 1.8750000000000003e-06, 'epoch': 32.5}
{'loss': 0.0208, 'grad_norm': 0.5331593155860901, 'learning_rate': 1.75e-06, 'epoch': 33.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.15it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.81it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.26it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.62it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.33it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.07it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.84it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.70it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.64it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.60it/s][A                                                 
                                               [A 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 660/800 [07:25<00:41,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.60it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:49:48,362 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-660
[INFO|configuration_utils.py:471] 2024-05-15 00:49:48,369 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-660/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:49:50,341 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-660/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:49:50,350 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-660/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:49:50,355 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-660/special_tokens_map.json
 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 661/800 [07:31<05:43,  2.47s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 662/800 [07:31<04:11,  1.82s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 663/800 [07:31<03:07,  1.37s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 664/800 [07:32<02:22,  1.05s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 665/800 [07:32<01:51,  1.21it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 666/800 [07:32<01:29,  1.50it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 667/800 [07:33<01:14,  1.80it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 668/800 [07:33<01:03,  2.08it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 669/800 [07:33<00:55,  2.34it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 670/800 [07:34<00:50,  2.57it/s]                                                  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 670/800 [07:34<00:50,  2.57it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 671/800 [07:34<00:47,  2.74it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 672/800 [07:34<00:44,  2.89it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 673/800 [07:34<00:42,  3.00it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 674/800 [07:35<00:40,  3.09it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 675/800 [07:35<00:39,  3.15it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 676/800 [07:35<00:38,  3.20it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 677/800 [07:36<00:38,  3.23it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 678/800 [07:36<00:37,  3.25it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 679/800 [07:36<00:37,  3.27it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 680/800 [07:37<00:35,  3.37it/s]                                                  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 680/800 [07:37<00:35,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:50:00,181 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:50:00,185 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:50:00,187 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:50:00,189 >>   Batch size = 8
{'eval_loss': 1.764967441558838, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4234, 'eval_samples_per_second': 145.427, 'eval_steps_per_second': 18.266, 'epoch': 33.0}
{'loss': 0.0186, 'grad_norm': 0.48274943232536316, 'learning_rate': 1.6250000000000001e-06, 'epoch': 33.5}
{'loss': 0.0186, 'grad_norm': 0.622961699962616, 'learning_rate': 1.5e-06, 'epoch': 34.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.19it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 22.03it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.42it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.73it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.34it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.10it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.91it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.77it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.66it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.59it/s][A                                                 
                                               [A 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 680/800 [07:38<00:35,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.59it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:50:01,632 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-680
[INFO|configuration_utils.py:471] 2024-05-15 00:50:01,657 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-680/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:50:03,658 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-680/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:50:03,666 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:50:03,672 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-680/special_tokens_map.json
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 681/800 [07:44<04:55,  2.49s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 682/800 [07:44<03:35,  1.83s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 683/800 [07:45<02:40,  1.37s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 684/800 [07:45<02:01,  1.05s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 685/800 [07:45<01:34,  1.21it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 686/800 [07:46<01:16,  1.50it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 687/800 [07:46<01:03,  1.79it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 688/800 [07:46<00:53,  2.08it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 689/800 [07:47<00:47,  2.34it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 690/800 [07:47<00:42,  2.57it/s]                                                  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 690/800 [07:47<00:42,  2.57it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 691/800 [07:47<00:39,  2.74it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 692/800 [07:47<00:37,  2.89it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 693/800 [07:48<00:35,  3.00it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 694/800 [07:48<00:34,  3.09it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 695/800 [07:48<00:33,  3.15it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 696/800 [07:49<00:32,  3.19it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 697/800 [07:49<00:31,  3.23it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 698/800 [07:49<00:31,  3.26it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 699/800 [07:50<00:30,  3.27it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 700/800 [07:50<00:29,  3.38it/s]                                                  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 700/800 [07:50<00:29,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:50:13,485 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:50:13,488 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:50:13,490 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:50:13,492 >>   Batch size = 8
{'eval_loss': 1.7733298540115356, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4216, 'eval_samples_per_second': 145.608, 'eval_steps_per_second': 18.289, 'epoch': 34.0}
{'loss': 0.0178, 'grad_norm': 0.5946810245513916, 'learning_rate': 1.3750000000000002e-06, 'epoch': 34.5}
{'loss': 0.0164, 'grad_norm': 0.5902403593063354, 'learning_rate': 1.25e-06, 'epoch': 35.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.03it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.93it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.32it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.65it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.28it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.05it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.85it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.74it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.66it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.62it/s][A                                                 
                                               [A 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 700/800 [07:51<00:29,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.62it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:50:14,937 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-700
[INFO|configuration_utils.py:471] 2024-05-15 00:50:14,944 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-700/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:50:16,939 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-700/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:50:16,949 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:50:16,955 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-700/special_tokens_map.json
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 701/800 [07:58<04:20,  2.63s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 702/800 [07:58<03:09,  1.93s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 703/800 [07:59<02:19,  1.44s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 704/800 [07:59<01:45,  1.10s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 705/800 [07:59<01:21,  1.16it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 706/800 [07:59<01:05,  1.44it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 707/800 [08:00<00:53,  1.73it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 708/800 [08:00<00:45,  2.02it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 709/800 [08:00<00:39,  2.29it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 710/800 [08:01<00:35,  2.52it/s]                                                  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 710/800 [08:01<00:35,  2.52it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 711/800 [08:01<00:32,  2.71it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 712/800 [08:01<00:30,  2.86it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 713/800 [08:02<00:29,  2.98it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 714/800 [08:02<00:27,  3.08it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 715/800 [08:02<00:27,  3.14it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 716/800 [08:02<00:26,  3.19it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 717/800 [08:03<00:25,  3.23it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 718/800 [08:03<00:25,  3.25it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 719/800 [08:03<00:24,  3.27it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 720/800 [08:04<00:23,  3.38it/s]                                                  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 720/800 [08:04<00:23,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:50:27,272 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:50:27,276 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:50:27,277 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:50:27,279 >>   Batch size = 8
{'eval_loss': 1.7731701135635376, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4232, 'eval_samples_per_second': 145.447, 'eval_steps_per_second': 18.269, 'epoch': 35.0}
{'loss': 0.0171, 'grad_norm': 0.499220073223114, 'learning_rate': 1.125e-06, 'epoch': 35.5}
{'loss': 0.0168, 'grad_norm': 1.519394040107727, 'learning_rate': 1.0000000000000002e-06, 'epoch': 36.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.26it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.79it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.26it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.61it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.25it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.06it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.86it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.73it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.63it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.56it/s][A                                                 
                                               [A 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 720/800 [08:05<00:23,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.56it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:50:28,726 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-720
[INFO|configuration_utils.py:471] 2024-05-15 00:50:28,734 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-720/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:50:30,718 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-720/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:50:30,726 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-720/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:50:30,733 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-720/special_tokens_map.json
 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 721/800 [08:11<03:14,  2.47s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 722/800 [08:11<02:21,  1.82s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 723/800 [08:12<01:44,  1.36s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 724/800 [08:12<01:19,  1.04s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 725/800 [08:12<01:01,  1.22it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 726/800 [08:13<00:49,  1.50it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 727/800 [08:13<00:40,  1.80it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 728/800 [08:13<00:34,  2.09it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 729/800 [08:14<00:30,  2.35it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 730/800 [08:14<00:27,  2.57it/s]                                                  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 730/800 [08:14<00:27,  2.57it/s] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 731/800 [08:14<00:25,  2.75it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 732/800 [08:14<00:23,  2.90it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 733/800 [08:15<00:22,  3.01it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 734/800 [08:15<00:21,  3.09it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 735/800 [08:15<00:20,  3.16it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 736/800 [08:16<00:19,  3.20it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 737/800 [08:16<00:19,  3.23it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 738/800 [08:16<00:19,  3.25it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 739/800 [08:17<00:18,  3.27it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 740/800 [08:17<00:17,  3.37it/s]                                                  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 740/800 [08:17<00:17,  3.37it/s][INFO|trainer.py:765] 2024-05-15 00:50:40,512 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:50:40,516 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:50:40,517 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:50:40,519 >>   Batch size = 8
{'eval_loss': 1.7853875160217285, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4236, 'eval_samples_per_second': 145.405, 'eval_steps_per_second': 18.263, 'epoch': 36.0}
{'loss': 0.0156, 'grad_norm': 0.32934170961380005, 'learning_rate': 8.75e-07, 'epoch': 36.5}
{'loss': 0.0189, 'grad_norm': 0.3674474060535431, 'learning_rate': 7.5e-07, 'epoch': 37.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.11it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.97it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.33it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.65it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.29it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.09it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.81it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.72it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.66it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.62it/s][A                                                 
                                               [A 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 740/800 [08:18<00:17,  3.37it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.62it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:50:41,968 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-740
[INFO|configuration_utils.py:471] 2024-05-15 00:50:41,975 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-740/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:50:43,984 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-740/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:50:43,993 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-740/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:50:43,998 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-740/special_tokens_map.json
 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 741/800 [08:25<02:38,  2.68s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 742/800 [08:25<01:54,  1.97s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 743/800 [08:26<01:23,  1.47s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 744/800 [08:26<01:02,  1.12s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 745/800 [08:26<00:48,  1.14it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 746/800 [08:27<00:37,  1.42it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 747/800 [08:27<00:30,  1.72it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 748/800 [08:27<00:25,  2.01it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 749/800 [08:28<00:22,  2.28it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 750/800 [08:28<00:19,  2.51it/s]                                                  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 750/800 [08:28<00:19,  2.51it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 751/800 [08:28<00:18,  2.69it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 752/800 [08:28<00:16,  2.85it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 753/800 [08:29<00:15,  2.98it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 754/800 [08:29<00:14,  3.07it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 755/800 [08:29<00:14,  3.14it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 756/800 [08:30<00:13,  3.19it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 757/800 [08:30<00:13,  3.19it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 758/800 [08:30<00:13,  3.23it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 759/800 [08:31<00:12,  3.24it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 760/800 [08:31<00:11,  3.35it/s]                                                  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 760/800 [08:31<00:11,  3.35it/s][INFO|trainer.py:765] 2024-05-15 00:50:54,493 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:50:54,497 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:50:54,499 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:50:54,501 >>   Batch size = 8
{'eval_loss': 1.7964427471160889, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4281, 'eval_samples_per_second': 144.944, 'eval_steps_per_second': 18.206, 'epoch': 37.0}
{'loss': 0.016, 'grad_norm': 0.3100169897079468, 'learning_rate': 6.25e-07, 'epoch': 37.5}
{'loss': 0.0166, 'grad_norm': 0.3822616636753082, 'learning_rate': 5.000000000000001e-07, 'epoch': 38.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.27it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.96it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.35it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.64it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.27it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.09it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.90it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.72it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.61it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.54it/s][A                                                 
                                               [A 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 760/800 [08:32<00:11,  3.35it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.54it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:50:55,949 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-760
[INFO|configuration_utils.py:471] 2024-05-15 00:50:55,955 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-760/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:50:58,041 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-760/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:50:58,050 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-760/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:50:58,057 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-760/special_tokens_map.json
 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 761/800 [08:39<01:40,  2.58s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 762/800 [08:39<01:11,  1.89s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 763/800 [08:39<00:52,  1.42s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 764/800 [08:40<00:38,  1.08s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 765/800 [08:40<00:29,  1.18it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 766/800 [08:40<00:23,  1.46it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 767/800 [08:41<00:18,  1.76it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 768/800 [08:41<00:15,  2.05it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 769/800 [08:41<00:13,  2.31it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 770/800 [08:41<00:11,  2.54it/s]                                                  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 770/800 [08:41<00:11,  2.54it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 771/800 [08:42<00:10,  2.72it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 772/800 [08:42<00:09,  2.88it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 773/800 [08:42<00:09,  3.00it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 774/800 [08:43<00:08,  3.08it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 775/800 [08:43<00:07,  3.15it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 776/800 [08:43<00:07,  3.19it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 777/800 [08:44<00:07,  3.23it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 778/800 [08:44<00:06,  3.25it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 779/800 [08:44<00:06,  3.27it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 780/800 [08:44<00:05,  3.38it/s]                                                  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 780/800 [08:44<00:05,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:51:08,096 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:51:08,100 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:51:08,102 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:51:08,103 >>   Batch size = 8
{'eval_loss': 1.7911357879638672, 'eval_accuracy': 0.5555555555555556, 'eval_runtime': 1.4269, 'eval_samples_per_second': 145.07, 'eval_steps_per_second': 18.221, 'epoch': 38.0}
{'loss': 0.0155, 'grad_norm': 0.5930668711662292, 'learning_rate': 3.75e-07, 'epoch': 38.5}
{'loss': 0.0163, 'grad_norm': 0.48172202706336975, 'learning_rate': 2.5000000000000004e-07, 'epoch': 39.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.14it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.96it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.34it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.68it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.32it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.07it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.90it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.77it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.69it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.64it/s][A                                                 
                                               [A 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 780/800 [08:46<00:05,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.64it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:51:09,547 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-780
[INFO|configuration_utils.py:471] 2024-05-15 00:51:09,570 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-780/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:51:11,584 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-780/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:51:11,593 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:51:11,599 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-780/special_tokens_map.json
 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 781/800 [08:52<00:48,  2.57s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 782/800 [08:53<00:33,  1.89s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 783/800 [08:53<00:24,  1.41s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 784/800 [08:53<00:17,  1.08s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 785/800 [08:54<00:12,  1.18it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 786/800 [08:54<00:09,  1.47it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 787/800 [08:54<00:07,  1.76it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 788/800 [08:54<00:05,  2.05it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 789/800 [08:55<00:04,  2.31it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 790/800 [08:55<00:03,  2.54it/s]                                                  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 790/800 [08:55<00:03,  2.54it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 791/800 [08:55<00:03,  2.72it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 792/800 [08:56<00:02,  2.88it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 793/800 [08:56<00:02,  2.99it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 794/800 [08:56<00:01,  3.08it/s] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 795/800 [08:57<00:01,  3.15it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 796/800 [08:57<00:01,  3.20it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 797/800 [08:57<00:00,  3.23it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 798/800 [08:57<00:00,  3.26it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 799/800 [08:58<00:00,  3.27it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [08:58<00:00,  3.38it/s]                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [08:58<00:00,  3.38it/s][INFO|trainer.py:765] 2024-05-15 00:51:21,678 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:51:21,681 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:51:21,683 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:51:21,685 >>   Batch size = 8
{'eval_loss': 1.7974388599395752, 'eval_accuracy': 0.5603864734299517, 'eval_runtime': 1.4212, 'eval_samples_per_second': 145.648, 'eval_steps_per_second': 18.294, 'epoch': 39.0}
{'loss': 0.0158, 'grad_norm': 0.3446134924888611, 'learning_rate': 1.2500000000000002e-07, 'epoch': 39.5}
{'loss': 0.0162, 'grad_norm': 0.49807652831077576, 'learning_rate': 0.0, 'epoch': 40.0}

  0%|          | 0/26 [00:00<?, ?it/s][A
 12%|‚ñà‚ñè        | 3/26 [00:00<00:00, 29.19it/s][A
 23%|‚ñà‚ñà‚ñé       | 6/26 [00:00<00:00, 21.92it/s][A
 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 20.29it/s][A
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:00<00:00, 19.67it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 19.31it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 19.08it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:00<00:00, 18.88it/s][A
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.78it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 18.68it/s][A
 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 18.61it/s][A                                                 
                                               [A100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [08:59<00:00,  3.38it/s]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.61it/s][A
                                               [A[INFO|trainer.py:3203] 2024-05-15 00:51:23,129 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-800
[INFO|configuration_utils.py:471] 2024-05-15 00:51:23,136 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-800/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:51:25,162 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-800/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:51:25,171 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:51:25,177 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-800/special_tokens_map.json
[INFO|trainer.py:2231] 2024-05-15 00:51:28,898 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2436] 2024-05-15 00:51:28,900 >> Loading best model from /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/checkpoint-540 (score: 1.7236655950546265).
                                                 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [09:12<00:00,  3.38it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 800/800 [09:12<00:00,  1.45it/s]
[INFO|trainer.py:3203] 2024-05-15 00:51:35,167 >> Saving model checkpoint to /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20
[INFO|configuration_utils.py:471] 2024-05-15 00:51:35,175 >> Configuration saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/config.json
[INFO|modeling_utils.py:2474] 2024-05-15 00:51:37,171 >> Model weights saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/model.safetensors
[INFO|tokenization_utils_base.py:2502] 2024-05-15 00:51:37,213 >> tokenizer config file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-05-15 00:51:37,219 >> Special tokens file saved in /ukp-storage-1/yang/LLM_Anonymization/programming_runs/root/roberta-large_reddit_clss_b16_e20/special_tokens_map.json
{'eval_loss': 1.7982027530670166, 'eval_accuracy': 0.5555555555555556, 'eval_runtime': 1.4224, 'eval_samples_per_second': 145.53, 'eval_steps_per_second': 18.279, 'epoch': 40.0}
{'train_runtime': 567.4795, 'train_samples_per_second': 22.415, 'train_steps_per_second': 1.41, 'train_loss': 0.7736774892546237, 'epoch': 40.0}
***** train metrics *****
  epoch                    =       40.0
  train_loss               =     0.7737
  train_runtime            = 0:09:27.47
  train_samples            =        318
  train_samples_per_second =     22.415
  train_steps_per_second   =       1.41
05/15/2024 00:51:37 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:765] 2024-05-15 00:51:37,309 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response. If guess_correctness, feature, hardness, personality, sentence, question_asked, guess, response are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3512] 2024-05-15 00:51:37,312 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-05-15 00:51:37,314 >>   Num examples = 207
[INFO|trainer.py:3517] 2024-05-15 00:51:37,316 >>   Batch size = 8
  0%|          | 0/26 [00:00<?, ?it/s] 12%|‚ñà‚ñè        | 3/26 [00:00<00:01, 17.14it/s] 19%|‚ñà‚ñâ        | 5/26 [00:00<00:01, 17.35it/s] 27%|‚ñà‚ñà‚ñã       | 7/26 [00:00<00:01, 18.06it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:00<00:00, 18.40it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 11/26 [00:00<00:00, 18.62it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 13/26 [00:00<00:00, 18.77it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:00<00:00, 18.86it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [00:00<00:00, 18.93it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [00:01<00:00, 18.97it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:01<00:00, 18.98it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:01<00:00, 19.02it/s] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:01<00:00, 19.03it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:01<00:00, 18.48it/s]
[INFO|modelcard.py:450] 2024-05-15 00:51:38,976 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.5507246376811594}]}
***** eval metrics *****
  epoch                   =       40.0
  eval_accuracy           =     0.5507
  eval_loss               =     1.7237
  eval_runtime            = 0:00:01.50
  eval_samples            =        207
  eval_samples_per_second =    137.577
  eval_steps_per_second   =      17.28
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: - 0.133 MB of 0.133 MB uploadedwandb: \ 0.133 MB of 0.226 MB uploadedwandb: | 0.226 MB of 0.226 MB uploadedwandb: / 0.226 MB of 0.226 MB uploadedwandb: 
wandb: Run history:
wandb:           eval/accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:               eval/loss ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:            eval/runtime ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñà
wandb: eval/samples_per_second ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÅ
wandb:   eval/steps_per_second ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñÅ
wandb:             train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:       train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:         train/grad_norm ‚ñÑ‚ñÉ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:     train/learning_rate ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:              train/loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:            eval/accuracy 0.55072
wandb:                eval/loss 1.72367
wandb:             eval/runtime 1.5046
wandb:  eval/samples_per_second 137.577
wandb:    eval/steps_per_second 17.28
wandb:               total_flos 2963872170455040.0
wandb:              train/epoch 40.0
wandb:        train/global_step 800
wandb:          train/grad_norm 0.49808
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.0162
wandb:               train_loss 0.77368
wandb:            train_runtime 567.4795
wandb: train_samples_per_second 22.415
wandb:   train_steps_per_second 1.41
wandb: 
wandb: üöÄ View run reddit_roberta-large_lr1e-5_B16_E40 at: https://wandb.ai/ukp-conv/Privacy-NLP/runs/97axf02o
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240515_004219-97axf02o/logs
